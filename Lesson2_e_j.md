# Lesson 2

[Video](https://youtu.be/Egp4Zajhzog) / [Lesson Forum]((https://forums.fast.ai/t/lesson-2-official-resources-and-updates/28630)) / [General Forum]((https://forums.fast.ai/t/faq-resources-and-official-course-updates/27934/))

## Deeper Dive into Computer Vision
コンピュータビジョンへ深く向かう

Taking a deeper dive into computer vision applications, taking some of the amazing stuff you've all been doing during the week, and going even further.

今週中に皆さんが今までやってきた素晴らしいことのいくつかを取り上げ、さらに進んでいく、コンピュータービジョンアプリケーションについてもっと深く掘り下げること。

### Forum tips and tricks [[0:17]](https://youtu.be/Egp4Zajhzog?t=17)
フォーラムの知識とトリック

Two important forum topics:
二つの重要なフォーラムの話題

- [FAQ, resources, and official course updates](https://forums.fast.ai/t/faq-resources-and-official-course-updates/27934/)

- [Lesson 2 official resources and updates](https://forums.fast.ai/t/lesson-2-official-resources-and-updates/28630)


#### "Summarize This Topic" [[2:32]](https://youtu.be/Egp4Zajhzog?t=152)
トピックの要約

After just one week, the most popular thread has 1.1k replies which is intimidatingly large number. You shouldn't need to read all of it.  What you should do is click "Summarize This Topic" and it will only show the most liked ones.

たった1週間後、最も人気のあるスレッドは1.1kの返信を寄せられ、非常に大きな数となっています。 全部読む必要はないはずです。 あなたがすべきことは "Summarize This Topic"をクリックすることであり、それは最も好きなものだけを表示するでしょう。

![](lesson2/1.png)

####  Returning to work [[3:19]](https://youtu.be/Egp4Zajhzog?t=199)
職場に戻る

https://course-v3.fast.ai/ now has a "Returning to work" section which will show you (for each specific platform you use):
https://course-v3.fast.ai/に「職場に戻る」セクションがあり、（あなたが使う特定のプラットフォームごとに）あなたを紹介します：


- How to make sure you have the latest notebooks
- How to make sure you have the latest fastai library 

 - 最新のノートブックを確実に入手する方法
 - 最新のfastaiライブラリがあることを確認する方法
 
If things aren't working for you, if you get into some kind of messy situation, which we all do, just delete your instance and start again unless you've got mission-critical stuff there — it's the easiest way just to get out of a sticky situation.

問題がうまくいかない場合、私たち全員がそうした厄介な状況に陥った場合は、インスタンスを削除して、ミッションクリティカルなものがない限りやり直すだけです。 ねばねばした状況。

### What people have been doing this week [[4:19]](https://youtu.be/Egp4Zajhzog?t=259)
今週何をしているのか

[Share your work here](https://forums.fast.ai/t/share-your-work-here/27676/) 
宿題をシェアする

![](lesson2/2.png)

- [Figuring out who is talking — is it Ben Affleck or Joe Rogan](https://forums.fast.ai/t/share-your-work-here/27676/143) 
- [Cleaning up Watsapp downloaded images folder to get rid of memes](https://forums.fast.ai/t/share-your-work-here/27676/97)



![](lesson2/3.png)

[Forum post](https://forums.fast.ai/t/share-your-work-here/27676/215)

One of the really interesting projects was looking at the sound data that was used in [this paper](https://arxiv.org/abs/1608.04363). In this paper, they were trying to figure out what kind of sound things were. They got a state of the art of nearly 80% accuracy. Ethan Sutin then tried using the lesson 1 techniques and got 80.5% accuracy, so I think this is pretty awesome. Best as we know, it's a new state of the art for this problem. Maybe somebody since has published something we haven't found it yet. So take all of these with a slight grain of salt, but I've mentioned them on Twitter and lots of people on Twitter follow me, so if anybody knew that there was a much better approach, I'm sure somebody would have said so.

本当に興味深いプロジェクトの1つは、[この文書]（https://arxiv.org/abs/1608.04363）で使用されているサウンドデータを見ていました。 本稿では、彼らはどのような健全なものがあるのか把握しようとしていました。 彼らは80％近くの精度の最先端技術を手に入れました。 Ethan Sutinはその後、レッスン1のテクニックを使用してみたところ、80.5％の正確さで得られたので、これは非常に素晴らしいと思います。 私たちが知っているように最高の、それはこの問題のための最先端の技術です。 誰かがそれ以来私たちがそれをまだ見つけていない何かを発表したのかもしれません。 だから、私はTwitterでそれらを言及し、Twitterの多くの人々が私をフォローしているので、誰かがもっと良いアプローチがあることを知っていれば、誰かがそう言ったはずだ 。

[[6:01](https://youtu.be/Egp4Zajhzog?t=361)]

![](lesson2/4.png)

[Forum post](https://forums.fast.ai/t/share-your-work-here/27676/38)

Suvash has a new state of the art accuracy for Devanagari text recognition. I think he's got it even higher than this now. This is actually confirmed by the person on Twitter who created the dataset. I don't think he had any idea, he just posted here's a nice thing I did and this guy on Twitter said: "Oh, I made that dataset. Congratulations, you've got a new record." So that was pretty cool.

Suvashは、Devanagariテキスト認識のための新しい最先端の精度を持っています。 私は彼が今これ以上にそれを持っていると思います。 これは実際にデータセットを作成したTwitterの人によって確認されています。 私は彼が何か考えを持っていたとは思わない、彼がちょうどここに私がした素晴らしいことを投稿したそしてTwitterのこの男は言った：「ああ、私はそのデータセットを作った。 とてもクールでした。

[[6:28](https://youtu.be/Egp4Zajhzog?t=388)]

![](lesson2/5.png)

[The Mystery of the Origin](https://medium.com/@alenaharley/the-mystery-of-the-origin-cancer-type-classification-using-fast-ai-libray-212eaf8d3f4e)

I really like this post from Alena Harley. She describes in quite a bit of detail about the issue of metastasizing cancers and the use of point mutations and why that's a challenging important problem. She's got some nice pictures describing what she wants to do with this and how she can go about turning this into pictures. This is the cool trick — it's the same with urning sounds into pictures and then using the lesson 1 approach. Here is turning point mutations into pictures and then using the lesson 1 approach. And it seems that she's got a new state of the art result by more than 30% beating the previous best. Somebody on Twitter who is a VP at a genomics analysis company looked at this as well and thought it looked to be a state of the art in this particular point mutation one as well. So that's pretty exciting. 

私はAlena Harleyからのこの記事が本当に好きです。 彼女は、転移性癌の問題と点突然変異の使用について、そしてそれがなぜ難しい挑戦的な問題であるのかについて、かなり詳細に説明しています。 彼女はこれで何をしたいのか、そしてどうやってこれを写真に変えることができるのかを説明している素敵な写真をいくつか持っています。 これはクールなトリックです - それは絵に音を出すこと、そしてレッスン1のアプローチを使うことと同じです。 ここで点突然変異を写真に変えてから、レッスン1のアプローチを使います。 そして彼女は30％以上が前のベストを破って新しい最先端の結果を得たようです。 ゲノミクス分析会社の副社長であるTwitterの誰かもこれを見て、それもこの特定の点突然変異の中では最先端だと思いました。 とても興奮しています。

When we talked about last week this idea that this simple process is something which can take you a long way, it really can. I will mention that something like this one in particular is using a lot of domain expertise, like figuring out that picture to create. I wouldn't know how to do that because I don't really know what a point mutation is, let alone how to create something that visually is meaningful that a CNN could recognize. But the actual deep learning side is actually straight forward.

先週、この単純なプロセスは、あなたに長い道のりを要するものであるというこの考えについて話したとき、それは本当に可能です。 私は特にこのようなものが作成するためにその絵を考え出すことのような多くのドメインの専門知識を使っていることに言及するでしょう。 CNNが認識できるように視覚的に意味のあるものを作成する方法はもちろんのこと、私は点突然変異が何であるかについて実際にはわからないので、その方法を知りません。 しかし、実際の深い学習面は実際には簡単です。

[[8:07](https://youtu.be/Egp4Zajhzog?t=487)]

![](lesson2/6.png)

Another cool result from Simon Willison and Natalie Downe, they created a cougar or not web application over the weekend and won the Science Hack Day award in San Francisco. So I think that's pretty fantastic. So lots of examples of people doing really interesting work. Hopefully this will be inspiring to you to think well to think wow, this is cool that I can do this with what I've learned. It can also be intimidating to think like wow, these people are doing amazing things. But it's important to realize that as thousands of people are doing this course, I'm just picking out a few of really amazing ones. And in fact Simon is one of these very annoying people like Christine Payne who we talked about last week who seems to be good at everything he does. He created Django which is the world's most popular web frameworks, he founded a very successful startup, etc. One of those annoying people who tends to keep being good at things, now turns out he's good at deep learning as well. So that's fine. Simon can go on and win a hackathon on his first week of playing with deep learning. Maybe it'll take you two weeks to win your first hackathon. That's okay. 

Simon WillisonとNatalie Downeによるもう1つの素晴らしい結果は、彼らが週末にクーガーまたはウェブではないアプリケーションを作成し、サンフランシスコでサイエンスハックデー賞を受賞したことです。それで、私はそれがかなり素晴らしいと思います。とても面白い仕事をしている人々の例はたくさんあります。うまくいけば、これはあなたがすごいと思うためによく考えるために刺激するでしょう、これは私が私が学んだことでこれを行うことができることはクールです。うわー、これらの人々は素晴らしいことをやっているように考えることも威圧的なことができます。しかし、何千人もの人々がこのコースをやっているので、本当に素晴らしいものをいくつか選んでいるだけであることを認識することが重要です。そして実際には、SimonはChristine Payneのような非常に厄介な人々の一人です。彼は先週お話しましたが、彼がしていることはすべてうまくいっているようです。彼は世界で最も人気のあるWebフレームワークであるDjangoを作成し、彼は非常に成功したスタートアップなどを設立しました。物事が上手くいきがちな迷惑な人々のうちの1人です。それで大丈夫です。サイモンは、ディープラーニングでプレーした最初の週に、続けてハッカソンを獲得することができます。たぶんそれはあなたの最初のハッカソンを獲得するのに2週間かかるでしょう。大丈夫。

[[9:22](https://youtu.be/Egp4Zajhzog?t=562)]

![](lesson2/7.png)

I think it's important to mention this because there was this really inspiring blog post this week from James Dellinger who talked about how he created a bird classifier using techniques from lesson 1. But what I really found interesting was at the end, he said he nearly didn't start on deep learning at all because he went through the scikit-learn website which is one of the most important libraries of Python and he saw this. And he described in this post how he was just like that's not something I can do. That's not something I understand. Then this kind of realization of like oh, I can do useful things without reading the Greek, so I thought that was really cool message. 

今週のJames Dellingerが、レッスン1のテクニックを使って鳥の分類器を作成する方法について話した、本当に感動的なブログ記事があったので、これを言及することは重要だと思います。 彼はPythonの最も重要なライブラリの1つであるscikit-learnのWebサイトにアクセスしたので、まったく深い学習を始めませんでした。 そして彼はこの記事の中で、自分ができることではないということをどのように表現したかを説明しました。 それは私が理解していることではありません。 それなら、ギリシャ語を読まなくても便利なことができるので、本当にかっこいいメッセージだと思いました。

[[10:01](https://youtu.be/Egp4Zajhzog?t=601)]

![](lesson2/8.png)

I really wanted to highlight Daniel Armstrong on the forum. I think really shows he's a great role model here. He was saying I want to contribute to the library and I looked at the docs and I just found it overwhelming. The next message, one day later, was I don't know what any of this is, I didn't know how much there is to it, caught me off guard, my brain shut down but I love the way it forces me to learn so much. And a day later, I just submitted my first pull request. So I think that's awesome. It's okay to feel intimidated. There's a lot. But just pick one piece and dig into it. Try and push a piece of code or a documentation update, or create a classifier or whatever.

私は本当にフォーラムでDaniel Armstrongを強調したいと思いました。 私は本当に彼がここで素晴らしいロールモデルであることを示していると思います。 彼は私が図書館に貢献したいと言っていました、そして、私はドキュメントを見ました、そして、私はそれが圧倒的に見つけたところです。 1日後の次のメッセージは、これが何であるかわからない、それがどれほどあるのかわからない、私を見張って捕らえた、私の脳をシャットダウンしたが、私がそれを強制する方法が大好き とても学びます。 そして一日後、私は最初のpull requestを送ったところです。 だから私はそれが素晴らしいと思います。 おびえていても大丈夫です。 たくさんあります。 しかし、ただ一つの作品を選んでそれを掘り下げるだけです。 コードの一部またはドキュメントの更新を試してプッシュするか、分類子などを作成してください。

[[10:49](https://youtu.be/Egp4Zajhzog?t=649)]

So here's lots of cool classifiers people have built. It's been really inspiring. 

だからここに人々が作ったクールな分類子がたくさんあります。 それは本当に刺激的でした。

- Trinidad and Tobago islanders versus masquerader classifier
- A zucchini versus cucumber classifier
- Dog and cat breed classifier from last week and actually doing some exploratory work to see what the main features were, and discovered that one was most hairy dog and naked cats. So there are interesting you can do with interpretation. 
- Somebody else in the forum took that and did the same thing for anime to find that they had accidentally discovered an anime hair color classifier.
- We can now detect the new versus the old Panamanian buses.
- Henri Palacci discovered that he can recognize with 85% accuracy which of 110 countries a satellite image is of which is definitely got to be beyond human performance of just about anybody. 
- Batik cloth classification with a hundred percent accuracy
- Dave Luo did this interesting one. He actually went a little bit further using some techniques we'll be discussing in the next couple of courses to build something that can recognize complete/incomplete/foundation buildings and actually plot them on aerial satellite view. 

 - トリニダードトバゴの島民となりすましの分類器
 - ズッキーニとキュウリの分類器
 - 先週の犬と猫の品種分類器。実際に主な機能が何であるかを調べるために探索的な作業を行い、最も毛深い犬と裸の猫であることを発見しました。それであなたが解釈ですることができる面白いがあります。
 - フォーラムの他の誰かがそれを取り、彼らが誤ってアニメのヘアカラー分類子を発見したことを見つけるためにアニメに同じことをしました。
 - 新しいパナマバスと古いパナマバスの関係を確認できます。
 - アンリ・パラッチ氏は、衛星画像が110カ国のうちどれが間違いなくほぼすべての人の人間のパフォーマンスを超えていることがわかっているかを85％の精度で認識できることを発見しました。
 - 百パーセント精度のバティック布の分類
 -  Dave Luoがこの面白いものをやった。彼は実際、次の2、3のコースで議論するいくつかのテクニックを使って、完成した/不完全な/基礎の建物を認識し、実際に空中衛星ビューにそれらをプロットすることができる何かを構築するために少し進めました。

So lots and lots of fascinating projects. So don't worry. It's only been one week. It doesn't mean everybody has to have had a project out yet. A lot of the folks who already have a project out have done a previous course, so they've got a bit of a head start. But we will see today how you can definitely create your own classifier this week. 


とてもたくさんの魅力的なプロジェクト。 だから心配しないでください。 たった一週間です。 誰もがまだプロジェクトを手に入れていなければならないわけではありません。 すでにプロジェクトを実施している多くの人が前のコースをやっているので、彼らは少し頭が上がってきました。 しかし、私たちは今日、あなたが今週どのようにしてあなた自身の分類器を確実に作ることができるかを今日見るでしょう。

[[12:56]](https://youtu.be/Egp4Zajhzog?t=776)

![](lesson2/9.png)

So from today, after we did a bit deeper into really how to make these computer vision classifiers and particular work well, we're then going to look at the same thing for text. We're then going to look at the same thing for tabular data. They are more like spreadsheets and databases. Then we're going to look at collaborative filtering (i.e. recommendation systems). That's going to take us into a topic called embeddings which is a key underlying platform behind these applications. That will take us back into more computer vision and then back into more NLP. So the idea here is that it turns out that it's much better for learning if you see things multiple times so rather than being like okay, that's computer vision, you won't see it again for the rest of the course, we're actually going to come back to the two key applications NLP and computer vision a few weeks apart. That's going to force your brain to realize oh, I have to remember this. It's not must something I can throw away. 

それで今日から、私たちがこれらのコンピュータビジョン分類子と特にうまくいくようにする方法を実際に少し深くした後に、私たちはそれからテキストのために同じ事を見るつもりです。 それでは、表形式のデータについても同じことを調べます。 それらはスプレッドシートやデータベースのようなものです。 それから私達は共同フィルタリング（すなわち推薦システム）を見るつもりです。 それはこれらのアプリケーションの背後にある重要な基盤プラットフォームである埋め込みと呼ばれるトピックに私たちを連れて行くつもりです。 それは私たちをより多くのコンピュータビジョンに、そしてそれからより多くのNLPにもどすでしょう。 ですから、ここでの考え方は、物事を何度も見れば学習に向いているということです。それで、大丈夫のようなものではなく、それがコンピュータビジョンです。 数週間後に、2つの重要なアプリケーションNLPとコンピュータビジョンに戻ってくる予定です。 それはあなたの脳にああを理解させるために強制するつもりです、私はこれを覚えておく必要があります。 私が捨てることができるものである必要はありません。

[[14:06]](https://youtu.be/Egp4Zajhzog?t=846)

![](lesson2/10.png)

For people who have more of a hard sciences background in particular, a lot of folks find this hey, here's some code, type it in, start running it approach rather than here's lots of theory approach confusing and surprising and odd at first. So for those of you, I just wanted to remind you this basic tip which is keep going. You're not expected to remember everything yet. You're not expected to understand everything yet. You're not expected to know why everything works yet. You just want to be in a situation where you can enter the code and you can run it and you can get something happening and then you can start to experiment and you get a feel for what's going on. Then push on. Most of the people who have done the course and have gone on to be really successful watch the videos at least three times. So they kind of go through the whole lot and then go through it slowly the second time, then they go through it really slowly the third time. I consistently hear them say I get a lot more out of it each time I go through. So don't pause at lesson 1 and stop until you can continue. 

特にハードサイエンスの知識が豊富な人にとっては、多くの人がこのようなことに気付くでしょう。最初は、ここで説明する多くの理論的アプローチが混乱して奇妙ではなく、ここにコードを入力して実行し始めます。それであなたのそれらのために、私はちょうどあなたに行き続けるこの基本的な先端を思い出させたかったです。あなたはまだすべてを覚えているとは思われません。あなたはまだすべてを理解することは期待されていません。あなたはすべてがまだうまくいく理由を知ることを期待されていません。あなたはただコードを入力してそれを実行することができ、そして何かを起こすことができ、それからあなたは実験を始めることができ、そして何が起こっているのかを感じることができる状況になりたいだけです。それから押してください。このコースを修了して成功した人々のほとんどは、少なくとも3回はビデオを見ています。それで彼らはまるでロット全体を通り抜け、それから二度目にゆっくりと通り抜け、そして三度目にそれを本当にゆっくり通り抜けます。私は彼らが私が経験するたびに私はそれからずっともっと多くを得ると言うことを一貫して聞いています。そのため、レッスン1で一時停止しないで、続行できるまで停止してください。

This approach is based on a lot of academic research into learning theory. One guy in particular David Perkins from Harvard has this really great analogy. He is a researcher into learning theory. He describes this approach of whole game which is basically if you're teaching a kid to play soccer, you don't first of all teach them about how the friction between a ball and grass works and then teach them how to saw a soccer ball with their bare hands, and then teach them the mathematics of parabolas when you kick something in the air. No. You say, here's a ball. Let's watch some people playing soccer. Okay, now we'll play soccer and then gradually over the following years, learn more and more so that you can get better and better at it. So this is kind of what we're trying to get you to do is to play soccer which in our case is to type code and look at the inputs and look at the outputs. 


このアプローチは、学習理論に関する多くの学術研究に基づいています。 特にハーバード大学のDavid Perkinsの一人が、この非常に素晴らしいアナロジーを持っています。 彼は学習理論の研究者です。 彼はサッカーをするように子供に教えているなら基本的に、ゲーム全体のこのアプローチを説明します、あなたはまずボールと草の間の摩擦がどのように働くかについて彼らに教えてからサッカーボールを見る方法を教えます 素手で、空中で何かを蹴ったときに放物線の数学を教えてください。 いいえ、ボールがあります。 サッカーをしている人を見ましょう。 さて、今、私たちはサッカーをし、その後次の年に徐々に徐々に、あなたがそれで良くなることができるように、ますます多くを学びます。 それで、これは私達があなたにさせようとしていることの一種は私達の場合私達の場合コードをタイプし、入力を見、そして出力を見ることであるサッカーをすることです。

## Teddy bear detector using Google Images [[16:21](https://youtu.be/Egp4Zajhzog?t=981)]
Google Imagesを使用したテディベア検出器

Let's dig into our first notebook which is called [lesson2-download.ipynb](https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson2-download.ipynb). What we are going to do is we are going to see how to create your own classifier with your own images. It's going to be a lot like last week's pet detector but it will detect whatever you like. So to be like some of those examples we just saw. How would you create your own Panama bus detector from scratch. This is approach is inspired by Adrian Rosebrock who has a terrific website called [pyimagesearch](https://www.pyimagesearch.com/) and he has this nice explanation of  [how to create a deep learning dataset using Google Images](https://www.pyimagesearch.com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/). So that was definitely an inspiration for some of the techniques we use here, so thank you to Adrian and you should definitely check out his site. It's full of lots of good resources.

[lesson2-download.ipynb]（https://github.com/fastai/course-v 3 / blob / master / nbs / dl1 / lesson2-download.ipynb）と呼ばれる最初のノートブックを掘り下げましょう。 私たちがやろうとしているのは、あなた自身の画像を使ってあなた自身の分類器を作成する方法を見ることです。 それは先週のペット探知機とよく似たものになるでしょうが、それはあなたが好きなものは何でも探知します。 それで私達が今見たそれらの例のいくつかのようになるように。 どのように最初からあなた自身のパナマバス探知機を作成するでしょうか。 これは、[pyimagesearch]（https://www.pyimagesearch.com/）という素晴らしいWebサイトを持っているAdrian Rosebrockに触発されたアプローチで、[Google Imagesを使用して詳細な学習データセットを作成する方法]（https ：//www.pyimagesearch.com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/）。 それで、これは我々がここで使用する技術のいくつかのための確かにインスピレーションでした、それでAdrianにあなたに感謝します、そしてあなたは間違いなく彼のサイトをチェックアウトするべきです。 それはたくさんの良いリソースでいっぱいです。

We are going to try to create a teddy bear detector. And we're going to separate teddy bears from black bears, from grizzly bears. This is very important. I have a three year old daughter and she needs to know what she's dealing with. In our house, you would be surprised at the number of monsters, lions, and other terrifying threats that are around particularly around Halloween. So we always need to be on the lookout to make sure that the things we're about to cuddle is in fact a genuine teddy bear. So let's deal with that situation as best as we can.

テディベア探知機を作ろうとしています。 そして、テディベアとブラックベア、グリズリーベアとを区別します。 これはとても重要です。 私は3歳の娘がいます、そして、彼女は彼女が何を扱っているかについて知る必要があります。 私たちの家では、特にハロウィーンのまわりにあるモンスター、ライオン、および他の恐ろしい脅威の数にあなたは驚かされるでしょう。 ですから、抱きしめようとしているものが実際に本物のテディベアであることを確認するために、常に注意を払う必要があります。 それでは、できる限りその状況に対処しましょう。

### Step 1: Gather URLs of each class of images
各クラスの画像のURLを収集する

Our starting point is to find some pictures of teddy bears so we can learn what they look like. So I go to  https://images.google.com/ and I type in Teddy bear and I just scroll through until I find a goodly bunch of them. Okay, that looks like plenty of teddy bears to me.

私たちの出発点はテディベアの写真を見つけることです。 それで私はhttps://images.google.com/に行き、私はテディベアをタイプインし、そして私がそれらのかなりの束を見つけるまで私はただスクロールします。 さて、それは私にたくさんのテディベアのように見えます。

Then I go back to [the notebook](https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson2-download.ipynb) and you can see it says "go to Google Images and search and scroll." The next thing we need to do is to get a list of all the URLs there. To do that, back in your google images, you hit <kbd>Ctrl</kbd><kbd>Shift</kbd><kbd>J</kbd> in Windows/Linux and <kbd>Cmd</kbd><kbd>Opt</kbd><kbd>J</kbd> in Mac, and you paste the following into the window that appears:

それから私は[ノートブック]（https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson2-download.ipynb）に戻ります、そして、あなたはそれが「グーグルのイメージに行き、 検索してスクロールします。」 次にやるべきことは、そこにあるすべてのURLのリストを取得することです。 そのためには、Googleの画像に戻って、Windows / Linuxでは<kbd> Ctrl + </ kb>> <kbd> Shift </ kb>> <kbd> J </kbd>を、<kbd> Cmd </kbd> <を押します。 Macでは、kbd> Opt </kbd> <kbd> J </kbd>の順にクリックして、表示されるウィンドウに次のコードを貼り付けます。

``` javascript
urls = Array.from(document.querySelectorAll('.rg_di .rg_meta')).map(el=>JSON.parse(el.textContent).ou);
window.open('data:text/csv;charset=utf-8,' + escape(urls.join('\n')));
```

![](lesson2/11.png)

This is a Javascript console for those of you who haven't done any Javascript before. I hit enter and it downloads my file for me. So I would call this teddies.txt and press "Save". Okay, now I have a file containing URLs of teddies. Then I would repeat that process for black bears and for grizzly bears, and I put each one in a file with an appropriate name. 

これはJavascriptを一度も経験したことがない皆さんのためのJavascriptコンソールです。 Enterを押すとファイルがダウンロードされます。 だから私はこのteddies.txtを呼んで「保存」を押すでしょう。 さて、今私はテディのURLを含むファイルを持っています。 それから私はツキノワグマとハイイログマのためにそのプロセスを繰り返すでしょう、そして私はそれぞれを適切な名前のファイルに入れます。

### Step 2: Download images [[19:39](https://youtu.be/Egp4Zajhzog?t=1179)]
画像のダウンロード

So step 2 is we now need to download those URLs to our server. Because remember when we're using Jupyter Notebook, it's not running on our computer. It's running on SageMaker or Crestle, or Google cloud, etc. So to do that, we start running some Jupyer cells. Let's grab the fastai library:

そのため、ステップ2では、これらのURLをサーバーにダウンロードする必要があります。 Jupyter Notebookを使用しているときは覚えているので、それは私たちのコンピュータでは動作していません。 SageMaker、Crestle、またはGoogleクラウドなどで実行されています。そのために、いくつかのJupyerセルを実行し始めます。 fastaiライブラリを入手しましょう。

```python
from fastai import *
from fastai.vision import *
```

And let's start with black bears. So I click on this cell for black bears and I'll run it. So here, I've got three different cells doing the same thing but different information. This is one way I like to work with Jupyter notebook. It's something that a lot of people with more strict scientific background are horrified by. This is not reproducible research. I click on the black bear cell, and run it to create a folder called black and a file called urls_black.txt for my black bears. I skip the next two cells.

クロクマから始めましょう。 だから私はツキノワグマのためにこのセルをクリックして、私はそれを実行します。 だからここで、私は3つの異なるセルが同じことをしているが異なる情報をしている。 これは私がJupyterノートブックを扱うのが好きな方法の1つです。 より厳密な科学的背景を持つ多くの人々が怖がっているということです。 これは再現可能な研究ではありません。 クロクマのセルをクリックし、それを実行してblackという名前のフォルダーとblack bears用のurls_black.txtというファイルを作成します。 次の2つのセルは飛ばします。

```python
folder = 'black'
file = 'urls_black.txt'
```

```python
folder = 'teddys'
file = 'urls_teddys.txt'
```

```python
folder = 'grizzly'
file = 'urls_grizzly.txt'
```



 Then I run this cell to create that folder.
それからこのセルを実行してそのフォルダを作成します。

```python
path = Path('data/bears')
dest = path/folder
dest.mkdir(parents=True, exist_ok=True)
```



Then I go down to the next section and I run the next cell which is download images for black bears. So that's just going to download my black bears to that folder. 

それから私は次のセクションに行き、私はツキノワグマのためのダウンロード画像である次のセルを走らせます。 だからそれはちょうどそのフォルダに私のクロクマをダウンロードするつもりです。

```python
classes = ['teddys','grizzly','black']
```

```python
download_images(path/file, dest, max_pics=200)
```



Now I go back and I click on `'teddys'`. And I scroll back down and repeat the same thing. That way, I'm just going backwards and forwards to download each of the classes that I want. Very manual but for me, I'm very iterative and very experimental, that works well for me. If you are better at planning ahead than I am, you can write a proper loop or whatever and do it that way. But when you see my notebooks and see things that are kind of like configuration cells (i.e. doing the same thing in different places), this is a strong sign that I didn't run this in order. I clicked one place, went to another, ran that. For me, I'm experimentalist. I really like to experiment in my notebook, I treat it like a lab journal, I try things out and I see what happens. So this is how my notebooks end up looking. 

今、私は戻って、私は「テディ」をクリックします。 そしてスクロールダウンして同じことを繰り返します。 そのようにして、私はただ行き来して、私が欲しいそれぞれのクラスをダウンロードしようとしています。 非常に手動ですが、私にとっては、非常に反復的で非常に実験的なので、うまくいきます。 あなたが私よりも前もって計画を立てることが得意であれば、あなたは適切なループか何かを書いてそれをそのようにすることができます。 しかし、私のノートブックを見て、設定セルのようなものを見たとき（つまり、同じことを別の場所で行っているとき）、これは私がこれを順番に実行しなかったことを示す強い兆候です。 私はある場所をクリックし、別の場所に行き、それを実行しました。 私にとっては、私は実験主義者です。 私は本当に自分のノートで実験するのが好きです、私はそれを実験室ジャーナルのように扱います、私は物事を試してみて、私は何が起こるか見ます。 だから私のノートはどうやって見に行くのです。

It's a really controversial topic. For a lot of people, they feel this is "wrong" that you should only ever run things top to bottom. Everything you do should be reproducible. For me, I don't think that's the best way of using human creativity. I think human creativity is best inspired by trying things out and seeing what happens and fiddling around. You can see how you go. See what works for you.

それは本当に物議をかもすトピックです。 多くの人にとって、彼らはこれが「間違っている」と感じています。 あなたがするすべては再現可能であるべきです。 私にとっては、それが人間の創造性を使う最善の方法だとは思いません。 人間の創造性は、物事を試して何が起こるのかを見て、周りをいじることに最も影響を受けていると思います。 あなたはどうやって行くのか見ることができます。 何がうまくいくのかをご覧ください。

So that will download the images to your server. It's going to use multiple processes to do so. One problem there is if something goes wrong, it's a bit hard to see what went wrong. So you can see in the next section, there's a commented out section that says `max_workers=0`. That will do it without spinning up a bunch of processes and will tell you the errors better. So if things aren't downloading, try using the second version. 


だからそれはあなたのサーバーに画像をダウンロードします。 そのためには複数のプロセスを使用します。 問題が1つあります。何かがうまくいかない場合、何がうまくいかなかったのかを見るのは少し難しいです。 次のセクションでわかるように、 `max_workers = 0`というコメントアウトされたセクションがあります。 それはたくさんのプロセスを回転させずにそれを行い、エラーをより良く伝えるでしょう。 ダウンロードできない場合は、2番目のバージョンを試してください。

```python
# If you have problems download, try with `max_workers=0` to see exceptions:
# download_images(path/file, dest, max_pics=20, max_workers=0)
```



### Step 3: Create ImageDataBunch [[22:50](https://youtu.be/Egp4Zajhzog?t=1370)]
データ集を作る

The next thing that I found I needed to do was to remove the images that aren't actually images at all. This happens all the time. There's always a few images in every batch that are corrupted for whatever reason. Google image told us this URL had an image but it doesn't anymore. So we got this thing in the library called `verify_images` which will check all of the images in a path and will tell you if there's a problem. If you say `delete=True`, it will actually delete it for you. So that's a really nice easy way to end up with a clean dataset. 

私がしなければならなかった次のことは、実際にはまったく画像ではない画像を削除することでした。 これは常に起こります。 何らかの理由で破損しているすべてのバッチには、常に少数のイメージがあります。 Googleの画像から、このURLに画像があるとの連絡がありましたが、もう表示されません。 そのため、パス内のすべての画像をチェックして問題があるかどうかを確認する「verify_images」というライブラリにこのことを入れました。 あなたが `delete = True`と言ったら、それは実際にあなたのためにそれを削除します。 それで、それはきれいなデータセットを得るための本当に素晴らしい簡単な方法です。

```python
for c in classes:
    print(c)
    verify_images(path/c, delete=True, max_workers=8)
```

![](lesson2/12.png)

So at this point, I now have a bears folder containing a grizzly folder, teddys folder, and black folder. In other words, I have the basic structure we need to create an ImageDataBunch to start doing some deep learning. So let's go ahead and do that.

それで、この時点で、私は今グリズリーフォルダー、teddysフォルダー、そして黒いフォルダーを含むbearsフォルダーを持っています。 言い換えれば、私はImageDataBunchを作成するために必要な基本構造を持っています。 それでは先に進みましょう。

Now, very often, when you download a dataset from like Kaggle or from some academic dataset, there will often be folders called train, valid, and test containing the different datasets. In this case, we don't have a separate validation set because we just grabbed these images from Google search. But you still need a validation set, otherwise you don't know how well your model is going and we'll talk more about this in a moment. 

さて、あなたがKaggleのような、あるいは何らかの学術的なデータセットからデータセットをダウンロードするとき、たいていの場合、train、valid、およびtestと呼ばれるフォルダーが異なるデータセットを含んでいるでしょう。 この場合、Google検索からこれらの画像を取得しただけなので、別の検証セットはありません。 しかし、まだ検証セットが必要です。そうでなければ、モデルがどの程度うまくいっているのかわからないので、この点については後で詳しく説明します。

Whenever you create a data bunch, if you don't have a separate training and validation set, then you can just say the training set is in the current folder (i.e. `.` because by default, it looks in a folder called `train`) and I want you to set aside 20% of the data, please. So this is going to create a validation set for you automatically and randomly. You'll see that whenever I create a validation set randomly, I always set my random seed to something fixed beforehand. This means that every time I run this code, I'll get the same validation set.  In general, I'm not a fan of making my machine learning experiments reproducible (i.e. ensuring I get exactly the same results every time). The randomness is to me a really important part of finding out your is solution stable and it is going to work each time you run it. But what is important is that you always have the same validation set. Otherwise when you are trying to decide has this hyper parameter change improved my model but you've got a different set of data you are testing it on, then you don't know maybe that set of data just happens to be a bit easier. So that's why I always set the random seed here.

データセットを作成するときはいつでも、別のトレーニングと検証セットを持っていなければ、トレーニングセットは現在のフォルダー（つまり、デフォルトでは `train`と呼ばれるフォルダーの中にあるため）にあると言えます。 そして、データの20％を取っておいてください。だからこれはあなたのために自動的にそしてランダムにバリデーションセットを作成しようとしています。私がランダムにバリデーションセットを作成するときはいつでも、私は常に私のランダムシードをあらかじめ固定されたものに設定することがわかります。つまり、このコードを実行するたびに、同じ検証セットが取得されます。一般的に、私は自分の機械学習実験を再現可能にすることには賛成ではありません（つまり、毎回まったく同じ結果が得られるようにすること）。無作為性は、あなたにとって安定したソリューションであることを知る上で非常に重要な部分であり、実行するたびにうまくいくでしょう。しかし重要なのは、あなたがいつも同じ検証セットを持っているということです。そうでなければ、このハイパーパラメータの変更によってモデルが改善されたと判断しようとしたときに、テストするデータのセットが異なる場合は、そのデータのセットが少し簡単になることがわかりません。だから私はいつもここにランダムシードを設定します。

```python
np.random.seed(42)
data = ImageDataBunch.from_folder(path, train=".", valid_pct=0.2,
        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)
```



[[25:37](https://youtu.be/Egp4Zajhzog?t=1537)]

We've now got a data bunch, so you can look inside at the `data.classes` and you'll see these are the folders that we created. So it knows that the classes (by classes, we mean all the possible labels) are black bear, grizzly bear, or teddy bear.

これでデータの束ができたので、内部で `data.classes`を見れば、これらが私たちが作成したフォルダであることがわかります。 それで、クラス（クラスによって、我々はすべての可能なラベルを意味する）がクロクマ、ハイイログマ、またはテディベアであることを知っています。

```python
data.classes
```

```
['black', 'grizzly', 'teddys']
```



We can run `show_batch` and take a little look. And it tells us straight away that some of these are going to be a little bit tricky.  Some of them are not photo, for instance. Some of them are cropped funny, if you ended up with a black bear standing on top of a grizzly bear, that might be tough. 

`show_batch`を実行して少し見てみましょう。 そして、これらのうちのいくつかは少しトリッキーになるだろうということをすぐに私たちに伝えます。 例えば、それらのいくつかは写真ではありません。 あなたがグリズリーベアの上に立っている黒い熊になったならば、それらのいくつかは変わってトリミングされます、それは難しいかもしれません。

```python
data.show_batch(rows=3, figsize=(7,8))
```

![](lesson2/bears.png)

You can kind of double check here. Remember, `data.c` is the attribute which the classifiers tell us how many possible labels there are. We'll learn about some other more specific meanings of `c` later. We can see how many things are now training set, how many things are in validation set. So we've got 473 training set, 140 validation set.  

ここで二重チェックをすることができます。 覚えておいてほしいのですが、 `data.c`は分類子が私たちに可能なラベルの数を教えてくれる属性です。 `c`の他のより具体的な意味については後で学びます。 現在トレーニングセットがいくつあるか、検証セットにあるものがいくつあるかがわかります。 だから我々は473トレーニングセット、140検証セットを持っています。

```python
data.classes, data.c, len(data.train_ds), len(data.valid_ds)
```

```
(['black', 'grizzly', 'teddys'], 3, 473, 140)
```



### Step 4: Training a model [[26:49](https://youtu.be/Egp4Zajhzog?t=1609)]
モデルを訓練する

So at that point, we can go ahead and create our convolutional neural network using that data. I tend to default to using a resnet34, and let's print out the error rate each time.

その時点で、そのデータを使用して先に進み、畳み込みニューラルネットワークを作成できます。 私はデフォルトでresnet34を使う傾向があり、毎回エラー率を表示しましょう。

```python
learn = create_cnn(data, models.resnet34, metrics=error_rate)
```

Then run `fit_one_cycle` 4 times and see how we go. And we have a 2% error rate. So that's pretty good. Sometimes it's easy for me to recognize a black bear from a grizzly bear, but sometimes it's a bit tricky. This one seems to be doing pretty well.  

それから `fit_one_cycle`を4回実行してください。 そして私達は2％の誤り率を持っています。 それで、それはかなり良いです。 グリズリーベアからクロクマを見分けるのは簡単なことではありませんが、ややトリッキーです。 これはかなりうまくいっているようです。

```python
learn.fit_one_cycle(4)
```

```
Total time: 00:54
epoch  train_loss  valid_loss  error_rate
1      0.710584    0.087024    0.021277    (00:14)
2      0.414239    0.045413    0.014184    (00:13)
3      0.306174    0.035602    0.014184    (00:13)
4      0.239355    0.035230    0.021277    (00:13)
```

After I make some progress with my model and things are looking good, I always like to save where I am up to to save me the 54 seconds of going back and doing it again. 

私のモデルをある程度進歩させて物事が良く見えた後は、54秒間戻ってまたやり直すことができるようにするために、常に自分の現在地を保存するのが好きです。

```python
learn.save('stage-1')
```

As per usual, we unfreeze the rest of our model. We are going to be learning more about what that means during the course. 

いつものように、私たちはモデルの残りの部分を凍結解除します。 私たちはコースの間にそれが何を意味するのかについてもっと学ぶつもりです。

```python
learn.unfreeze()
```

Then we run the learning rate finder and plot it (it tells you exactly what to type). And we take a look.

それから学習率ファインダーを実行してそれをプロットします（それは正確に何をタイプするべきかを教えてくれます）。 そして、私たちは見てみましょう。

```python
learn.lr_find()
```

```
LR Finder complete, type {learner_name}.recorder.plot() to see the graph.
```



We are going to be learning about learning rates today, but for now, here's what you need to know. On the learning rate finder, what you are looking for is the strongest downward slope that's kind of sticking around for quite a while. It's something you are going to have to practice with and get a feel for﹣which bit works. So if you are not sure which, try both learning rates and see which one works better. I've been doing this for a while and I'm pretty sure this (between 10^-5 and 10^-3) looks like where it's really learning properly, so I will probably pick something back here for my learning rate [[28:28](https://youtu.be/Egp4Zajhzog?t=1708)].

私たちは今日学習率について学びますが、今のところ、ここであなたが知る必要があるものです。 学習率ファインダーでは、あなたが探しているのは、かなり長い間固執するようなものである最強の下り坂です。 それはあなたが練習しなければならないことになっているものであり、そしてどのビットがうまくいくかについての感触を得ることです。 ですから、どちらがわからない場合は、両方の学習率を試して、どちらがより効果的かを確認してください。 私はしばらくこれをやっていて、これ（10 ^ -5と10 ^ -3の間）が本当に適切に学習されている場所のように見えると確信しているので、私はおそらく私の学習率のためにここで何かを選ぶでしょう[[ 28:28]（https://youtu.be/Egp4Zajhzog?t=1708）]。

```python
learn.recorder.plot()
```

![](lesson2/13.png)

So you can see, I picked `3e-5` for my bottom learning rate. For my top learning rate, I normally pick 1e-4 or 3e-4, it's kind of like I don't really think about it too much. That's a rule of thumb﹣it always works pretty well. One of the things you'll realize is that most of these parameters don't actually matter that much in detail. If you just copy the numbers that I use each time, the vast majority of the time, it'll just work fine. And we'll see places where it doesn't today.

あなたが見ることができるように、私は私の最低の学習率のために `3e-5`を選びました。 私の最高の学習率として、私は通常1e-4か3e-4を選びます、私はあまりそれについてあまり考えないようなものです。 これは経験則であり、常にうまく機能します。 理解できることの1つは、これらのパラメーターのほとんどは実際にはそれほど詳細には関係ないということです。 あなたが私が毎回使う数字を大部分コピーするだけなら、それはただうまくいくでしょう。 そして今日はそうでない場所を見るでしょう。

```python
learn.fit_one_cycle(2, max_lr=slice(3e-5,3e-4))
```

```
Total time: 00:28
epoch  train_loss  valid_loss  error_rate
1      0.107059    0.056375    0.028369    (00:14)
2      0.070725    0.041957    0.014184    (00:13)
```

So we've got 1.4% error rate after doing another couple of epochs, so that's looking great. So we've downloaded some images from Google image search, created a classifier, and we've got 1.4% error rate, let's save it.

それで私達は別の2、3のエポックをした後に1.4％のエラー率を得ました、それでそれは素晴らしく見えます。 そのため、Googleの画像検索からいくつかの画像をダウンロードし、分類子を作成しました。エラー率は1.4％です。保存してみましょう。

```python
learn.save('stage-2')
```



### Interpretation [[29:38](https://youtu.be/Egp4Zajhzog?t=1778)]
対話

As per usual, we can use the ClassificationInterpretation class to have a look at what's going on.

いつものように、Classification Interpretationクラスを使って何が起こっているのかを見ることができます。

```python
learn.load('stage-2')
```

```python
interp = ClassificationInterpretation.from_learner(learn)
```

```python
interp.plot_confusion_matrix()
```

![](lesson2/14.png)

In this case, we made one mistake. There was one black bear classified as grizzly bear. So that's a really good step. We've come a long way. But possibly you could do even better if your dataset was less noisy. Maybe Google image search didn't give you exactly the right images all the time. So how do we fix that? We want to clean it up. So combining human expert with a computer learner is a really good idea. Very very few people publish on this or teach this, but to me, it's the most useful skill, particularly for you. Most of the people watching this are domain experts, not computer science experts, so this is where you can use your knowledge of point mutations in genomics or Panamanian buses or whatever. So let's see how that would work. What I'm going to do is, do you remember the plot top losses from last time where we saw the images which it was either the most wrong about or the least confident about. We are going to look at those and decide which of those are noisy. If you think about it, it's very unlikely that if there is a mislabeled data that it's going to be predicted correctly and with high confidence. That's really unlikely to happen. So we're going to focus on the ones which the model is saying either it's not confident of or it was confident of and it was wrong about. They are the things which might be mislabeled. 

この場合、1つ間違いをしました。グリズリーベアーとして分類された1つのクロクマがありました。だからそれは本当に良い一歩です。私達は長い道のりを歩んできました。しかし、おそらくあなたのデータセットがそれほどうるさくないならば、あなたはさらに良いことができるでしょう。 Googleの画像検索では、常に正しい画像が表示されなかった可能性があります。それでは、どのように修正しますか。それを片付けたいです。そのため、人間の専門家とコンピュータの学習者を組み合わせることは非常に良い考えです。これを公開したり教えたりする人はほとんどいませんが、私にとっては、これは特にあなたにとって最も有用なスキルです。これを見ている人たちの大部分はコンピュータサイエンスの専門家ではなくドメインの専門家なので、ここでゲノミクスやパナマのバスなどで点突然変異に関するあなたの知識を使うことができます。それではそれがどのように機能するか見てみましょう。私がやろうとしているのは、あなたが前回それが最も間違っていたか最も自信がなかったかのどちらかの画像を見たときのプロットトップの損失を覚えているか、です。私達はそれらを見て、それらのどれが騒々しいかを決めるつもりです。あなたがそれについて考えるならば、誤ったラベルが付けられたデータがあるならば、それが正しくそして高い信頼性をもって予測されることになるということは非常にありそうもないです。それは起こりそうもないです。それで、我々はモデルがそれが確信していないか、それが確信していたかについて間違っていると言っているものに焦点を合わせるつもりです。それらは誤ったラベルを付けられるかもしれないものです。

A big shout-out to the San Francisco fastai study group who created this new widget this week called the FileDeleter. Zach, Jason, and Francisco built this thing where we basically can take the top losses from that interpretation object we just created. There is not just `plot_top-losses` but there's also `top_losses` and top_losses returns two things: the losses of the things that were the worst and the indexes into the dataset of the things that were the worst. If you don't pass anything at all, it's going to actually return the entire dataset, but sorted so the first things will be the highest losses. Every dataset in fastai has `x` and `y` and the `x` contains the things that are used to, in this case, get the images. So this is the image file names and the `y`'s will be the labels. So if we grab the indexes and pass them into the dataset's `x`, this is going to give us the file names of the dataset ordered by which ones had the highest loss (i.e. which ones it was either confident and wrong about or not confident about). So we can pass that to this new widget that they've created.

今週この新しいウィジェットを作成したサンフランシスコのfastai研究グループへの大きなコメントはFileDeleterと呼ばれています。ザック、ジェイソン、そしてフランシスコは、私たちが今作成したその解釈オブジェクトからトップロスを取ることができるところでこのことを作りました。 `plot_top-loss`だけではなく、` top_losses`もあります。top_lossesは2つのことを返します：最悪のものの損失と最悪のもののデータセットへのインデックスです。何も渡さないと、実際にはデータセット全体が返されますが、ソートされて最初のものが最大の損失になります。 fastaiのすべてのデータセットには `x`と` y`があり、 `x`にはこの場合は画像を取得するために使用されるものが含まれています。これが画像ファイルの名前で、 `y`がラベルになります。したがって、インデックスを取得してデータセットの `x`に渡すと、データセットのファイル名が最も損失の大きかった順（つまり、どのファイルが自信を持っているのか、間違っているのか、自信がないのか）で示されます。約）。それで、それを彼らが作ったこの新しいウィジェットに渡すことができます。

Just to clarify, this `top_loss_paths` contains all of the file names in our dataset. When I say "out dataset", this particular one is our validation dataset. So what this is going to do is it's going to clean up mislabeled images or images that shouldn't be there and we're going to remove them from a validation set so that our metrics will be more correct. You then need to rerun these two steps replacing `valid_ds` with `train_ds` to clean up your training set to get the noise out of that as well. So it's a good practice to do both. We'll talk about test sets later as well, if you also have a test set, you would then repeat the same thing. 

わかりやすくするために、この `top_loss_paths`にはデータセット内のすべてのファイル名が含まれています。 私が "out dataset"と言うとき、これは私たちの検証データセットです。 そのため、ラベル付けされていないイメージや存在しないはずのイメージを整理して、メトリックがより正確になるように検証セットから削除します。 それから、ノイズを除去するためにトレーニングセットをクリーンアップするために `valid_ds`を` train_ds`に置き換えてこれら2つのステップを再実行する必要があります。 だから両方をするのは良い習慣です。 テストセットについても後で説明します。テストセットもある場合は、同じことを繰り返します。

```python
from fastai.widgets import *

losses,idxs = interp.top_losses()
top_loss_paths = data.valid_ds.x[idxs]
```

```python
fd = FileDeleter(file_paths=top_loss_paths)
```

![](lesson2/16.png)

So we run FileDeleter passing in that sorted list of paths and so what pops up is basically the same thing as `plot_top_losses`. In other words, these are the ones which is either wrong about or least confident about. So not surprisingly, this one her (the second from left) does not appear to be a teddy bear, black bear, or grizzly bear. So this shouldn't be in our dataset. So what I do is I wack on the delete button, all the rest do look indeed like bears, so I can click confirm and it'll bring up another five. 

そのため、FileDeleterを実行して、ソートされたパスのリストを渡し、ポップアップ表示されるものは基本的に `plot_top_losses`と同じものになります。 言い換えれば、これらは間違っているか、少なくとも自信がないものです。 だから驚くことではないが、この彼女（左から二番目）はテディベア、クロクマ、グリズリーベアーではないようだ。 したがって、これは私たちのデータセットには含まれません。 それで私がするのは、削除ボタンをクリックすることです。残りはすべてクマのように見えるので、確認をクリックするとさらに5つ表示されます。

What I tend to do when I do this is I'll keep going confirm until I get to a coupe of screen full of the things that all look okay and that suggests to me that I've got past the worst bits of the data. So that's it so now you can go back for the training set as well and retrain your model. 

私がこれをするときにする傾向があるのは私がすべてが大丈夫に見えることの完全なスクリーンのクーペに達するまで私が確認し続けることであり、それはデータの最悪の部分を過ぎたことを私に示唆する。 それで、これでトレーニングセットに戻ってモデルを再トレーニングできるようになりました。

I'll just note here that what our San Francisco study group did here was that they actually built a little app inside Jupyter notebook which you might not have realized as possible. But not only is it possible, it's actually surprisingly straightforward. Just like everything else, you can hit double question mark to find out their secrets. So here is the source code. 

私たちのサンフランシスコの研究グループがここでやったことは、彼らがJupyterのノートブックの中に実際にはちょっとしたアプリを作ったということです。 しかし、それが可能であるだけでなく、実際には驚くほど簡単です。 他のすべてのものと同様に、あなたは彼らの秘密を見つけるために二重の疑問符を押すことができます。 だからここにソースコードがあります。

![](lesson2/17.png)

Really, if you've done any GUI programming before, it'll look incredibly normal. There's basically call backs for what happens when you click on a button where you just do standard Python things and to actually render it, you just use widgets and you can lay it out using standard boxes. So this idea of creating applications inside notebooks is really underused but it's super neat because it lets you create tools for your fellow practitioners or experimenters. And you could definitely envisage taking this a lot further. In fact, by the time you're watching this on the MOOC, you will probably find that there's a whole a lot more buttons here because we've already got a long list of to-do that we're going to add to this particular thing. 

実際、以前にGUIプログラミングをしたことがあるなら、それは信じられないほど普通に見えるでしょう。 基本的にPythonの標準的なことをしているところでボタンをクリックしたときに何が起こるのかについてのコールバックがあります。実際にそれをレンダリングするには、ウィジェットを使うだけで標準のボックスを使ってレイアウトできます。 それで、ノートブックの中にアプリケーションを作成するというこの考えは実際には十分に活用されていませんが、それはあなたがあなたの同僚の実務家または実験者のためのツールを作成することを可能にするのでとても素晴らしいです。 そして、あなたは間違いなくこれをもっともっと進めることを想像することができます。 実際、MOOCでこれを見ているときには、これに追加する予定の長いリストがすでにあるので、ここにはもっとたくさんのボタンがあることがわかるでしょう。 特別なこと。

I'd love for you to have a think about, now that you know it's possible to write applications in your notebook, what are you going to write and if you google for "[ipywidgets](https://ipywidgets.readthedocs.io/en/stable/)", you can learn about the little GUI framework to find out what kind of widgets you can create, what they look like, and how they work, and so forth. You'll find it's actually a pretty complete GUI programming environment you can play with. And this will all work nice with your models. It's not a great way to productionize an application because it is sitting inside a notebook. This is really for things which are going to help other practitioners or experimentalists. For productionizing things, you need to actually build a production web app which we will look at next. 

あなたのノートブックにアプリケーションを書くことが可能であることを知っているので、あなたが考えてみることを望みます。 / en / stable /） "、どんな種類のウィジェットを作成できるのか、それらがどのように見えるのか、そしてそれらがどのように動作するのかなどを見つけるためにあなたは小さなGUIフレームワークについて学ぶことができます。 あなたはそれが実際にあなたが遊ぶことができるかなり完全なGUIプログラミング環境であることがわかります。 そしてこれはすべてあなたのモデルでうまくいくでしょう。 アプリケーションはノートブックの中にあるので、アプリケーションを制作するのには良い方法ではありません。 これは本当に他の実務家や実験者を助けようとしているもののためのものです。 ものを生産するためには、実際に私達が次に見る生産Webアプリを構築する必要があります。

### Putting your model in production [[37:36](https://youtu.be/Egp4Zajhzog?t=2256)]

モデルを生産に投入する

After you have cleaned up your noisy images, you can then retrain your model and hopefully you'll find it's a little bit more accurate. One thing you might be interested to discover when you do this is it actually doesn't matter most of the time very much. On the whole, these models are pretty good at dealing with moderate amounts of noisy data. The problem would occur is if your data was not randomly noisy but biased noisy. So I guess the main thing I'm saying is if you go through this process of cleaning up your data and then rerun your model and find it's .001% better, that's normal. It's fine. But it's still a good idea just to make sure that you don't have too much noise in your data in case it is biased.

ノイズの多い画像をクリーンアップした後は、モデルを再トレーニングすることで、もう少し正確になることを願います。 あなたがこれをするとき発見することに興味があるかもしれない1つの事はそれが実際にはほとんど問題ではないということです。 概して、これらのモデルは、適度な量のノイズの多いデータを扱うのが得意です。 問題は、データがランダムにノイズが多いのではなく、バイアスが多いのが原因で発生します。 だから私が言っている主なことはあなたがあなたのデータをクリーンアップするというこのプロセスを経て、それからあなたのモデルを再実行してそれが.001％良いことを見つけたなら、それは普通です。 大丈夫だよ。 しかし、データに偏りがある場合に備えて、データにノイズが多すぎないようにすることをお勧めします。

At this point, we're ready to put our model in production and this is where I hear a lot of people ask me about which mega Google Facebook highly distributed serving system they should use and how do they use a thousand GPUs at the same time. For the vast majority of things you all do, you will want to actually run in production on a CPU, not a GPU. Why is that? Because GPU is good at doing lots of things at the same time, but unless you have a very busy website, it's pretty unlikely that you're going to have 64 images to classify at the same time to put into a batch into a GPU. And if you did, you've got to deal with all that queuing and running it all together, all of your users have to wait until that batch has got filled up and run﹣it's whole a lot of hassle. Then if you want to scale that, there's another whole lot of hassle. It's much easier if you just wrap one thing, throw it at a CPU to get it done, and comes back again. Yes, it's going to take maybe 10 or 20 times longer so maybe it'll take 0.2 seconds rather than 0.01 seconds. That's about the kind of times we are talking about. But it's so easy to scale. You can chuck it on any standard serving infrastructure. It's going to be cheap, and you can horizontally scale it really easily.  So most people I know who are running apps that aren't at Google scale, based on deep learning are using CPUs. And the term we use is "inference". When you are not training a model but you've got a trained model and you're getting it to predict things, we call that inference. That's why we say here:

現時点で、私たちはモデルを生産する準備ができています、そしてここで私は多くの人々が彼らが使うべきである巨大なグーグルフェイスブック高度に分配されたサービングシステムと彼らが同時にどうやって1000のGPUを使うかについて私に尋ねるところです。あなたがしていることの大部分は、実際にはGPUではなくCPU上で実稼働環境で実行したいと思うでしょう。何故ですか？ GPUは多くのことを同時に行うのが得意ですが、非常に忙しいWebサイトを持っていない限り、GPUにまとめて分類するために64個の画像を分類することはほとんどありません。そして、そうした場合、あなたはそれらすべての待ち行列に対処してまとめて実行しなければなりません。すべてのユーザーは、そのバッチがいっぱいになって実行されるまで待たなければなりません。それであなたがそれをスケールしたいならば、面倒なことがもう一つたくさんあります。 1つのことをラップし、それをCPUで実行してやり直すと、はるかに簡単です。はい、それはおそらく10または20倍長い時間がかかるだろうので多分それは0.01秒ではなく0.2秒かかるでしょう。それは私たちが話しているような時代についてです。しかし、スケール変更はとても簡単です。あなたはどんな標準的なサービングインフラストラクチャーでそれをチャックすることができます。それは安くなるでしょう、そしてあなたは本当にそれを水平に拡大縮小することができます。そのため、ディープラーニングに基づいて、Googleスケールではないアプリケーションを実行していると私が知っているほとんどの人はCPUを使用しています。そして私たちが使う言葉は「推論」です。モデルを訓練していないが訓練済みのモデルを持っていて、物事を予測するためにそれを得ているとき、私たちはその推論を呼び出します。だからこそ、ここで言うのです。


> You probably want to use CPU for inference
>おそらく推論にCPUを使いたい


At inference time, you've got your pre-trained model, you saved those weights, and how are you going to use them to create something like Simon Willison's cougar detector?

推論時には、事前に訓練されたモデルを入手し、それらの重みを保存しました。そして、Simon Willisonのクーガー検出器のようなものを作成するためにそれらをどのように使用しますか？

The first thing you're going to need to know is what were the classes that you trained with. You need to know not just what are they but what were the order. So you will actually need to serialize that or just type them in, or in some way make sure you've got exactly the same classes that you trained with. 

最初に知っておく必要があるのは、あなたがトレーニングしたクラスは何でしたか。 あなたは彼らが何であるかだけでなく、何が注文であったかを知る必要があります。 ですから、実際にはそれを直列化するか、単にタイプするか、あるいは何らかの方法で自分が訓練したクラスとまったく同じクラスを持っていることを確認する必要があります。

```python
data.classes
```

```
['black', 'grizzly', 'teddys']
```

If you don't have a GPU on your server, it will use the CPU automatically. If you have a GPU machine and you want to test using a CPU, you can just uncomment this line and that tells fastai that you want to use CPU by passing it back to PyTorch.  

サーバーにGPUがない場合は、CPUを自動的に使用します。 もしあなたがGPUマシンを持っていてCPUを使ってテストしたいのであれば、この行のコメントを外すだけでそれをPyTorchに渡してCPUを使いたいことをfastaiに伝えます。

```python
# fastai.defaults.device = torch.device('cpu')
```



[[41:14](https://youtu.be/Egp4Zajhzog?t=2474)]

So here is an example. We don't have a cougar detector, we have a teddy bear detector. And my daughter Claire is about to decide whether to cuddle this friend. What she does is she takes daddy's deep learning model and she gets a picture of this and here is a picture that she's uploaded to the web app and here is a picture of the potentially cuddlesome object. We are going to store that in a variable called `img` , and open_image is how you open an image in fastai, funnily enough.

だからここに例があります。 クーガー探知機はありません、テディベア探知機があります。 そして私の娘クレアはこの友人を抱きしめるかどうかを決定しようとしています。 彼女がしていることは彼女がパパの深い学習モデルを取り、彼女がこれの写真を撮ることです。 私たちはそれを `img`と呼ばれる変数に格納しようとしています、そしてopen_imageは画像をfastaiで開く方法です。

```python
img = open_image(path/'black'/'00000021.jpg')
img
```

![](lesson2/bear.png)

Here is that list of classes that we saved earlier. And as per usual, we created a data bunch, but this time, we're not going to create a data bunch from a folder full of images, we're going to create a special kind of data bunch which is one that's going to grab one single image at a time. So we're not actually passing it any data. The only reason we pass it a path is so that it knows where to load our model from. That's just the path that's the folder that the model is going to be in. 

これは、以前に保存したクラスのリストです。 そしていつもどおり、データの束を作成しましたが、今回は、画像でいっぱいのフォルダーからデータの束を作成するのではなく、特別な種類のデータの束を作成します。 一度に1つの画像をつかみます。 そのため、実際にはデータを渡していません。 パスを渡す唯一の理由は、モデルのロード元がわかるようにするためです。 これは、モデルが配置されるフォルダのパスです。

But what we need to do is that we need to pass it the same information that we trained with. So the same transforms, the same size, the same normalization. This is all stuff we'll learn more about. But just make sure it's the same stuff that you used before. 

しかし、私たちがする必要があるのは、私たちが訓練したのと同じ情報を渡す必要があるということです。 だから、同じ変換、同じサイズ、同じ正規化。 これですべてのことがわかります。 ただし、それが以前に使用したものと同じものであることを確認してください。

Now you've got a data bunch that actually doesn't have any data in it at all. It's just something that knows how to transform a new image in the same way that you trained with so that you can now do inference. 

これで、データがまったく含まれていないデータ束ができました。 それはあなたが今あなたが推論をすることができるようにあなたが訓練したのと同じ方法で新しい画像を変換する方法を知っていることだけです。

You can now `create_cnn` with this kind of fake data bunch and again, you would use exactly the same model that you trained with. You can now load in those saved weights. So this is the stuff that you only do once﹣just once when your web app is starting up. And it takes 0.1 of a second to run this code.

このような偽のデータ束を使って `create_cnn`を実行することができます。また、訓練したモデルとまったく同じモデルを使用することになります。 これで、保存した重みを読み込むことができます。 そのため、これは、Webアプリが起動したときに1回だけ実行することです。 そしてこのコードを実行するのに0.1秒かかります。

```python
classes = ['black', 'grizzly', 'teddys']
data2 = ImageDataBunch.single_from_classes(path, classes, tfms=get_transforms(), size=224).normalize(imagenet_stats)
learn = create_cnn(data2, models.resnet34)
learn.load('stage-2')
```

Then you just go `learn.predict(img)` and it's lucky we did that because it's not a teddy bear. This is actually a black bear. So thankfully due to this excellent deep learning model, my daughter will avoid having a very embarrassing black bear cuddle incident. 

それからあなたはただ `learn.predict（img）`に行き、それがテディベアではないので我々がそれをしたのはラッキーです。 これは実際にはツキノワグマです。 とてもありがたいことに、この優れたディープラーニングモデルのおかげで、私の娘は非常に厄介なツキノワグマの抱擁事件を避けることになります。

```python
pred_class,pred_idx,outputs = learn.predict(img)
pred_class
```

```
'black'
```

So what does this look like in production? I took [Simon Willison's code](https://github.com/simonw/cougar-or-not), shamelessly stole it, made it probably a little bit worse, but basically it's going to look something like this. Simon used a really cool web app toolkit called [Starlette](https://www.starlette.io/). If you've ever used Flask, this will look extremely similar but it's kind of a more modern approach﹣by modern what I really mean is that you can use `await` which is basically means that you can wait for something that takes a while, such as grabbing some data, without using up a process. So for things like I want to get a prediction or I want to load up some data, it's really great to be able to use this modern Python 3 asynchronous stuff. So Starlette could come highly recommended for creating your web app.   

それで、これはプロダクションでどのように見えますか？ 私は[Simon Willisonのコード]（https://github.com/simonw/cougar-or-not）を取って、それを恥ずべきに盗んで、おそらくもう少し悪くしましたが、基本的にはこのようになります。 Simonは[Starlette]（https://www.starlette.io/）という本当にクールなWebアプリツールキットを使いました。 Flaskを使ったことがあるなら、これは非常に似ているように見えるでしょうが、それはもっと近代的なアプローチのようなものです - 現代では、awaitを使うことができるということです。 プロセスを使い果たすことなく、データを取得するなど。 それで、私が予測を得たい、あるいはデータをロードしたいなどのために、このモダンなPython 3非同期のものを使うことができるのは本当に素晴らしいです。 そのため、StarletteはWebアプリケーションの作成に強く推奨される可能性があります。

You just create a route as per usual, in that you say this is `async` to ensure it doesn't steal the process while it's waiting for things. 

通常通りにルートを作成するだけです。つまり、物事を待っている間にプロセスが盗まれないようにするために、これは「非同期」であるということです。

You open your image you call `learner.predict`  and you return that response. Then you can use Javascript client or whatever to show it. That's it. That's basically the main contents of your web app.  

あなたは「learner.predict」と呼ぶあなたの画像を開き、そしてあなたはその応答を返します。 それからあなたはそれを示すためにJavascriptクライアントか何かを使うことができます。 それでおしまい。 それが基本的にあなたのWebアプリの主な内容です。

```python
@app.route("/classify-url", methods=["GET"])
async def classify_url(request):
    bytes = await get_bytes(request.query_params["url"])
    img = open_image(BytesIO(bytes))
    _,_,losses = learner.predict(img)
    return JSONResponse({
        "predictions": sorted(
            zip(cat_learner.data.classes, map(float, losses)),
            key=lambda p: p[1],
            reverse=True
        )
    })
```

So give it a go this week. Even if you've never created a web application before, there's a lot of nice little tutorials online and kind of starter code, if in doubt, why don't you try Starlette. There's a free hosting that you can use, there's one called [PythonAnywhere](https://www.pythonanywhere.com/), for example. The one Simon has used, [Zeit Now](https://zeit.co/now), it's something you can basically package it up as a docker thing and shoot it off and it'll serve it up for you. So it doesn't even need to cost you any money and all these classifiers that you're creating, you can turn them into web application. I'll be really interested to see what you're able to make of that. That'll be really fun. 

それで、今週それをやってみましょう。 これまでにWebアプリケーションを作成したことがない場合でも、オンラインでたくさんの素晴らしいチュートリアルやスターターコードがあります。疑わしい場合は、Starletteを試してみませんか。 あなたが使うことができる無料のホスティングがあります、例えば[PythonAnywhere]（https://www.pythonanywhere.com/）と呼ばれるものがあります。 Simonが使ったもの[Zeit Now]（https://zeit.co/now）は、基本的にそれをdockerのものとしてパッケージ化して撃つことができ、それがあなたのために役立つでしょう。 ですから、あなたがお金とあなたが作成しているこれらの分類子すべてにあなたにお金をかける必要さえもない、あなたはそれらをウェブアプリケーションに変えることができます。 私はあなたがそれで何ができるかを見ることに本当に興味があるでしょう。 それは本当に楽しいでしょう。

https://course-v3.fast.ai/deployment_zeit.html

### Things that can go wrong [[46:06](https://youtu.be/Egp4Zajhzog?t=2766)]
うまくいかないことがあるもの

I mentioned that most of the time, the kind of rules of thumb I've shown you will probably work. And if you look at the share your work thread, you'll find most of the time, people are posting things saying I downloaded these images, I tried this thing, they worked much better than I expected, well that's cool. Then like 1 out of 20 says I had a problem. So let's have a talk about what happens when you have a problem. This is where we start getting into a little bit of theory because in order to understand why we have these problems and how we fix them, it really helps to know a little bit about what's going on.

私は、ほとんどの場合、私があなたに示した種類の経験則がおそらくうまくいくだろうと述べました。 そして、仕事用スレッドのシェアを見ると、ほとんどの場合、人々は私がこれらの画像をダウンロードしたと言っているものを投稿しています。私はこれを試してみました。 それから20のうちの1が私が問題を抱えていたと言うように。 それでは、問題が発生したときに何が起こるかについて話しましょう。 なぜ私たちがこれらの問題を抱えているのか、そしてどのようにしてそれらを直すのかを理解するためには、何が起こっているのかを少し知ることが本当に助けになるからです。

First of all, let's look at examples of some problems. The problems basically will be either:

まず最初に、いくつかの問題の例を見てみましょう。 問題は基本的にどちらかになります。


- Your learning rate is too high or low
- Your number of epochs is too high or low 

 - 学習率が高すぎるか低すぎる
 - あなたのエポック数が多すぎるか少なすぎる
 
So we are going to learn about what those mean and why they matter. But first of all, because we are experimentalists, let's try them. 

だから我々はそれらが何を意味するのか、そしてなぜそれらが重要なのかについて学ぶつもりです。 しかし、まず最初に、私たちは実験主義者なので、試してみましょう。

#### Learning rate (LR) too high
高い学習率

So let's grow with our teddy bear detector and let's make our learning rate really high. The default learning rate is 0.003 that works most of the time. So what if we try a learning rate of 0.5. That's huge. What happens? Our validation loss gets pretty darn high. Remember, this is something that's normally something underneath 1. So if you see your validation loss do that, before we even learn what validation loss is, just know this, if it does that, your learning rate is too high. That's all you need to know. Make it lower. Doesn't matter how many epochs you do. If this happens, there's no way to undo this. You have to go back and create your neural net again and fit from scratch with a lower learning rate.

それでは、テディベアディテクタを使って成長し、学習率を本当に高くしましょう。 デフォルトの学習率は0.003で、ほとんどの場合有効です。 0.5の学習率を試すとどうなりますか。 それは巨大です。 何が起こるのですか？ 私たちの検証損失はかなり高くなります。 検証損失があるかどうかを確認する前に、検証損失が何であるかを学習する前に、学習率が高すぎることを確認してください。 あなたが知る必要があるのはそれだけです。 低くしてください。 あなたが何エポックするかは関係ありません。 これが発生した場合、これを元に戻す方法はありません。 あなたは戻って再びあなたのニューラルネットを作り、そしてより低い学習率で最初からフィットしなければなりません。

```python
learn = create_cnn(data, models.resnet34, metrics=error_rate)
```

```python
learn.fit_one_cycle(1, max_lr=0.5)
```

```
Total time: 00:13
epoch  train_loss  valid_loss  error_rate       
1      12.220007   1144188288.000000  0.765957    (00:13)
```



#### Learning rate (LR) too low [[48:02](https://youtu.be/Egp4Zajhzog?t=2882)]
低い学習率

What if we used a learning rate not of 0.003 but 1e-5 (0.00001)? 

0.003ではなく1e-5（0.00001）の学習率を使用した場合はどうなりますか

```python
learn = create_cnn(data, models.resnet34, metrics=error_rate)
```



This is just copied and pasted what happened when we trained before with a default learning rate:

これは、デフォルトの学習率で以前にトレーニングしたときに起こったことをコピーして貼り付けるだけです。

```
Total time: 00:57
epoch  train_loss  valid_loss  error_rate
1      1.030236    0.179226    0.028369    (00:14)
2      0.561508    0.055464    0.014184    (00:13)
3      0.396103    0.053801    0.014184    (00:13)
4      0.316883    0.050197    0.021277    (00:15)
```

And within one epoch, we were down to a 2 or 3% error rate.

1エポック以内に、エラー率は2または3％に低下しました。

With this really low learning rate, our error rate does get better but very very slowly. 

この本当に低い学習率で、私たちのエラー率は良くなりますが、非常にゆっくりとします。

```python
learn.fit_one_cycle(5, max_lr=1e-5)
```

```
Total time: 01:07
epoch  train_loss  valid_loss  error_rate
1      1.349151    1.062807    0.609929    (00:13)
2      1.373262    1.045115    0.546099    (00:13)
3      1.346169    1.006288    0.468085    (00:13)
4      1.334486    0.978713    0.453901    (00:13)
5      1.320978    0.978108    0.446809    (00:13)
```

And you can plot it. So `learn.recorder`  is an object which is going to keep track of lots of things happening while you train. You can call `plot_losses` to plot out the validation and training loss. And you can just see them gradually going down so slow. If you see that happening, then you have a learning rate which is too small. So bump it by 10 or bump it up by 100 and try again. The other thing you see if your learning rate is too small is that your training loss will be higher than your validation loss. You never want a model where your training loss is higher than your validation loss. That always means you haven't fitted enough which means either your learning rate is too low or your number of epochs is too low. So if you have a model like that, train it some more or train it with a higher learning rate. 

そしてあなたはそれをプロットすることができます。 そのため `learn.recorder`は訓練中に起こっている多くのことを追跡するオブジェクトです。 検証とトレーニングの損失をプロットするために `plot_losses`を呼び出すことができます。 そして、あなたはそれらがゆっくりとゆっくり落ちるのを見ることができます。 あなたがそれが起こっているのを見るならば、あなたは小さすぎる学習率を持っています。 だから、それを10まで上げるか、100まで上げて、もう一度試してください。 あなたの学習率が小さすぎる場合にあなたが見るもう一つのことはあなたのトレーニングの損失があなたの検証の損失よりも高くなるということです。 あなたはあなたのトレーニングの損失があなたの検証の損失よりも高いモデルを望んでいません。 それはいつもあなたが十分にフィットしていないことを意味します、それはあなたの学習率が低すぎるかあなたのエポック数が低すぎることを意味します。 あなたがそのようなモデルを持っているのであれば、もう少しそれを訓練するか、より高い学習率でそれを訓練してください。

```python
learn.recorder.plot_losses()
```

![](lesson2/15.png)



As well as taking a really long time, it's getting too many looks at each image, so may overfit.

非常に長い時間がかかるだけでなく、各画像を見ている回数が多くなり過ぎているため、オーバーフィットする可能性があります。

#### Too few epochs [[49:42](https://youtu.be/Egp4Zajhzog?t=2982)]
少ないエポック

What if we train for just one epoch? Our error rate is certainly better than random, 5%. But look at this, the difference between training loss and validation loss ﹣ a training loss is much higher than the validation loss. So too few epochs and too lower learning rate look very similar. So you can just try running more epochs and if it's taking forever, you can try a higher learning rate. If you try a higher learning rate and the loss goes off to 100,000 million, then put it back to where it was and try a few more epochs. That's the balance. That's all you care about 99% of the time. And this is only the 1 in 20 times that the defaults don't work for you.


私たちがたった1つの時代のために訓練するならば、どうですか？ 私達のエラー率は確かにランダムより5％優れています。 しかし、これを見てください。トレーニングの損失と検証の損失の違い - トレーニングの損失は検証の損失よりはるかに大きいです。 そのため、エポック数が少なすぎて学習率が低すぎる場合も、よく似ています。 ですから、もっとエポックを実行してみることができますし、それが永遠に進んでいる場合は、より高い学習率を試すことができます。 あなたがもっと高い学習率を試してみて、その損失が1億ドルになったら、それを元の位置に戻して、もう少しエポックを試してください。 それがバランスです。 それがあなたが99％の時間について気にしているすべてです。 そしてこれはデフォルトがあなたのために働かないことを20回に1回だけです。

```python
learn = create_cnn(data, models.resnet34, metrics=error_rate, pretrained=False)
```

```python
learn.fit_one_cycle(1)
```

```
Total time: 00:14
epoch  train_loss  valid_loss  error_rate
1      0.602823    0.119616    0.049645    (00:14)
```



#### Too many epochs [[50:30](https://youtu.be/Egp4Zajhzog?t=3030)]
エポックが多すぎる

Too many epochs create something called "overfitting". If you train for too long as we're going to learn about it, it will learn to recognize your particular teddy bears but not teddy bears in general. Here is the thing. Despite what you may have heard, it's very hard to overfit with deep learning. So we were trying today to show you an example of overfitting and I turned off everything. I turned off all the data augmentation, dropout, and weight decay. I tried to make it overfit as much as I can. I trained it on a small-ish learning rate, I trained it for a really long time. And maybe I started to get it to overfit. Maybe. 

エポックが多すぎると「オーバーフィット」と呼ばれるものが作成されます。 我々がそれについて学ぶつもりであるのであなたがあまりにも長く訓練するならば、それはあなたの特定のテディベアを認識することを学ぶが、一般にテディベアを認識しないであろう。 これが事です。 あなたが聞いたことがあるかもしれないにもかかわらず、それは徹底的な学習にあふれるのは非常に難しいです。 それで私達は今日あなたに過剰装備の例を見せようとしていました、そして私はすべてを止めました。 私は全てのデータ増強、脱落、体重減少を止めました。 私はそれが私がそうすることができるのと同じくらいやり過ぎにしようとしました。 私はそれをちょっとした学習率で訓練しました、私は本当に長い間それを訓練しました。 そして多分私はそれをやり過ぎにし始めた。 多分。

So the only thing that tells you that you're overfitting is that the error rate improves for a while and then starts getting worse again. You will see a lot of people, even people that claim to understand machine learning, tell you that if your training loss is lower than your validation loss, then you are overfitting. As you will learn today in more detail and during the rest of course, that is **absolutely not true**. 

だから、あなたが装備しすぎていることをあなたに言う唯一のことは、エラー率がしばらくの間改善し、そして再び悪化し始めるということです。 機械学習を理解していると主張する人でさえ、多くの人に会うでしょう。あなたのトレーニングの損失があなたの検証の損失よりも低いなら、あなたは過剰装備であるとあなたに言います。 あなたが今日もっと詳細にそして残りの部分の間に今日学ぶであろうように、それは**絶対に真実ではない**。


> Any model that is trained correctly will always have a lower training loss than validation loss. 

>正しく訓練されたモデルは、検証損失よりも常に低い訓練損失があります。

That is not a sign of overfitting. That is not a sign you've done something wrong. That is a sign you have done something right. The sign that you're overfitting is that your error starts getting worse, because that's what you care about. You want your model to have a low error. So as long as you're training and your model error is improving, you're not overfitting. How could you be?

それは過剰装備の兆候ではありません。 それはあなたが間違ったことをしたというサインではありません。 それはあなたが正しいことをしたというサインです。 あなたが過剰装備しているというサインはあなたのエラーが悪化し始めるということです、それはあなたが気にかけているものだからです。 あなたはあなたのモデルに低い誤差を持たせたいのです。 トレーニングしていてモデルの誤差が改善されている限りは、やり過ぎではありません。 お元気ですか？

```python
np.random.seed(42)
data = ImageDataBunch.from_folder(path, train=".", valid_pct=0.9, bs=32, 
        ds_tfms=get_transforms(do_flip=False, max_rotate=0, max_zoom=1, max_lighting=0, max_warp=0
                              ),size=224, num_workers=4).normalize(imagenet_stats)
```

```python
learn = create_cnn(data, models.resnet50, metrics=error_rate, ps=0, wd=0)
learn.unfreeze()
```

```python
learn.fit_one_cycle(40, slice(1e-6,1e-4))
```

```
Total time: 06:39
epoch  train_loss  valid_loss  error_rate
1      1.513021    1.041628    0.507326    (00:13)
2      1.290093    0.994758    0.443223    (00:09)
3      1.185764    0.936145    0.410256    (00:09)
4      1.117229    0.838402    0.322344    (00:09)
5      1.022635    0.734872    0.252747    (00:09)
6      0.951374    0.627288    0.192308    (00:10)
7      0.916111    0.558621    0.184982    (00:09)
8      0.839068    0.503755    0.177656    (00:09)
9      0.749610    0.433475    0.144689    (00:09)
10     0.678583    0.367560    0.124542    (00:09)
11     0.615280    0.327029    0.100733    (00:10)
12     0.558776    0.298989    0.095238    (00:09)
13     0.518109    0.266998    0.084249    (00:09)
14     0.476290    0.257858    0.084249    (00:09)
15     0.436865    0.227299    0.067766    (00:09)
16     0.457189    0.236593    0.078755    (00:10)
17     0.420905    0.240185    0.080586    (00:10)
18     0.395686    0.255465    0.082418    (00:09)
19     0.373232    0.263469    0.080586    (00:09)
20     0.348988    0.258300    0.080586    (00:10)
21     0.324616    0.261346    0.080586    (00:09)
22     0.311310    0.236431    0.071429    (00:09)
23     0.328342    0.245841    0.069597    (00:10)
24     0.306411    0.235111    0.064103    (00:10)
25     0.289134    0.227465    0.069597    (00:09)
26     0.284814    0.226022    0.064103    (00:09)
27     0.268398    0.222791    0.067766    (00:09)
28     0.255431    0.227751    0.073260    (00:10)
29     0.240742    0.235949    0.071429    (00:09)
30     0.227140    0.225221    0.075092    (00:09)
31     0.213877    0.214789    0.069597    (00:09)
32     0.201631    0.209382    0.062271    (00:10)
33     0.189988    0.210684    0.065934    (00:09)
34     0.181293    0.214666    0.073260    (00:09)
35     0.184095    0.222575    0.073260    (00:09)
36     0.194615    0.229198    0.076923    (00:10)
37     0.186165    0.218206    0.075092    (00:09)
38     0.176623    0.207198    0.062271    (00:10)
39     0.166854    0.207256    0.065934    (00:10)
40     0.162692    0.206044    0.062271    (00:09)
```



[[52:23](https://youtu.be/Egp4Zajhzog?t=3143)]

So they are the main four things that can go wrong. There are some other details that we will learn about during the rest of this course but honestly if you stopped listening now (please don't, that would be embarrassing) and you're just like okay I'm going to go and download images, I'm going to create CNNs with resnet32 or resnet50, I'm going to make sure that my learning rate and number of epochs is okay and then I'm going to chuck them up in a Starlette web API, most of the time you are done. At least for computer vision. Hopefully you will stick around because you want to learn about NLP, collaborative filtering, tabular data, and segmentation, etc as well. 

 したがって、それらはうまくいかない可能性がある主な4つのことです。 このコースの残りの間に私たちが学ぶであろういくつかの他の詳細がありますが、正直に言って、あなたが今聞くのをやめたならば（恥ずかしいでしょう、そうしないでください）。 resnet32またはresnet50でCNNを作成します。学習率とエポック数が問題ないことを確認してから、ほとんどの場合はStarlette Web APIでそれらをまとめます。 これで終わりです。 少なくともコンピュータビジョンのために。 NLP、協調フィルタリング、表形式データ、セグメンテーションなどについても学びたいので、頑張ってください。

[[53:10](https://youtu.be/Egp4Zajhzog?t=3190)]

Let's now understand what's actually going on. What does "loss" mean? What does "epoch" mean? What does "learning rate" mean? Because for you to really understand these ideas, you need to know what's going on. So we are going to go all the way to the other side. Rather than creating a state of the art cougar detector, we're going to go back and create the simplest possible linear model. So we're going to actually seeing a little bit of math. But don't be turned off. It's okay. We're going to do a little bit of math but it's going to be totally fine. Even if math is not your thing. Because the first thing we're going to realize is that when we see a picture like this number eight:

実際に何が起こっているのかを理解しましょう。 「損失」とはどういう意味ですか？ 「エポック」とはどういう意味ですか？ 「学習率」とはどういう意味ですか？ あなたが本当にこれらのアイデアを理解するためには、何が起こっているのかを知る必要があるからです。 だから我々は反対側にずっと行くつもりです。 最先端のクーガー検出器を作成するのではなく、戻って可能な限り単純な線形モデルを作成します。 だから私たちは実際に少し数学を見に行くつもりです。 しかし、消さないでください。 いいんだよ。 私たちは少し数学をするつもりですが、それは完全にうまくいくでしょう。 数学があなたのものでなくても。 私たちが最初に気づくのは、この8のような絵を見たときです。

![](lesson2/18.png)

It's actually just a bunch of numbers. For this grayscale one, it's a matrix of numbers. If it was a color image, it would have a third dimension. So when you add an extra dimension, we call it a tensor rather than a matrix. It would be a 3D tensor of numbers ﹣ red, green, and blue. 

それは実際には単なる数字の束です。 このグレースケールのもののために、それは数のマトリックスです。 それがカラー画像だった場合、それは三次元を持つでしょう。 そのため、追加の次元を追加すると、行列ではなくテンソルと呼ばれます。 赤、緑、青の3次元テンソル数になります。

![](lesson2/19.png)

So when we created that teddy bear detector, what we actually did was we created a mathematical function that took the numbers from the images of the teddy bears and a mathematical function converted those numbers into, in our case, three numbers: a number for the probability that it's a teddy, a probability that it's a grizzly, and the probability that it's a black bear. In this case, there's some hypothetical function that's taking the pixel representing a handwritten digit and returning ten numbers: the probability for each possible outcome (i.e. the numbers from zero to nine). 

テディベア検出器を作成したときに、実際に行ったのは、テディベアの画像から数字を取得する数学関数と、この場合は3つの数字に変換する数学関数です。 それがテディである可能性、それがグリズリーである可能性、そしてそれがクロクマである可能性。 この場合、手書きの数字を表すピクセルを取得して10個の数値を返すという仮定の関数があります。それぞれの考えられる結果の確率（つまり、0から9までの数値）です。

So what you'll often see in our code and other deep learning code is that you'll find this bunch of probabilities and then you'll find a function called max or argmax attached to it. What that function is doing is, it's saying find the highest number (i.e. probability) and tell me what the index is.  So `np.argmax` or `torch.argmax` of the above array would return the index 8. 

そのため、私たちのコードや他の深い学習コードでよく見られるのは、この一連の確率を見つけて、それにmaxまたはargmaxという名前の関数が付いていることです。 その機能がしていることは、それは最高の数（すなわち確率）を見つけて、インデックスが何であるかを私に言うということです。 そのため、上記の配列の `np.argmax`または` torch.argmax`はインデックス8を返します。

In fact, let's try it. We know that the function to predict something is called `learn.predict`. So we can chuck two question marks before or after it to get the source code.

実際に試してみましょう。 私たちは、何かを予測する機能が `learn.predict`と呼ばれることを知っています。 そのため、その前後に2つの疑問符をチャックしてソースコードを取得できます。

![](lesson2/20.png) 

And here it is. `pred_max = res.argmax()`. Then what is the class? We just pass that into the classes array. So you should find that the source code in the fastai library can both strengthen your understanding of the concepts and make sure that you know what's going on and really help you here.

そしてここにあります。 `pred_max = res.argmax（）`。 それではクラスは何ですか？ それをクラス配列に渡すだけです。 そのため、fastaiライブラリのソースコードは、概念に対する理解を深めることと、何が起こっているのかを確実に理解し、ここで実際に役立つことの両方を確実にすることができます。

![](lesson2/21.png)



**Question**:  Can we have a definition of the error rate being discussed and how it is calculated? I assume it's cross validation error [[56:38](https://youtu.be/Egp4Zajhzog?t=3398)]. 

**質問**：議論されているエラー率の定義とその計算方法を教えてください。 クロスバリデーションエラーだと思います

Sure. So one way to answer the question of how is error rate calculated would be to type `error_rate??` and look at the source code, and it's 1 - accuracy.

もちろんです。 そのため、エラー率の計算方法についての質問に答える1つの方法は、 `error_rate ??`と入力してソースコードを調べることです。それは1  - 精度です。

![](lesson2/22.png) 

So then a question might be what is accuracy:

それでは、正確さとは何か、という疑問があります。

![](lesson2/23.png)

It is argmax. So we now know that means find out which particular thing it is, and then look at how often that equals the target (i.e. the actual value) and take the mean. So that's basically what it is. So then the question is, okay, what does that being applied to and always in fastai, metrics (i.e. the things that we pass in) are always going to be applied to the validation set. Any time you put a metric here, it'll be applied to the validation set because that's your best practice:

argmaxです。 それで、我々は今それがそれがどの特定のものであるかを見つけて、そしてそれがどれほど頻繁にそれが目標（すなわち実際の値）に等しいかを見てそして平均をとるという意味を知っています。 だからそれは基本的にそれが何であるかです。 それで、問題は、大丈夫です、それが常にfastaiに適用されていること、そしてメトリック（つまり、渡されるもの）は常に検証セットに適用されようとしているということです。 ここにメトリックを設定すると、それが検証セットに適用されます。これがベストプラクティスです。

![](lesson2/24.png)

That's what you always want to do is make sure that you're checking your performance on data that your model hasn't seen, and we'll be learning more about the validation set shortly. 

それはあなたが常にやりたいことですあなたのモデルが見ていないデータであなたのパフォーマンスをチェックしていることを確認することです、そして我々はまもなく検証セットについてもっと学ぶでしょう。

Remember, you can also type `doc` if the source code is not what you want which might well not be, you actually want the documentation, that will both give you a summary of the types in and out of the function and a link to the full documentation where you can find out all about how metrics work and what other metrics there are and so forth. Generally speaking, you'll also find links to more information where, for example, you will find complete run through and sample code showing you how to use all these things. So don't forget that the `doc` function is your friend. Also both in the doc function and in documentation, you'll see a source link. This is like `??` but what the source link does is it takes you into the exact line of code in Github. So you can see exactly how that's iplemented and what else is around it. So lots of good stuff there. 

覚えておいて、あなたがソースコードがあなたが望んでいるものではないかもしれないが、実際にはドキュメントが欲しいなら、 `doc`とタイプすることもできます。 メトリックがどのように機能するのか、および他にどのようなメトリックがあるのかなどに関するすべての情報を入手できる完全なドキュメント。 一般的に言って、あなたはより多くの情報へのリンクを見つけるでしょう。例えば、あなたは完全な実行とこれらすべての使い方を示すサンプルコードを見つけるでしょう。 それで `doc`関数があなたの友達であることを忘れないでください。 doc関数とドキュメントの両方で、あなたはソースリンクを見るでしょう。 これは `??`に似ていますが、ソースリンクがすることはGithubのコードの正確な行にあなたを連れて行くことです。 それで、あなたはそれがどのようにして実行されているか、そしてそれ以外に何があるのかを正確に見ることができます。 とても良いことがたくさんあります。

**Question**: Why were you using `3e` for your learning rates earlier? With `3e-5` and `3e-4` [[59:11](https://youtu.be/Egp4Zajhzog?t=3551)]? 
**質問**：なぜあなたは早くあなたの学習率に `3e`を使っていたのですか？ 3e-5と3e-4の場合

We found that 3e-3 is just a really good default learning rate. It works most of the time for your initial fine-tuning before you unfreeze. And then, I tend to kind of just multiply from there. So then the next stage, I will pick 10 times lower than that for the second part of the slice, and whatever the LR finder found for the first part of the slice. The second part of the slice doesn't come from the LR finder. It's just a rule of thumb which is 10 times less than your first part which defaults to 3e-3, and then the first part of the slice is what comes out of the LR finder. We'll be learning a lot more about these learning rate details both today and in the coming lessons. But for now, all you need to remember is that your basic approach looks like this:

3e-3が本当に良いデフォルトの学習率であることがわかりました。 あなたが解凍する前にそれはあなたの最初の微調整のためにほとんどの時間働きます。 そして、私はそこからただ増えるようなものになりがちです。 それで、次の段階では、スライスの2番目の部分のそれより10倍低く、そしてLRファインダーがスライスの最初の部分で見つけたものは何でもします。 スライスの2番目の部分はLRファインダーから来ていません。 これは単なる経験則で、最初の部分のデフォルトの3e-3よりも10倍小さく、それからスライスの最初の部分はLRファインダーから出るものです。 今日とこれからのレッスンの両方で、これらの学習率の詳細についてもっと詳しく学びます。 しかし今のところ、覚えておく必要があるのは、基本的なアプローチが次のようになっていることだけです。

- `learn.fit_one_cycle`
  - Some number of epochs, I often pick 4
  - Some learning rate which defaults to 3e-3. I'll just type it up fully so you can see. 
- Then we do that for a bit and then we unfreeze it. 
- Then we learn some more and so this is a bit where I just take whatever I did last time and divide it by 10. Then I also write like that (`slice`) then I have to put one more number in here and that's the number I get from the learning rate finder﹣a bit where it's got the strongest slope.

 - いくつかのエポック、私はよく選ぶ4
    - デフォルトの3e-3の学習率。 あなたが見ることができるように私はそれを完全にタイプアップするだけです。
 - それから私達はそれを少しの間し、そしてそれを解凍します。
 - それからもう少し学ぶので、これは私が前回行ったことを10で割るだけのちょっとした方法です。それから私もそのように書いて（ `slice`）、それからここにもう一つ数字を入れなければなりません。 私が学習率ファインダーから得た数﹣それが最も強い勾配を得たところ。
 
```python
learn.fit_one_cycle(4, 3e-3)
learn.unfreeze()
learn.fit_one_cycle(4, slice(xxx, 3e-4))
```

So that's kind of don't have to think about it, don't really have to know what's going on rule of thumb that works most of the time. But let's now dig in and actually understand it more completely.

そのため、それについて考える必要はなく、ほとんどの場合に有効な経験則で何が起こっているのかを実際に知る必要はありません。 しかし、ここで掘り下げて、実際にそれをより完全に理解しましょう。

#### Digging in and looking at the math [[1:01:17](https://youtu.be/Egp4Zajhzog?t=3677)]
掘り下げて数学を見る

We're going to create this mathematical function that takes the numbers that represent the pixels and spits out probabilities for each possible class. 

ここでは、ピクセルを表す数を取り、可能性のあるクラスごとに確率を吐き出すこの数学関数を作成します。

![](lesson2/adam.gif)



By the way, a lot of the stuff that we're using here, we are stealing from other people who are awesome and so we're putting their details here. So please check out their work because they've got great work that we are highlighting in our course. I really like this idea of this little animated gif of the numbers, so thank you to  [Adam Geitgey](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721) for creating that.

ちなみに、ここで使用しているものの多くは、他の素晴らしい人々から盗んでいるので、その詳細をここに掲載しています。 私たちのコースで強調している素晴らしい仕事がありますので、ぜひ彼らの仕事をチェックしてください。 私はこの小さなアニメーションGIFのアイデアを本当に気に入っています。[Adam Geitgey](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)  それを作成するための。

[[1:02:05](https://youtu.be/Egp4Zajhzog?t=3725)]

Let's look and see how we create one of these functions, and let's start with the simplest functions I know:

これらの関数の1つを作成する方法を見てみましょう。次に、私が知っている最も単純な関数から始めましょう。

<img src="http://latex.codecogs.com/gif.latex?y&space;=&space;ax&space;&plus;&space;b" title="y = ax + b" />

That's a line where

- a: gradient of the line
- b: the intercept of the line

それはどこの行です

 -  a：線のグラデーション
 -  b：線の切片
 
Hopefully when we said that you need to know high school math to do this course, these are the things we are assuming that you remember. If we do mention some math thing which I am assuming you remember and you don't remember it, don't freak out. It happens to all of us. [Khan Academy](https://www.khanacademy.org/) is actually terrific. It's not just for school kids. Go to Khan Academy, find the concept you need a refresher on, and he explains things really well. So strongly recommend checking that out. Remember, I'm just a philosophy student, so all the time I'm trying to either remind myself about something or I never learnt something. So we have the whole internet to teach us these things.  

うまくいけば、私たちがあなたがこのコースを行うために高校の数学を知る必要があると言ったとき、これらはあなたが覚えていると私たちが仮定していることです。 私たちがあなたが覚えていて、あなたがそれを覚えていないと私が仮定していることを私が仮定しているいくつかの数学の事柄について言及しているのであれば、気にしないでください。 それは私たち全員に起こります。 [カーンアカデミー]（https://www.khanacademy.org/）は実際に素晴らしいです。 それは学校の子供たちだけのためではありません。 カーンアカデミーに行き、復習が必要な概念を見つけ、そして彼は物事を本当によく説明します。 それをチェックすることを強くお勧めします。 覚えておいて、私はただ哲学の学生なので、私は何かについて自分自身を思い出させようとしているか、または私は何かを学んだことがないことを常に試みている。 だから私たちは私たちにこれらのことを教えるためのインターネット全体があります。

I'm going to re-write this slightly:

私はこれを少し書き直すつもりです：

<img src="http://latex.codecogs.com/gif.latex?y&space;=&space;a_1x&space;&plus;&space;a_2" title="y = a_1x + a_2" />

So let's just replace <img src="http://latex.codecogs.com/gif.latex?b" title="b" /> with a <img src="http://latex.codecogs.com/gif.latex?a_2" title="a_2" />_, just give it a different name. So there's another way of saying the same thing. Then another way of saying that would be if I could multiply<img src="http://latex.codecogs.com/gif.latex?a_2" title="a_2" /> by the number 1.  


ではこれ <img src="http://latex.codecogs.com/gif.latex?b" title="b" /> をこれに置き換えましょう。 <img src="http://latex.codecogs.com/gif.latex?a_2" title="a_2" />_, 別の名前をつけてください。だから同じことを言う別の方法があります。 それから別の言い方をすれば、<img src = "http://latex.codecogs.com/gif.latex?a_2" title = "a_2" />に1を掛けることができればということになります。

<img src="http://latex.codecogs.com/gif.latex?y&space;=&space;a_1x&space;&plus;&space;a_2\cdot&space;1" title="y = a_1x + a_2\cdot 1" />

This still is the same thing. Now at this point, I'm actually going to say let's not put the number 1 there, but put an <img src="http://latex.codecogs.com/gif.latex?x_1" title="x_1" /> here and <img src="http://latex.codecogs.com/gif.latex?x_2" title="x_2" /> here:

これはまだ同じことです。 さて、この時点で、実際には1を入れずに<img src = "http://latex.codecogs.com/gif.latex?x_1" title = "x_1" /としましょう。 >ここと<img src = "http://latex.codecogs.com/gif.latex?x_2" title = "x_2" />ここ：

<img src="http://latex.codecogs.com/gif.latex?y&space;=&space;a_1x_1&space;&plus;&space;a_2x_2" title="y = a_1x_1 + a_2x_2" />

<img src="http://latex.codecogs.com/gif.latex?x_2&space;=&space;1" title="x_2 = 1" />

So far, this is pretty early high school math. This is multiplying by 1 which I think we can handle. So this and <img src="http://latex.codecogs.com/gif.latex?y&space;=&space;ax&space;&plus;&space;b" title="y = ax + b" /> are equivalent with a bit of renaming. Now in machine learning, we don't just have one equation, we've got lots. So if  we've got some data that represents the temperature versus the number of ice creams sold, then we have lots of dots. 

これまでのところ、これはかなり初期の高校の数学です。 これは私たちが扱うことができると思う1を掛けています。 したがって、これと<img src = "http://latex.codecogs.com/gif.latex?y&space;=&space;ax&space;&plus;&space;b" title = "y = ax + b" />は、 ちょっとした改名。 機械学習では、方程式が1つだけではなく、たくさんあります。 つまり、販売されたアイスクリームの数と気温の関係を表すデータがあるとすると、ドットがたくさんあります。

![](lesson2/25.png)

So each one of those dots, we might hypothesize is based on this formula (<img src="http://latex.codecogs.com/gif.latex?y&space;=&space;a_1x_1&space;&plus;&space;a_2x_2" title="y = a_1x_1 + a_2x_2" />).  And basically there's lots of values of y and lots of values of x so we can stick little <img src="http://latex.codecogs.com/gif.latex?_i" title="_i" /> here:

そのため、これらの点のそれぞれは、この公式に基づいていると仮定することができます。<= "y = a_1x_1 + a_2x_2" />）。 そして、基本的にたくさんのyの値とたくさんのxの値があるので、ここではちょっと<img src = "http://latex.codecogs.com/gif.latex?_i" title = "_ i" />をつけることができます

<img src="http://latex.codecogs.com/gif.latex?y_i&space;=&space;ax_i&space;&plus;&space;b" title="y_i = ax_i + b" />

The way we do that is a lot like numpy indexing, but rather things in square brackets, we put them down here in the subscript in our equation:

これを行う方法は、かっこいい索引付けに非常に似ていますが、角括弧で囲まれたもので、式の添え字にそれらを置きます。

<img src="http://latex.codecogs.com/gif.latex?y_i&space;=&space;a_1x_i_,_1&space;&plus;&space;a_2x_i_,_2" title="y_i = a_1x_i_,_1 + a_2x_i_,_2" />

 So this is now saying there's actually lots of these different <img src="http://latex.codecogs.com/gif.latex?y_i" title="y_i" />'s based on lots of different <img src="http://latex.codecogs.com/gif.latex?x_i_,_1" title="x_i_,_1" /> and <img src="http://latex.codecogs.com/gif.latex?x_i_,_2" title="x_i_,_2" /> but notice there is still one of each of these (<img src="http://latex.codecogs.com/gif.latex?a_1" title="a_1" />, <img src="http://latex.codecogs.com/gif.latex?a_2" title="a_2" />). They called the coefficients or the parameters. So this is our linear equation and we are still going to say that every <img src="http://latex.codecogs.com/gif.latex?x_i_,_2" title="x_i_,_2" /> is equal to 1. Why did I do it that way? Because I want to do linear algebra? Why do I want to do in linear algebra? One reason is because [Rachel teaches the world's best linear algebra course](https://github.com/fastai/numerical-linear-algebra/blob/master/README.md), so if you're interested, check it out. So it's a good opportunity for me to throw in a pitch for this which we make no money but never mind. But more to the point right now, it's going to make life much easier. Because I hate writing loops, I hate writing code, I just want the computer to do everything for me. And anytime you see this little _i_ subscripts, that sounds like you're going to have to do loops and all kind of stuff. But what you might remember from school is that when you've got two things being multiplied together, two things being multiplied together, then they get added up, that's called a "dot product". If you do that for lots and lots of different numbers _i_, then that's called a matrix product. So in fact, this whole thing can be written like this:

そのため、これは、さまざまな<img src =に基づく<img src = "http://latex.codecogs.com/gif.latex?y_i" title = "y_i" />というものが実際にはたくさんあるということです。 "http://latex.codecogs.com/gif.latex?x_i_,_1" title = "x_i _、_ 1" />および<img src = "http://latex.codecogs.com/gif.latex?x_i_、 _2 "title =" x_i _、_ 2 "/>ただし、これらの各1つがまだあることに注意してください（<img src =" http://latex.codecogs.com/gif.latex?a_1 "title =" a_1 "/> <img src = "http://latex.codecogs.com/gif.latex?a_2" title = "a_2" />）。彼らは係数またはパラメータと呼んだ。だからこれは私たちの線形方程式であり、私たちはまだ<img src = "http://latex.codecogs.com/gif.latex?x_i_,_2" title = "x_i _、_ 2" />はすべて等しいと言うつもりです。 1.どうしてそんなことをしたのですか？線形代数をしたいのですか？なぜ線形代数でやりたいのですか？理由の1つは、[Rachelが世界最高の線形代数コースを教えている]（https://github.com/fastai/numerical-linear-algebra/blob/master/README.md）なので、興味のある方はぜひご覧ください。 。だから私は私達がお金をもうけないが気にしないでこれのためにピッチを投入する良い機会です。しかし、今のところもっと言えば、それは人生をずっと楽にするでしょう。私はループを書くのが嫌いなので、コードを書くのが嫌いです。私はコンピュータにすべてのことをさせたいのです。そして、あなたがこの小さな_i_添え字を見たときはいつでも、それはあなたがループやあらゆる種類のものをしなければならないだろうように思えます。しかし、あなたが学校で覚えているのは、2つのことが掛け合わされ、2つのことが掛け合わされ、それからそれらが足し合わされるということです。それは「内積」と呼ばれます。あなたがそれをたくさんの、そしてたくさんの異なった数の_i_のためにするならば、それはそれから行列積と呼ばれます。したがって、実際には、このこと全体を次のように書くことができます。

<img src="http://latex.codecogs.com/gif.latex?\vec{y}&space;=&space;X\vec{a}" title="\vec{y} = X\vec{a}" />

Rather than lots of different <img src="http://latex.codecogs.com/gif.latex?y_i" title="y_i" />'s, we can say there's one vector called <img src="http://latex.codecogs.com/gif.latex?y" title="y" /> which is equal to one matrix called <img src="http://latex.codecogs.com/gif.latex?X" title="X" /> times one vector called <img src="http://latex.codecogs.com/gif.latex?a" title="a" />. At this point, I know a lot of you don't remember that. That's fine. We have a picture to show you.

たくさんの<img src = "http://latex.codecogs.com/gif.latex?y_i" title = "y_i" />があるのではなく、<img src = "httpという1つのベクトルがあると言えます。 //latex.codecogs.com/gif.latex?y "title =" y "/>これは<img src =" http://latex.codecogs.com/gif.latex?X "という名前の1つの行列に相当します。 = "X" />×<img src = "http://latex.codecogs.com/gif.latex?a" title = "a" />という1つのベクトル。 この時点で、私はあなたの多くがそれを覚えていないことを知っています。 それはいいです。 あなたに見せるための写真があります。

Andre Staltz created this fantastic called http://matrixmultiplication.xyz/ and here we have a matrix by a vector, and we are going to do a matrix vector product.

Andre Staltzはこの素晴らしいhttp://matrixmultiplication.xyz/を作成しました。ここではベクトルごとの行列を作成し、行列ベクトル積を作成します。

![](lesson2/matrix.gif)

That is what matrix vector multiplication does. In other words, it's just <img src="http://latex.codecogs.com/gif.latex?y_i&space;=&space;a_1x_i_,_1&space;&plus;&space;a_2x_i_,_2" title="y_i = a_1x_i_,_1 + a_2x_i_,_2" /> except his version is much less messy. 

それが行列ベクトルの掛け算です。 言い換えれば、それはただ<img src = "http://latex.codecogs.com/gif.latex?y_i&space;=&space;a_1x_i_,_1&space;&pls;&space;a_2x_i_,_2" title = "y_i = a_1x_i _、_ 1です。 + a_2x_i _、_ 2 "/>彼のバージョンを除けばはるかに手間がかかりません。

**Question**: When generating new image dataset, how do you know how many images are enough? What are ways to measure "enough"? [[1:08:35](https://youtu.be/Egp4Zajhzog?t=4115)]
**質問**：新しい画像データセットを生成するとき、あなたはどのくらいの数の画像が十分であるかをどのように知っていますか？ 「十分」と判断する方法は何ですか？

Great question. Another possible problem you have is you don't have enough data. How do you know if you don't have enough data? Because you found a good learning rate (i.e. if you make it higher than it goes off into massive losses; if you make it lower, it goes really slowly) and then you train for such a long time that your error starts getting worse. So you know that you trained for long enough. And you're still not happy with the accuracy﹣it's not good enough for the teddy bear cuddling level of safety you want. So if that happens, there's a number of things you can do and we'll learn pretty much all of them during this course but one of the easiest one is get more data. If you get more data, then you can train for longer, get a higher accuracy, lower error rate, without overfitting. 

素晴らしい質問です。 あなたが持っているもう一つの問題はあなたが十分なデータを持っていないということです。 あなたが十分なデータを持っていないかどうかあなたはどのように知っていますか？ あなたが良い学習率を見つけたので（すなわち、あなたがそれを大規模な損失になるよりも高くするなら;それを低くするなら、それは本当にゆっくり進みます）そしてあなたはあなたのエラーが悪化し始めるように長い間訓練します。 だからあなたはあなたが十分長い間訓練したことを知っています。 それでも正確さに満足していません﹣それはあなたが望む安全性のレベルを抱きしめているテディベアにとって十分ではありません。 それでそれが起こるならば、あなたがすることができる多くのことがあります、そして、我々はこのコースの間にそれらのほとんどすべてを学びますが、最も簡単なものの一つはより多くのデータを得ることです。 あなたがより多くのデータを得れば、あなたはより長くフィットすることなく、あなたはより長く訓練し、より高い精度、より低いエラー率を得ることができます。

Unfortunately there is no shortcut. I wish there was. I wish there's some way to know ahead of time how much data you need. But I will say this﹣most of the time, you need less data than you think. So organizations very commonly spend too much time gathering data, getting more data than it turned out they actually needed. So get a small amount first and see how you go.

残念ながら近道はありません。 あったらいいのに。 どのくらいの量のデータが必要かを事前に知る方法があるといいのですが。 しかし、私はこれを言います - ほとんどの場合、あなたはあなたが思うより少ないデータを必要とします。 そのため、組織は非常に一般的にデータ収集に時間をかけすぎて、実際に必要な結果より多くのデータを取得しています。 だから最初に少量を手に入れて、あなたがどうやって行くのか見てみましょう。

**Question**: What do you do if you have unbalanced classes such as 200 grizzly and 50 teddy? [[1:10:00](https://youtu.be/Egp4Zajhzog?t=4200)]
**質問**：グリズリーが200人、テディが50人のようにバランスの取れていないクラスがある場合、どうしますか？

Nothing. Try it. It works. A lot of people ask this question about how do I deal with unbalanced data. I've done lots of analysis with unbalanced data over the last couple of years and I just can't make it not work. It always works. There's actually a paper that said if you want to get it slightly better then the best thing to do is to take that uncommon class and just make a few copies of it. That's called "oversampling" but I haven't found a situation in practice where I needed to do that. I've found it always just works fine, for me. 

何もない それを試してみてください。 できます。 私はどのようにして不均衡なデータを扱うのかについて多くの人がこの質問をします。 私は過去2、3年の間にバランスの取れていないデータでたくさんの分析をしました、そして、私はそれがうまくいかないようにすることができません。 それはいつもうまくいきます。 実際にそれを少し良くしたいのであれば、それを行うための最良の方法は、その珍しいクラスを利用して、そのコピーをいくつか作成することです。 これは「オーバーサンプリング」と呼ばれますが、実際にそれを行う必要がある状況は見つかっていません。 私はそれがいつもうまく動くことを常に見つけました。

**Question**: Once you unfreeze and retrain with one cycle again, if your training loss is still higher than your validation loss (likely underfitting), do you retrain it unfrozen again (which will technically be more than one cycle) or you redo everything with longer epoch per the cycle?  [[1:10:47](https://youtu.be/Egp4Zajhzog?t=4247)] 

**質問**：いったん1サイクル解凍して再度トレーニングした後で、トレーニングの損失がまだ検証損失よりも多い場合（不足している可能性があります）、それを再度解凍しますか（技術的に1サイクル以上になります）。 サイクルあたりの長いエポックですべてをやり直しますか？

You guys asked me that last week. My answer is still the same. I don't know. Either is fine. If you do another cycle, then it'll maybe generalize a little bit better. If you start again, do twice as long, it's kind of annoying, depends how patient you are. It won't make much difference. For me personally, I normally just train a few more cycles. But it doesn't make much difference most of the time.

皆さん、先週そのことを私に尋ねました。 私の答えはまだ同じです。 知りません。 どちらでも構いません。 あなたが別のサイクルをするなら、それは多分もう少し一般化するでしょう。 あなたが再び始めるならば、2倍長くしなさい、それは一種の厄介なことである、あなたがどれくらい辛抱強いかによる。 それほど大きな違いはありません。 私個人としては、通常、あと数サイクルトレーニングするだけです。 しかし、ほとんどの場合、それほど大きな違いはありません。

**Question**: Question about this code example:
**質問**：このコード例に関する質問：

```
classes = ['black', 'grizzly', 'teddys']
data2 = ImageDataBunch.single_from_classes(path, classes, tfms=get_transforms(), size=224).normalize(imagenet_stats)
learn = create_cnn(data2, models.resnet34)
learn.load('stage-2')
```

This requires `models.resnet34` which I find surprising: I had assumed that the model created by `.save(...)` (which is about 85MB on disk) would be able to run without also needing a copy of `resnet34`. [[1:11:37](https://youtu.be/Egp4Zajhzog?t=4297)]

これは私が驚くべきと思うmodels.resnet34を必要とします。  [[1:11:37]（https://youtu.be/Egp4Zajhzog?t=4297）]

We're going to be learning all about this shortly. There is no "copy of ResNet34", ResNet34 is what we call "architecture"﹣it's a functional form. Just like <img src="http://latex.codecogs.com/gif.latex?y&space;=&space;ax&space;&plus;&space;b" title="y = ax + b" /> is a linear functional form. It doesn't take up any room, it doesn't contain anything, it's just a function. ResNet34 is just a function. I think the confusion here is that we often use a pre-trained neural net that's been learnt on ImageNet. In this case, we don't need to use a pre-trained neural net. Actually, to avoid that even getting created, you can actually pass `pretrained=False`:

私たちはまもなくこのことについてすべて学ぶつもりです。 「ResNet34のコピー」はありません、ResNet34は「アーキテクチャー」と呼ばれるもので、機能的な形式です。 <img src = "http://latex.codecogs.com/gif.latex?y&space;=&space;ax&space;&plus;&space;b" title = "y = ax + b" />が線形関数形式であるのと同じです。 。 場所を取らず、何も含まず、単なる機能です。 ResNet34は単なる機能です。 ここでの混乱は、ImageNetで習得した訓練済みのニューラルネットをよく使用することだと思います。 この場合、事前にトレーニングされたニューラルネットを使用する必要はありません。 実際には、作成されないようにするために、実際には `pretrained = False`を渡すことができます。

![](lesson2/26.png)

That'll ensure that nothing even gets loaded which will save you another 0.2 seconds, I guess. But we'll be learning a lot more about this. So don't worry if this is a bit unclear. The basic idea is `models.resnet34` above is basically the equivalent of saying is it a line or is it a quadratic or is it a reciprocal﹣this is just a function. This is a ResNet34 function. It's a mathematical function. It doesn't take any storage, it doesn't have any numbers, it doesn't have to be loaded as opposed to a pre-trained model. When we did it at the inference time, the thing that took space is this bit:

それはあなたにさらに0.2秒節約させる何もロードされないことを確実にするでしょう、と私は思います。 しかし、これについてはもっと学ぶことになります。 それで、これが少しはっきりしなくても心配しないでください。 基本的な考え方は、上の `models.resnet34`が基本的にそれが線であるか、それが二次であるか、それが逆数であると言うことと同等です。これは単なる関数です。 これはResNet34の機能です。 それは数学関数です。 それは少しの記憶も必要としません、それは少しの数も持っていません、それは事前に訓練されたモデルとは対照的にロードされる必要はありません。 推論時に実行したとき、スペースを取ったのはこのビットです。

![](lesson2/27.png)

Which is where we load our parameters. It is basically saying, as we are about find out, what are the values of <img src="http://latex.codecogs.com/gif.latex?a" title="a" /> and <img src="http://latex.codecogs.com/gif.latex?b" title="b" />﹣ we have to store these numbers. But for ResNet 34, you don't just store 2 numbers, you store a few million or few tens of millions of numbers. 

これがパラメータをロードする場所です。 基本的に言っているのは、私たちが見つけようとしているように、<img src = "http://latex.codecogs.com/gif.latex?a" title = "a" />と<img src ="http://latex.codecogs.com/gif.latex?b" title = "b" /> の値は何ですか？  - これらの番号を保存する必要があります。 しかし、ResNet 34では、2つの数字を保存するだけではなく、数百万または数千万の数字を保存します。

[[1:14:13](https://youtu.be/Egp4Zajhzog?t=4453) ]

So why did we all this? It's because I wanted to be able to write it out like this: <img src="http://latex.codecogs.com/gif.latex?\vec{y}&space;=&space;X\vec{a}" title="\vec{y} = X\vec{a}" /> and the reason I wanted to be able to like this is that we can now do that in PyTorch with no loops, single line of code, and it's also going to run faster. 

では、なぜ私たちは皆これをしたのですか 私はこのように書き出したいと思ったからです。<img src = "http://latex.codecogs.com/gif.latex?\vec{y}&space;=&space;X\vec{a}" title = "\ vec {y} = X \ vec {a}" />このようにしたいのは、ループや単一行のコードを使用せずにPyTorchでこれを実行できるようになったからです。 速く走るつもりです。

> **PyTorch really doesn't like loops**
PyTorchは本当にループが好きではありません

It really wants you to send it a whole equation to do all at once. Which means, you really want to try and specify things in these kind of linear algebra ways. So let's go and take a look because what we're going to try and do then is we're going to try and take this <img src="http://latex.codecogs.com/gif.latex?\vec{y}&space;=&space;X\vec{a}" title="\vec{y} = X\vec{a}" /> (we're going to call this an architecture). It's the world's tiniest neural network. It's got two parameters <img src="http://latex.codecogs.com/gif.latex?a_1" title="a_1" /> and <img src="http://latex.codecogs.com/gif.latex?a_2" title="a_2" />. We are going to try and fit this architecture to some data. 

それは本当にあなたがそれをすべて一度にするためにそれに全体の方程式を送ることを望んでいる。 つまり、本当にこの種の線形代数的方法で物事を試して指定したいのです。 それでは、行って見てみましょう。何をしようとしているのか、これを試してみてください。<img src = "http://latex.codecogs.com/gif.latex?\vec { y \＆space; =＆space; X \ vec {a} "title =" \ vec {y} = X \ vec {a} "/>（これをアーキテクチャと呼びます） それは世界で最も小さいニューラルネットワークです。 <img src = "http://latex.codecogs.com/gif.latex?a_1" title = "a_1" />と<img src = "http://latex.codecogs.com/gifの2つのパラメータがあります。 latex？a_2 "title =" a_2 "/>。 このアーキテクチャをいくつかのデータに適合させるようにします。

### SGD [[1:15:06](https://youtu.be/Egp4Zajhzog?t=4506)]

So let's jump into a notebook and generate some dots, and see if we can get it to fit a line somehow. And the "somehow" is going to be using something called SGD. What is SGD? Well, there's two types of SGD. The first one is where I said in lesson 1 "hey you should all try building these models and try and come up with something cool" and you guys all experimented and found really good stuff. So that's where the S would be Student. That would be Student Gradient Descent. So that's version one of SGD.

それでは、ノートブックに飛び込んでドットをいくつか生成して、どうにか行に収まるかどうかを確認しましょう。 そして「どういうわけか」はSGDと呼ばれるものを使用することになるでしょう。 SGDとは SGDには2つの種類があります。 最初のレッスンでは、レッスン1で「これらのモデルを作成してかっこいいものを試してみて、試してみるべきだ」と言ったところで、みんな実験して本当に良いものを見つけました。 それで、Sが学生になるところです。 それは学生グラデーション降下です。 だからそれはSGDのバージョン1です。

Version two of SGD which is what I'm going to talk about today is where we are going to have a computer try lots of things and try and come up with a really good function and that would be called Stochastic Gradient Descent. The other one that you hear a lot on Twitter is Stochastic Grad student Descent. 

今日お話しするのは、SGDのバージョン2で、コンピュータに多くのことを試してもらい、本当に優れた機能を試してみることになります。これを確率勾配法と呼びます。 もう1つTwitterでよく聞かれるのはStochastic Gradの学生Descentです。

#### Linear Regression problem [[1:16:08](https://youtu.be/Egp4Zajhzog?t=4568)]
線形回帰問題

We are going to jump into [lesson2-sgd.ipynb](https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson2-sgd.ipynb). We are going to go bottom-up rather than top-down. We are going to create the simplest possible model we can which is going to be a linear model. And the first thing we need is we need some data. So we are going to generate some data. The data we're going to generate looks like this:

[lesson2-sgd.ipynb]（https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson2-sgd.ipynb）にジャンプします。 トップダウンではなくボトムアップになります。 できる限り単純なモデルを作成し、それを線形モデルにします。 そして最初に必要なことはデータが必要なことです。 だから我々はいくつかのデータを生成するつもりです。 これから生成するデータは次のようになります。

 ![](lesson2/n1.png)

So x-axis might represent temperature, y-axis might represent number of ice creams we sell, or something like that. But we're just going to create some synthetic data that we know is following a line. As we build this, we're actually going to learn a little bit about PyTorch as well. 

そのため、X軸は気温を表し、Y軸は販売するアイスクリームの数を表すことができます。 しかし、私たちは、ラインに沿っていることがわかっている合成データを作成することにしています。 これを構築しながら、実際にPyTorchについても少し学びます。

```python
%matplotlib inline
from fastai import *
```

```python
n=100
```

```python
x = torch.ones(n,2) 
x[:,0].uniform_(-1.,1)
x[:5]
```

```
tensor([[-0.1338,  1.0000],
        [-0.4062,  1.0000],
        [-0.3621,  1.0000],
        [ 0.4551,  1.0000],
        [-0.8161,  1.0000]])
```



Basically the way we're going to generate this data is by creating some coefficients.  <img src="http://latex.codecogs.com/gif.latex?a_1" title="a_1" /> will be 3 and <img src="http://latex.codecogs.com/gif.latex?a_2" title="a_2" /> will be 2. We are going to create a column of numbers for our <img src="http://latex.codecogs.com/gif.latex?x" title="x" />'s and a whole bunch of 1's.

基本的にこのデータを生成する方法はいくつかの係数を作成することです。 <img src = "http://latex.codecogs.com/gif.latex?a_1" title = "a_1" />は3になり、<img src = "http://latex.codecogs.com/gif.latex?a_2 "title =" a_2 "/>になります。 は2になります。<img src =" http://latex.codecogs.com/gif.latex?x "title =" xの列を作成します。 "/>と1がたくさん入っています。

```python
a = tensor(3.,2); a
```

```
tensor([3., 2.])
```



And then we're going to do this `x@a`. What is `x@a`? `x@a` in Python means a matrix product between `x` and `a`.  And it actually is even more general than that. It can be a vector vector product, a matrix vector product, a vector matrix product, or a matrix matrix product. Then actually in PyTorch, specifically, it can mean even more general things where we get into higher rank tensors which we will learn all about very soon. But this is basically the key thing that's going to go on in all of our deep learning. The vast majority of the time, our computers are going to be basically doing this﹣multiplying numbers together and adding them up which is the surprisingly useful thing to do. 

それでは、この「x @ a」を実行します。 x @ aとは何ですか？ Pythonの `x @ a`は` x`と `a`の間の行列積を意味します。 そしてそれは実際にそれよりもっと一般的です。 ベクトルベクトル積、行列ベクトル積、ベクトル行列積、または行列行列積になります。 それから、実際にはPyTorchで、具体的に言うと、それは私たちがよりすぐにすべてを学ぶことになるより高いランクのテンソルに入るところでさらに一般的なことを意味することができます。 しかし、これは基本的に私たちのすべての深い学びにおいて続くことになるだろう重要なことです。 ほとんどの場合、私たちのコンピュータは基本的にこのようにして﹣を掛け、それらを足し合わせることになるでしょう。これは驚くほど便利なことです。

```python
y = x@a + torch.rand(n)
```



[[1:17:57](https://youtu.be/Egp4Zajhzog?t=4677)]

So we basically are going to generate some data by creating a line and then we're going to add some random numbers to it. But let's go back and see how we created `x` and `a`. I mentioned that we've basically got these two coefficients 3 and 2. And you'll see that we've wrapped it in this function called `tensor`. You might have heard this word "tensor" before. It's one of these words that sounds scary and apparently if you're a physicist, it actually is scary. But in the world of deep learning, it's actually not scary at all. "Tensor" means array, but specifically it's an array of a regular shape. So it's not an array where row 1 has two things, row 3 has three things, and row 4 has one thing, what you call a "jagged array". That's not a tensor.  A tensor is any array which has a rectangular or cube or whatever ﹣ a shape where every row is the same length and every column is the same length. The following are all tensors:

ですから、基本的にはラインを作成してデータを生成し、それから乱数を追加します。 しかし戻って、どのようにして `x`と` a`を作成したのかを見てみましょう。 基本的にこれら2つの係数3と2を持っていると私は述べました。そして私たちはそれを `tensor`と呼ばれるこの関数で包んだことがわかります。 あなたは前にこの単語 "tensor"を聞いたことがあるかもしれません。 それはこれらの単語のうちの1つで、怖く聞こえますし、物理学者なら、明らかに怖いです。 しかし、ディープラーニングの世界では、実際にはまったく怖くありません。 「テンソル」とは配列を意味しますが、具体的には規則的な形状の配列です。 つまり、1行目に2つの要素、3行目に3つの要素、4行目に1つの要素がある配列ではありません。これを「ギザギザ配列」と呼びます。 それはテンソルではありません。 テンソルとは、長方形や立方体、あるいはあらゆるものを含む任意の配列のことです。つまり、すべての行が同じ長さで、すべての列が同じ長さになります。 以下はすべてテンソルです。

- 4 by 3 matrix
- A vector of length 4
- A 3D array of length 3 by 4 by 6

 -  4×3マトリックス
 - 長さ4のベクトル
 - 長さ3 x 4 x 6の3D配列

That's all tensor is. We have these all the time. For example, an image is a 3 dimensional tensor. It's got number of rows by number of columns by number of channels (normally red, green, blue). So for example, VGA picture could be 640 by 480 by 3 or actually we do things backwards so when people talk about images, they normally go width by height, but when we talk mathematically, we always go a number of rows by number of columns, so it would actually be 480 by 640 by 3 that will catch you out. We don't say dimensions, though, with tensors. We use one of two words, we either say rank or axis. Rank specifically means how many axes are there, how many dimensions are there. So an image is generally a rank 3 tensor. What we created here is a rank 1 tensor (also known as a vector). But in math, people come up with very different words for slightly different concepts. Why is a one dimensional array a vector and a two dimensional array is a matrix, and a three dimensional array doesn't have a name. It doesn't make any sense. With computers, we try to have some simple consistent naming conventions. They are all called tensors﹣rank 1 tensor, rank 2 tensor, rank 3 tensor. You can certainly have a rank 4 tensor. If you've got 64 images, then that would be a rank 4 tensor of 64 by 480 by 640 by 3. So tensors are very simple. They just mean arrays.

それがすべてテンソルです。私たちはいつもこれらを持っています。たとえば、画像は3次元テンソルです。行数、列数、チャンネル数（通常は赤、緑、青）です。ですから、例えばVGA画像は640 x 480 x 3であるかもしれませんし、実際には後ろ向きにしているので、人々が画像について話すとき、それらは通常高さによって幅になります、しかし数学的に話すときつまり、実際には480 x 640 x 3となるでしょう。しかし、テンソルを使って次元を言うことはしません。私たちは2つの単語のうちの1つを使用します、我々はどちらかランクまたは軸を言います。ランクとは、具体的には軸の数、次元の数を意味します。そのため、画像は通常ランク3のテンソルです。ここで作成したものはランク1テンソル（ベクトルとも呼ばれる）です。しかし、数学では、人々はわずかに異なる概念のために非常に異なる言葉を思い付きます。一次元配列がベクトルで、二次元配列が行列で、三次元配列に名前がないのはなぜですか。意味がありません。コンピュータでは、いくつかの単純で一貫した命名規則を使用しようとしています。それらはすべてテンソルcalledランク1テンソル、ランク2テンソル、ランク3テンソルと呼ばれます。あなたは確かにランク4のテンソルを持つことができます。 64個の画像を持っているなら、それは64×480×640×3のランク4テンソルになるでしょう。だからテンソルはとても簡単です。それらは単に配列を意味します。

In PyTorch, you say `tensor` and you pass in some numbers, and you get back, which in this case just a list,  a vector. This then represents our coefficients: the slope and the intercept of our line. 

PyTorchでは、あなたは `tensor`と言い、あなたはいくつかの数を渡し、そして戻ってきます。この場合は単なるリスト、ベクトルです。 これは私たちの係数を表しています。それは私たちの線の傾きと切片です。

![](lesson2/28.png)

Because we are not actually going to have a special case of <img src="http://latex.codecogs.com/gif.latex?ax&space;&plus;&space;b" title="ax + b" />, instead, we are going to say there's always this second <img src="http://latex.codecogs.com/gif.latex?x" title="x" /> value which is always 1

実際には、<img src = "http://latex.codecogs.com/gif.latex?ax&space;&plus;&space;b" title = "ax + b" />という特別な場合はありません。 、この2番目の値が常にあるとします。<img src = "http://latex.codecogs.com/gif.latex?x" title = "x" />常に1

<img src="http://latex.codecogs.com/gif.latex?y_i&space;=&space;a_1x_i_,_1&space;&plus;&space;a_2x_i_,_2" title="y_i = a_1x_i_,_1 + a_2x_i_,_2" />

You can see it here, always 1 which allows us just to do a simple matrix vector product:

あなたはここでそれを見ることができます、常に私たちが単純な行列ベクトル積をすることを可能にする常に1。

![](lesson2/29.png)

So that's <img src="http://latex.codecogs.com/gif.latex?a" title="a" />. Then we wanted to generate this <img src="http://latex.codecogs.com/gif.latex?x" title="x" /> array of data. We're going to put random numbers in the first column and a whole bunch of 1's in the second column. To do that, we say to PyTorch that we want to create a rank 2 tensor of `n` by 2. Since we passed in a total of 2 things, we get a rank 2 tensor. The number of rows will be `n` and the number of columns will be 2. In there, every single thing in it will be a 1﹣that's what `torch.ones` means. 

だから<img src = "http://latex.codecogs.com/gif.latex?a" title = "a" />です。 それから、この<img src = "http://latex.codecogs.com/gif.latex?x" title = "x" />データ配列を生成したいと思いました。 最初の列には乱数を、2番目の列には1の整数を入れます。 これを行うために、PyTorchに、2でnの階数2のテンソルを作成したいと言います。2つのものを渡したので、階数2のテンソルが得られます。 行数は `n`、列数は2になります。そこでは、その中のひとつひとつが1であり、それが` torch.ones`です。

[[1:22:45](https://youtu.be/Egp4Zajhzog?t=4965)] 

Then this is really important. You can index into that just like you can index into a list in Python. But you can put a colon anywhere and a colon means every single value on that axis/dimension. This here `x[:,0]` means every single row of column 0. So `x[:,0].uniform_(-1.,1)` is every row of column 0, I want you to grab a uniform random numbers. 

それからこれは本当に重要です。 Pythonのリストに索引を付けるのと同じように、それに索引を付けることができます。 しかし、コロンはどこにでも置くことができ、コロンはその軸/次元のすべての単一の値を意味します。 ここでの `x [：、0]`は列0のすべての行を意味します。したがって `x [：、0] .uniform _（ -  1.、1）`は列0のすべての行です。 乱数

Here is another very important concept in PyTorch. Anytime you've got a function that ends with an underscore, it means don't return to me that uniform random number, but replace whatever this is being called on with the result of this function.  So this `x[:,0].uniform_(-1.,1)` takes column 0 and replaces it with a uniform random number between -1 and 1. So there's a lot to unpack there. 

これがPyTorchにおけるもう一つの非常に重要な概念です。 あなたがアンダースコアで終わる関数を持っているときはいつでも、それは私にその一様な乱数を返さないことを意味しますが、これがこの関数の結果で呼び出されているものは何でも置き換えます。 それで、この `x [：、0] .uniform _（ -  1.、1）`は列0を取り、-1から1の間の一様乱数に置き換えます。そこでそこに展開することがたくさんあります。

![](lesson2/30.png)

But the good news is these two lines of code and `x@a` which we are coming to cover 95% of what you need to know about PyTorch. 

しかし、幸いなことに、これら2行のコードと `x @ a`が、PyTorchについて知っておくべきことの95％をカバーするようになります。

1. How to create an array
2. How to change things in an array
3. How to do matrix operations on an array

1.配列の作り方
配列内のものを変更する方法
3.配列に対して行列演算をする方法

So there's a lot to unpack, but these small number of concepts are incredibly powerful. So I can now print out the first five rows. `[:5]` is a standard Python slicing syntax to say the first 5 rows. So here are the first 5 rows, 2 columns looking like﹣my random numbers and my 1's. 

そのため、開梱するものはたくさんありますが、これらの少数の概念は非常に強力です。 だから私は今、最初の5行を印刷することができます。 `[：5]`は最初の5行を言う標準のPythonスライス構文です。 だからここに最初の5行、2列が﹣私の乱数と私の1のように見えます。

Now I can do a matrix product of that `x` by my `a`, add in some random numbers to add a bit of noise.  

今、私は私の `a`によってその` x`の行列積をすることができます。少しのノイズを加えるためにいくつかの乱数を加えます。

```python
y = x@a + torch.rand(n)
```

Then I can do a scatter plot. I'm not really interested in my scatter plot in this column of ones. They are just there to make my linear function more convenient. So I'm just going to plot my zero index column against my `y`'s.  

それから私は散布図をすることができます。 このコラムの散布図にはあまり興味がありません。 それらは私の線形関数をより便利にするためにあるだけです。 だから私はちょうど私の `y`に対して私のゼロインデックスカラムをプロットするつもりです。

```python
plt.scatter(x[:,0], y);
```

 ![](lesson2/n1.png) 

`plt` is what we universally use to refer to the plotting library, matplotlib. That's what most people use for most of their plotting in scientific Python. It's certainly a library you'll want to get familiar with because being able to plot things is really important. There are lots of other plotting packages. Lots of the other packages are better at certain things than matplotlib, but matplotlib can do everything reasonably well. Sometimes it's a little awkward, but for me, I do pretty much everything in matplotlib because there is really nothing it can't do even though some libraries can do other things a little bit better or prettier. But it's really powerful so once you know matplotlib, you can do everything. So here, I'm asking matplotlib to give me a scatterplot with my `x`'s against my `y`'s. So this is my dummy data representing temperature and ice cream sales. 

`plt`はプロットライブラリmatplotlibを指すために私たちが普遍的に使っているものです。 それが科学的なPythonでのプロットの大部分にほとんどの人が使っているものです。 物事を描くことができることは非常に重要であるため、それは確かにあなたが慣れ親しむことを望むライブラリーです。 他にもたくさんのプロットパッケージがあります。 他の多くのパッケージはmatplotlibよりも特定の点で優れていますが、matplotlibはすべてを合理的にうまくやることができます。 少しぎこちないこともありますが、私にとっては、matplotlibでほとんどすべてのことを実行しています。ライブラリによっては、他のことをもう少しうまくやることができても、できないことは何もないからです。 しかし、それは本当に強力なので、matplotlibを知ってしまえば、すべてを実行できます。 それで、ここで、私は私の `x`と私の` y`の散布図を私に与えるようにmatplotlibに頼んでいます。 だからこれは私の温度とアイスクリームの売り上げ高を表すダミーデータです。

[[1:26:18]](https://youtu.be/Egp4Zajhzog?t=5178)

Now what we're going to do is, we are going to pretend we were given this data and we don't know that the values of our coefficients are 3 and 2. So we're going to pretend that we never knew that and we have to figure them out. How would we figure them out? How would we draw a line to fit this data and why would that even be interesting? Well, we're going to look at more about why it's interesting in just a moment. But the basic idea is:

今、私たちがやろうとしていることは、このデータが与えられたふりをすることであり、そして私たちの係数の値が3と2であることを知らないということです。 我々はそれらを理解しなければなりません。 どのように我々はそれらを理解するでしょうか？ どのようにしてこのデータに合うように線を引くのでしょうか。それがなぜそれがさらに興味深いものになるのでしょうか。 さて、私たちはなぜそれがほんの少しの間おもしろいのかについてもっと調べるつもりです。 しかし基本的な考え方は、

>If we can find a way to find those two parameters to fit that line to those 100 points, we can also fit these arbitrary functions that convert from pixel values to probabilities.

>その線をそれらの100ポイントに合わせるための2つのパラメータを見つける方法が見つかれば、ピクセル値から確率に変換するこれらの任意の関数も当てはめることができます。

It will turn out that these techniques that we're going to learn to find these two numbers works equally well for the 50 million numbers in ResNet34. So we're actually going to use an almost identical approach. This is the bit that I found in previous classes people have the most trouble digesting. I often find, even after week 4 or week 5, people will come up to me and say:

この2つの数字を見つけるために私たちが学習しようとしているこれらのテクニックは、ResNet34の5000万の数字に対しても同様にうまく機能することがわかります。 だから私たちは実際にはほぼ同じアプローチを使うつもりです。 これは私が以前のクラスで私が見つけたことが最も苦労しているダイジェストです。 4週目や5週目を過ぎても、人々が私のところにやってきてこう言うでしょう。

Student: I don't get it. How do we actually train these models?

Jeremy: It's SGD. It's that thing we saw in the notebook with the 2 numbers.

Student: yeah, but... but we are fitting a neural network. 

Jeremy: I know and we can't print the 50 million numbers anymore, but it's literally identically doing the same thing.

学生：わかりません。 実際にどのようにこれらのモデルを訓練するのですか？

ジェレミー：それはSGDです。 それは私たちが2つの数字でノートブックで見たことです。

学生：ええ、でも…でも、ニューラルネットワークを当てはめています。

Jeremy：5000万の数字を印刷することはもうできませんが、文字通り同じことをしています。

The reason this is hard to digest is that the human brain has a lot of trouble conceptualizing of what an equation with 50 million numbers looks like and can do. So for now, you'll have to take my word for it. It can do things like recognize teddy bears. All these functions turn out to be very powerful. We're going to learn about how to make them extra powerful. But for now, this thing we're going to learn to fit these two numbers is the same thing that we've just been using to fit 50 million numbers. 

これを消化するのが難しい理由は、人間の脳が5000万の数を持つ方程式がどのように見えるか、そしてできることを概念化するのに多くの問題を抱えているからです。 だから今のところ、あなたはそれを私の言葉で伝えなければならないでしょう。 それはテディベアを認識するようなことをすることができます。 これらすべての機能は非常に強力であることがわかります。 それらをさらに強力にする方法について学習します。 しかし、今のところ、これら2つの数字を合わせるために学習することになるのは、5000万の数字を合わせるために使用してきたことと同じことです。

### Loss function [[1:28:36](https://youtu.be/Egp4Zajhzog?t=5316)]
損失関数

We want to find what PyTorch calls **parameters**, or in statistics, you'll often hear it called coefficient (i.e. these values of <img src="http://latex.codecogs.com/gif.latex?a_1" title="a_1" /> and <img src="http://latex.codecogs.com/gif.latex?a_2" title="a_2" />). We want to find these parameters such that the line that they create minimizes the error between that line and the points. In other words, if the <img src="http://latex.codecogs.com/gif.latex?a_1" title="a_1" /> and <img src="http://latex.codecogs.com/gif.latex?a_2" title="a_2" /> we came up with resulted in this line: 

PyTorchが** parameters **と呼んでいるもの、または統計では、係数と呼ばれることがよくあります（つまり、<img src = "http://latex.codecogs.com/gif.latex?a_1の値です。 "title =" a_1 "/>および<img src =" http://latex.codecogs.com/gif.latex?a_2 "title =" a_2 "/>）。 作成する線がその線と点の間の誤差を最小にするように、これらのパラメータを見つけたいと思います。 つまり、<img src = "http://latex.codecogs.com/gif.latex?a_1" title = "a_1" />および<img src = "http://latex.codecogs.com/の場合 gif.latex？a_2 "title =" a_2 "/>我々が思いついたのはこの行になった：

![](lesson2/31.png) 

Then we'd look and we'd see how far away is that line from each point. That's quite a long way. So maybe there was some other <img src="http://latex.codecogs.com/gif.latex?a_1" title="a_1" /> and <img src="http://latex.codecogs.com/gif.latex?a_2" title="a_2" /> which resulted in the gray line. And they would say how far away is each of those points. And then eventually we come up with the yellow line. In this case, each of those is actually very close.

それでは見てみると、各点からその線がどれだけ離れているかがわかります。 それはかなり長い道のりです。 そのため、他にもいくつか<img src = "http://latex.codecogs.com/gif.latex?a_1" title = "a_1" />や<img src = "http://latex.codecogs.com/があった可能性があります。 gif.latex？a_2 "title =" a_2 "/>これにより、灰色の線が表示されます。 そして、彼らはそれらのポイントのそれぞれがどれだけ離れているかを言うでしょう。 そして結局私達は黄色い線を思いつく。 この場合、それらのそれぞれは実際には非常に近いです。

![](lesson2/32.png)

So you can see how in each case we can say how far away is the line at each spot away from its point, and then we can take the average of all those. That's called the **loss**. That is the value of our loss. So you need a mathematical function that can basically say how far away is this line from those points. 


それで、あなたはそれぞれの場合に、その地点からそれぞれの地点での線がどれだけ離れているかを言うことができるのを見ることができ、それから我々はそれらすべての平均をとることができる。 それは**ロス**と呼ばれています。 それが私たちの損失の価値です。 それで、あなたは基本的にこれらの点からこの線がどれだけ離れているかを言うことができる数学関数が必要です。

For this kind of problem which is called a regression problem (a problem where your dependent variable is continuous, so rather than being grizzlies or teddies, it's some number between -1 and 6), the most common loss function is called mean squared error which pretty much everybody calls MSE. You may also see RMSE which is root mean squared error. The mean squared error is a loss which is the difference between some predictions that you made which is like the value of the line and the actual number of ice cream sales. In the mathematics of this, people normally refer to the actual as <img src="http://latex.codecogs.com/gif.latex?y" title="y" /> and the prediction, they normally call it <img src="https://latex.codecogs.com/gif.latex?\hat{y}" title="\hat{y}" /> (y hat).  

回帰問題と呼ばれるこの種の問題（従属変数が連続的で、グリズリーやテディではなく-1から6の間の数である問題）では、最も一般的な損失関数は平均二乗誤差と呼ばれます。 ほとんどの人がMSEを呼び出します。 二乗平均平方根誤差であるRMSEも表示される場合があります。 平均二乗誤差は、行の値と実際のアイスクリームの販売数のような、あなたが行ったいくつかの予測の間の差である損失です。 この数学では、人々は通常、実際のことを<img src = "http://latex.codecogs.com/gif.latex?y" title = "y" />およびその予測と呼び、通常は<それを<と呼びます。 img src = "https://latex.codecogs.com/gif.latex?\hat{y}" title = "\ hat {y}" />（yハット）

When writing something like mean squared error equation, there is no point writing "ice cream" and "temperature" because we want it to apply to anything. So we tend to use these mathematical placeholders. 

平均二乗誤差方程式のようなものを書くとき、「アイスクリーム」と「温度」を書いても意味がありません。 だから私たちはこれらの数学的プレースホルダーを使う傾向がある。

So the value of mean squared error is simply the difference between those two (`y_hat-y`) squared. Then we can take the mean because both `y_hat` and `y ` are rank 1 tensors, so we subtract one vector from another vector, it does something called "element-wise arithmetic" in other words, it subtracts each one from each other, so we end up with a vector of differences. Then if we take the square of that, it squares everything in that vector. So then we can take the mean of that to find the average square of the differences between the actuals and the predictions. 

そのため、平均二乗誤差の値は単純にそれらの2つの差（ `y_hat-y`）の二乗になります。 それから、 `y_hat`と` y`はどちらもランク1テンソルなので、平均をとることができます。したがって、あるベクトルを別のベクトルから減算します。つまり、「要素ごとの算術」と呼ばれる処理を行います。 だから、私たちは違いのベクトルになってしまいます。 それで、その二乗を取ると、それはそのベクトル内のすべてを二乗します。 それで、その平均を取って、実際と予測の差の二乗平均を見つけることができます。

```python
def mse(y_hat, y): return ((y_hat-y)**2).mean()
```

If you're more comfortable with mathematical notation, what we just wrote was:
あなたがより数学的な表記法に慣れているならば、私たちが今書いたのは次のとおりです。

<img src="https://latex.codecogs.com/gif.latex?\frac{\sum&space;(\hat{y}-y)^2}{n}" title="\frac{\sum (\hat{y}-y)^2}{n}" />

One of the things I'll note here is, I don't think `((y_hat-y)**2).mean()` is more complicated or unwieldy than <img src="https://latex.codecogs.com/gif.latex?\frac{\sum&space;(\hat{y}-y)^2}{n}" title="\frac{\sum (\hat{y}-y)^2}{n}" /> but the benefit of the code is you can experiment with it. Once you've defined it, you can use it, you can send things into it, get stuff out of it, and see how it works. So for me, most of the time, I prefer to explain things with code rather than with math. Because they are the same, just different notations. But one of the notations is executable. It's something you can experiment with. And the other is abstract. That's why I'm generally going to show code. 

ここで注意することの1つは、 `（（y_hat-y）** 2）.mean（）`が<img src = "https：//latex.codecogsより複雑で扱いにくいとは思わないことです。 .com / gif.latex？\ frac {\ sum＆space;（\ hat {y} -y）^ 2} {n} "title =" \ frac {\ sum（\ hat {y} -y）^ 2} { しかしコードの利点はあなたがそれを試すことができるということです。 それを定義したら、それを使用することができます、あなたはそれに物事を送ることができます、それからものを取り出し、そしてそれがどのように機能するかを見ることができます。 だから私にとって、ほとんどの場合、私は数学ではなくコードで物事を説明することを好む。 それらは同じなので、表記が異なるだけです。 しかし、表記の1つは実行可能です。 それはあなたが試すことができるものです。 そしてもう一つは抽象的です。 だから私は一般的にコードを見せるつもりです。

So the good news is, if you're a coder with not much of a math background, actually you do have a math background. Because code is math. If you've got more of a math background and less of a code background, then actually a lot of the stuff that you learned from math is going to translate directly into code and now you can start to experiment with your math. 

それで良いニュースは、あなたがあまり数学の背景を持っていないコーダーであれば、実際にはあなたは数学の背景を持っているということです。 コードは数学だからです。 あなたがより多くの数学の背景とより少ないコードの背景を持っているなら、実際にあなたが数学から学んだことの多くは直接コードに変換されることになっています、そして今あなたはあなたの数学を試すことができます。

[[1:34:03](https://youtu.be/Egp4Zajhzog?t=5643)]

 `mse` is a loss function. This is something that tells us how good our line is. Now we have to come up with what is the line that fits through here. Remember, we are going to pretend we don't know. So what you actually have to do is you have to guess. You actually have to come up with a guess what are the values of <img src="http://latex.codecogs.com/gif.latex?a_1" title="a_1" /> and <img src="http://latex.codecogs.com/gif.latex?a_2" title="a_2" />. So let's say we guess that <img src="http://latex.codecogs.com/gif.latex?a_1" title="a_1" /> and <img src="http://latex.codecogs.com/gif.latex?a_2" title="a_2" /> are -1 and 1.

`mse`は損失関数です。 これは、私たちのラインがどれほど優れているかを教えてくれるものです。 今、私たちはここに収まる線が何であるかを考え出す必要があります。 覚えておいて、私たちは知らないふりをするつもりです。 だからあなたが実際にしなければならないのはあなたが推測しなければならないということです。 実際には、<img src = "http://latex.codecogs.com/gif.latex?a_1" title = "a_1" />および<img src = "httpの値は何かを推測する必要があります。 //latex.codecogs.com/gif.latex?a_2 "title =" a_2 "/>。 それでは、<img src = "http://latex.codecogs.com/gif.latex?a_1" title = "a_1" />および<img src = "http://latex.codecogs.com/とします。 gif.latex？a_2 "title =" a_2 "/>は-1と1です。

```python
a = tensor(-1.,1)
```

Here is how you create that tensor and I wanted to write it this way because you'll see this all the time. Written out fully, it would be `tensor(-1.0, 1.0)`. We can't write it without the point because `tensor(-1, 1)` is now an int, not a floating point. So that's going to spit the dummy (Australian for "behave in a bad-tempered or petulant way") if you try to do calculations with that in neural nets. 

これがあなたがそのテンソルをどのように作成するかであり、私はあなたがこれをいつも見ているのでこのようにそれを書きたかったです。 完全に書くと、それは `tensor（-1.0、1.0）`になります。 `tensor（-1、1）`は今や浮動小数点ではなく整数になっているので、私達はそれを書くことができません。 それであなたがニューラルネットでそれを使って計算しようとするならば、それはダミーを吐き出すことになっています（オーストラリア人のために「悪くされたまたはかわいそうな方法で振る舞う」）。

I'm far too lazy to type `.0` every time. Python knows perfectly well that if you added `.` next to any of these numbers, then the whole thing is now floats. So that's why you'll often see it written this way, particularly by lazy people like me. 

毎回 `.0`を打つのが面倒すぎます。 Pythonは、あなたがこれらの数字のいずれかの隣に `.`を追加すると、全体がフロートになることを完全に知っています。 それで、あなたはそれがこのように書かれているのをよく見る理由です、特に私のような怠惰な人々によって。

So `a` is a tensor. You can see it's floating-point. You see, even PyTorch is lazy. They just put a dot. They don't bother with a zero.

だから `a`はテンソルです。 あなたはそれが浮動小数点であることがわかります。 PyTorchでさえ怠惰です。 彼らはただ点を打った。 彼らはゼロで気にしません。

![](lesson2/33.png)

But if you want to actually see exactly what it is, you can write `.type()` and you can see it's a FloatTensor:

しかし、実際にそれを正確に見たいのであれば、 `.type（）`と書くことができ、それがFloatTensorであることがわかります。

![](lesson2/34.png)

So now we can calculate our predictions with this random guess. `x@a` a matrix product of `x` and `a`. And we can now calculate the mean squared error of our predictions and their actuals, and that's our loss. So for this regression, our loss is 0.9.

だから今、私たちはこのランダムな推測で私たちの予測を計算することができます。 `x @ a`は` x`と `a`の行列積です。 そして、予測と実際の平均二乗誤差を計算することができます。それが損失です。 したがって、この回帰では、損失は0.9です。

```python
y_hat = x@a
mse(y_hat, y)
```

```
tensor(8.8945)
```



So we can now plot a scatter plot of `x` against `y` and we can plot the scatter plot of `x` against `y_hat`.  And there they are.

それで、 `x`の散布図を` y`に対してプロットし、 `x`の散布図を` y_hat`に対してプロットすることができます。 そしてそこにあります。

```python
plt.scatter(x[:,0],y)
plt.scatter(x[:,0],y_hat);
```

![](lesson2/n2.png)

 So that is not great﹣not surprising. It's just a guess. So SGD or gradient descent more generally and anybody who's done engineering or probably computer science at school would have done plenty of this like Newton's method, etc at university. If you didn't, don't worry. We're going to learn it now. 

それはそれほど素晴らしいことではありません。 それはただの推測です。 つまり、SGDやグラジエントディセント、そして学校で工学やコンピュータサイエンスをやっている人なら誰でも、大学ではニュートン法などの多くのことをやっていたでしょう。 そうでなかったら、心配しないでください。 私たちは今それを学ぶつもりです。

It's basically about taking this guess and trying to make it a little bit better. How do we make it a little better? Well, there are only two numbers and the two numbers are the two numbers are the intercept of the orange line and the gradient of the orange line. So what we are going to do with gradient descent is we're going to simply say:

それは基本的にこの推測をしてそれをもう少し良くしようとすることです。 どうやってそれを少し良くするのですか？ さて、2つの数字だけがあり、2つの数字はオレンジ色の線の切片とオレンジ色の線のグラデーションです。 それで、勾配降下法を使ってやろうとしているのは、単にこう言うことです。

- What if we changed those two numbers a little bit? 
  - What if we made the intercept a little bit higher or a little bit lower?
  - What if we made the gradient a little bit more positive or a little bit more negative?

 - これら2つの数字を少し変更したらどうなりますか？
    - 切片を少し高くしたり少し低くしたりした場合はどうなりますか。
    - グラデーションをもう少しポジティブにするか、もう少しネガティブにするとどうなりますか。
    
![](lesson2/35.png)

There are 4 possibilities and then we can calculate the loss for each of those 4 possibilities and see what works. Did lifting it up or down make it better? Did tilting it more positive or more negative make it better? And then all we do is we say, okay, whichever one of those made it better, that's what we're going to do. That's it.

4つの可能性があり、それからそれら4つの可能性のそれぞれについて損失を計算し、どのように機能するかを見ることができます。 それを上下に持ち上げてそれを改善しましたか？ それをもっと積極的にあるいはもっと否定的に傾けることはそれをより良くしましたか？ そして、私たちがするのは、私たちが言うことだけです、大丈夫、それらのうちの1つがそれをより良くした、それが私たちがやろうとしていることです。 それでおしまい。

But here is the cool thing for those of you that remember calculus. You don't actually have to move it up and down, and round about. You can actually calculate the derivative. The derivative is the thing that tells you would moving it up or down make it better, or would rotating it this way or that way make it better. The good news is if you didn't do calculus or you don't remember calculus, I just told you everything you need to know about it. It tells you how changing one thing changes the function. That's what the derivative is, kind of, not quite strictly speaking, but close enough, also called the gradient. The gradient or the derivative tells you how changing <img src="http://latex.codecogs.com/gif.latex?a_1" title="a_1" /> up or down would change our MSE, how changing <img src="http://latex.codecogs.com/gif.latex?a_2" title="a_2" /> up or down would change our MSE, and this does it more quickly than actually moving it up and down. 

しかし、これが微積分学を覚えているあなた方のための素晴らしいことです。 あなたは実際にそれを上下に動かしたり、丸めたりする必要はありません。 あなたは実際に微分を計算することができます。 微分とは、上下に動かすと音が良くなる、またはこのように回転させると音が良くなると言うことです。 あなたが微積分学をしなかったか、または微積分学を覚えていないならば、良いニュースはあなたがそれについて知る必要があるすべてをあなたに言いました。 それは一つのことを変えることがどのように機能を変えるかをあなたに教えます。 それが微分の種類です。厳密に言えば、グラデーションとも呼ばれます。 勾配または導関数は、<img src = "http://latex.codecogs.com/gif.latex?a_1" title = "a_1" />上下に変更するとMSEがどのように変更されるか、<img srcを変更すると変化することを示します。 = "http://latex.codecogs.com/gif.latex?a_2" title = "a_2" />上または下に移動すると、MSEが変わります。実際に上下に移動するよりも早くなります。

In school, unfortunately, they forced us to sit there and calculate these derivatives by hand. We have computers. Computers can do that for us. We are not going to calculate them by hand. 

学校では、残念なことに、彼らは私たちがそこに座ってこれらの導関数を手で計算することを強いました。 私たちはコンピューターを持っています。 コンピュータは私たちのためにそれをすることができます。 我々はそれらを手作業で計算するつもりはない。

```python
a = nn.Parameter(a); a
```

```
Parameter containing:
tensor([-1.,  1.], requires_grad=True)
```



[[1:39:12]](https://youtu.be/Egp4Zajhzog?t=5952)

Instead, we're doing to call `.backward()`. On our computer, that will calculate the gradient for us.  

代わりに、 `。backward（）`を呼び出すことにしています。 私たちのコンピューターでは、それが私たちの勾配を計算します。

```python
def update():
    y_hat = x@a
    loss = mse(y, y_hat)
    if t % 10 == 0: print(loss)
    loss.backward()
    with torch.no_grad():
        a.sub_(lr * a.grad)
        a.grad.zero_()
```

So here is what we're going to do. We are going to create a loop. We're going to loop through 100 times, and we're going to call a function called `update`. That function is going to:

だからここに私たちがやろうとしていることです。 ループを作ります。 100回ループして、 `update`という関数を呼びます。 その機能は次のとおりです。

- Calculate `y_hat` (i.e. our prediction)

- Calculate loss (i.e. our mean squared error)

- From time to time, it will print that out so we can see how we're going

 -  `y_hat`を計算します（すなわち私達の予測）

 - 損失を計算する（つまり、平均二乗誤差）

 - 時々、それがプリントアウトされるので、私達は自分達がどのように進んでいるかを見ることができます
 
- Calculate the gradient. In PyTorch, calculating the gradient is done by using a method called `backward`. Mean squared error was just a simple standard mathematical function. PyTorch keeps track of how it was calculated and lets us calculate the derivative. So if you do a mathematical operation on a tensor in PyTorch, you can call `backward` to calculate the derivative and the derivative gets stuck inside an attribute called `.grad`. 

 - 勾配を計算します。 PyTorchでは、勾配の計算は `backward`というメソッドを使って行われます。 二乗平均誤差は、単純な標準数学関数です。 PyTorchはそれがどのように計算されたかを追跡して、導関数を計算させます。 ですから、PyTorchでテンソルの上で数学的な操作をするならば、導関数を計算するために `backward`を呼び出すことができ、導関数は` .grad`と呼ばれる属性の中で動けなくなります。
 
- Take my coefficients and I'm going to subtract from them my gradient (`sub_`). There is an underscore there because that's going to do it in-place. It's going to actually update those coefficients `a` to subtract the gradients from them. Why do we subtract? Because the gradient tells us if I move the whole thing downwards, the loss goes up. If I move the whole thing upwards, the loss goes down. So I want to do the opposite of the thing that makes it go up. We want our loss to be small. That's why we subtract.

 - 私の係数を取り、私はそれらから私の勾配（ `sub_`）を引くつもりです。 それはその場でそれをやろうとしているのでそこにアンダースコアがあります。 それらから勾配を引くために実際にそれらの係数「ａ」を更新しようとしている。 なぜ我々は引きますか？ 全体を下に移動するとグラデーションが表示されるため、損失が大きくなります。 全体を上に動かすと、損失は減ります。 それで、私はそれを上がらせることの反対のことをしたいです。 私たちは私たちの損失を小さくしたいのです。 だからこそ私たちは引きます。
 
- `lr` is our learning rate. All it is is the thing that we multiply by the gradient. Why is there any `lr` at all? Let me show you why.

- 「lr」は私たちの学習率です。 それはグラデーションを掛けることだけです。 どうして `lr`があるの？ その理由をお見せしましょう。

#### Why is there any LR at all? [[1:41:31](https://youtu.be/Egp4Zajhzog?t=6091)]
なぜLRがあるのでしょうか。

Let's take a really simple example, a quadratic. And let's say your algorithm's job was to find where that quadratic was at its lowest point. How could it do this? Just like what we're doing now, the starting point would be just to pick some x value at random. Then find out what the value of y is. That's the starting point. Then it can calculate the gradient and the gradient is simply the slope, but it tells you moving in which direction is make you go down. So the gradient tells you, you have to go this way.
![](lesson2/lr.gif)

本当に簡単な例、2次式を見てみましょう。 そして、あなたのアルゴリズムの仕事は、その2次式がその最低点にある場所を見つけることであったとしましょう。 どうすればこれができますか？ 今行っていることと同じように、出発点はx値をランダムに選ぶことです。 次に、yの値が何であるかを調べます。 それが出発点です。 それからそれは勾配を計算することができます、そして、勾配は単に勾配です、しかしそれはあなたが下ることになっているどの方向に動いているかをあなたに伝えます。 だからグラデーションはあなたに伝えます、あなたはこのように行かなければなりません。
![](lesson2/lr.gif)

- If the gradient was really big, you might jump left a very long way, so you might jump all the way over to here. If you jumped over there, then that's actually not going to be very helpful because it's worse. We jumped too far so we don't want to jump too far. 

 - グラデーションが本当に大きかったなら、あなたは非常に長い道のりを左に飛び越えるかもしれないので、あなたはここまでずっとずっと飛び越えるかもしれません。 あそこに飛び越えても、それはそれがさらに悪いので実際にはあまり役に立ちません。 飛びすぎたので飛びすぎたくない。
 
- Maybe we should just jump a little bit. That is actually a little bit closer. So then we'll just do another little jump. See what the gradient is and do another little jump, and repeat. 

 - たぶんちょっとジャンプしてください。 それは実際にはもう少し近いです。 それではもう少しジャンプしましょう。 グラデーションが何であるかを見て、もう一度少しジャンプして、繰り返します。
 
- In other words, we find our gradient to tell us what direction to go and if we have to go a long way or not too far. But then we multiply it by some number less than 1 so we don't jump too far.

 - 言い換えれば、私達はどの方向に進むべきか、そして私達が長い道のりを行かなければならないのか、それほど遠くに行かなければならないのかを私達に伝えるために私達の勾配を見つけます。 しかし、それから我々はそれを1未満の数で乗算するので、私達はあまり飛びすぎないようにします。
 
Hopefully at this point, this might be reminding you of something which is what happened when our learning rate was too high. 

この時点でうまくいけば、これは私たちの学習率が高すぎたときに起こったことであることをあなたに思い出させているかもしれません。

![](lesson2/38.png)

Do you see why that happened now? Our learning rate was too high meant that we jumped all the way past the right answer further than we started with, and it got worse, and worse, and worse. So that's what a learning rate too high does. 

なぜそれが今起こったのかわかりますか？ 私たちの学習率が高すぎるということは、私たちが始めていたよりも正しい答えをはるかに超えて飛び越えたことを意味していました。 それで、学習率が高すぎるのはそういうことです。

On the other hand, if our learning rate is too low, then you just take tiny little steps and so eventually you're going to get there, but you are doing lots and lots of calculations along the way. So you really want to find something where it's either big enough steps like stairs or a little bit of back and forth. You want something that gets in there quickly but not so quickly it jumps out and diverges, not so slowly that it takes lots of steps. That's why we need a good learning rate and that's all it does.

一方、私たちの学習率が低すぎる場合、あなたはほんの少しのステップを踏むだけなので最終的にはそこに着くことになるでしょうが、あなたはその過程でたくさんそしてたくさんの計算をしています。 それで、あなたは本当にそれが階段のような十分に大きいステップまたは前後に少しのいずれかである何かを見つけたいと思います。 あなたはそこにすばやく入るがそれほど速くはないが飛び出して発散する何かがほしいと思う。 だからこそ私たちは良い学習率を必要としており、それがすべてです。

So if you look inside the source code of any deep learning library, you'll find this:

ですから、もしあなたがどんな深い学習ライブラリのソースコードの中に見ても、あなたはこれを見つけるでしょう：

 `a.sub_(lr * a.grad)`

You will find something that says "coefficients ﹣ learning rate times gradient". And we will learn about some easy but important optimization we can do to make this go faster.

あなたは「係数 - 学習率×勾配」と言う何かを見つけるでしょう。 そしてこれをより速くするために私たちができる簡単だが重要な最適化について学びます。

That's about it. There's a couple of other little minor issues that we don't need to talk about now: one involving zeroing out the gradient and other involving making sure that you turn gradient calculation off when you do the SGD update. If you are interested, we can discuss them on the forum or you can do our introduction to machine learning course which covers all the mechanics of this in more detail.

それはそれについてです。 ここでは説明する必要のない小さな問題がいくつかあります。1つはグラデーションをゼロにすること、もう1つはSGDアップデートを実行するときにグラデーション計算を確実にオフにすることです。 あなたが興味を持っているならば、我々はフォーラムでそれらを議論することができるか、あなたはより詳細にこれのすべての力学をカバーする機械学習コースへの我々の紹介をすることができます。

#### Training loop [[1:45:43](https://youtu.be/Egp4Zajhzog?t=6343)]
トレーニングループ

If we run `update` 100 times printing out the loss from time to time, you can see it starts at  8.9, and it goes down.

時々損失を出力するために `update`を100回実行すると、それは8.9から始まってそれが下がるのを見ることができます。

```python
lr = 1e-1
for t in range(100): update()
```

```
tensor(8.8945, grad_fn=<MeanBackward1>)
tensor(1.6115, grad_fn=<MeanBackward1>)
tensor(0.5759, grad_fn=<MeanBackward1>)
tensor(0.2435, grad_fn=<MeanBackward1>)
tensor(0.1356, grad_fn=<MeanBackward1>)
tensor(0.1006, grad_fn=<MeanBackward1>)
tensor(0.0892, grad_fn=<MeanBackward1>)
tensor(0.0855, grad_fn=<MeanBackward1>)
tensor(0.0843, grad_fn=<MeanBackward1>)
tensor(0.0839, grad_fn=<MeanBackward1>)
```

So you can then print out scatterplots and there it is.  

それで、散布図を印刷することができます。

```python
plt.scatter(x[:,0],y)
plt.scatter(x[:,0],x@a);
```

![](lesson2/n3.png)

 That's it! Believe it or not, that's gradient descent. So we just need to start with a function that's a bit more complex than `x@a` but as long as we have a function that can represent things like if this is a teddy bear, we now have a way to fit it.

それでおしまい！ 信じられないかもしれませんが、それは勾配降下です。 だから私たちはただ `x @ a`より少し複雑な関数から始める必要がありますが、これがテディベアのようなものを表すことができる関数を持っている限り、我々は今それをフィットさせる方法があります。

#### Animate it! [[1:46:20](https://youtu.be/Egp4Zajhzog?t=6380)]
動かしましょう！

Let's now take a look at this as an animation. This is one of the nice things that you can do with matplotlib. You can take any plot and turn it into an animation. So you can now actually see it updating each step. 

それではこれをアニメーションとして見てみましょう。 これはmatplotlibでできる素晴らしいことの1つです。 あなたはどんなプロットでもそれをアニメーションに変えることができます。 これで、各ステップを更新しているのがわかります。


```python
from matplotlib import animation, rc
rc('animation', html='html5')
```

> You may need to uncomment the following to install the necessary plugin the first time you run this:
> (after you run following commands, make sure to restart the kernal for this notebook)
> If you are running in colab, the installs are not needed; just change the cell above to be ... html='jshtml' instead of ... html='html5'

>初めて実行したときに必要なプラグインをインストールするには、以下のコメントを外す必要があります。
>（以下のコマンドを実行した後は、必ずこのノートブック用のカーネルを再起動してください）
>あなたがcolabで走っているなら、インストールは必要ありません。 上のセルを... html = 'html5'の代わりに... html = 'jshtml'に変更するだけです。

```python
#! sudo add-apt-repository -y ppa:mc3man/trusty-media  
#! sudo apt-get update -y 
#! sudo apt-get install -y ffmpeg  
#! sudo apt-get install -y frei0r-plugins
```

Let's see what we did here. We simply said, as before, create a scatter plot, but then rather than having a loop, we used matplotlib's `FuncAnimation` to call 100 times this `animate` function.  And this function just calls that `update` we created earlier then update the `y` data in our line. Repeat that 100 times, waiting 20 milliseconds after each one.  

ここで何をしたのか見てみましょう。 先ほどと同じように散布図を作成すると言っただけですが、それからループを持つのではなく、matplotlibの `FuncAnimation`を使ってこの` animate`関数を100回呼び出しました。 そしてこの関数は先ほど作成した `update`を呼び出してから行の` y`データを更新します。 100回繰り返して、それぞれ20ミリ秒待ってください。

```python
a = nn.Parameter(tensor(-1.,1))

fig = plt.figure()
plt.scatter(x[:,0], y, c='orange')
line, = plt.plot(x[:,0], x@a)
plt.close()

def animate(i):
    update()
    line.set_ydata(x@a)
    return line,

animation.FuncAnimation(fig, animate, np.arange(0, 100), interval=20)
```

![](lesson2/download.mp4)

You might think visualizing your algorithms with animations is something amazing and complex thing to do, but actually now you know it's 11 lines of code. So I think it's pretty darn cool.

アニメーションを使用してアルゴリズムを視覚化するのは驚くべき複雑なことだと思うかもしれませんが、実際には11行のコードであることがわかりました。 だから私はそれがかなりクールだと思います。

That is SGD visualized and we can't visualize as conveniently what updating 50 million parameters in a ResNet 34 looks like but basically doing the same thing. So studying these simple version is actually a great way to get an intuition. So you should try running this notebook with a really big learning rate, with a really small learning rate, and see what this animation looks like, and try to get a feel for it. Maybe you can even try a 3D plot. I haven't tried that yet, but I'm sure it would work fine.

これはSGDのビジュアライゼーションであり、ResNet 34で5000万個のパラメータを更新することはどのように見えるのでしょうが、基本的には同じことを視覚化することはできません。 だから、これらの単純なバージョンを勉強することは実際に直感を得るための素晴らしい方法です。 ですから、このノートブックを非常に大きな学習率で、非常に小さな学習率で実行して、このアニメーションがどのように見えるかを確認して、それを実感してみてください。 3Dプロットを試すこともできます。 まだ試したことはありませんが、うまくいくと確信しています。

#### Mini-batches [[1:48:08](https://youtu.be/Egp4Zajhzog?t=6488)]
ミニバッチ

The only difference between stochastic gradient descent and this is something called _mini-batches_. You'll see, what we did here was we calculated the value of the loss on the whole dataset on every iteration. But if your dataset is 1.5 million images in ImageNet, that's going to be really slow. Just to do a single update of your parameters, you've got to calculate the loss on 1.5 million images. You wouldn't want to do that. So what we do is we grab 64 images or so at a time at random, and we calculate the loss on those 64 images, and we update our weights. Then we have another 64 random images, and we update our weights. In other words, the loop basically looks exactly the same but add some random indexes on our `x` and `y` to do a mini-batch at a time, and that would be the basic difference.

確率勾配降下法とこれとの唯一の違いは_ミニバッチ_と呼ばれるものです。 ご存じのとおり、ここで行ったことは、反復ごとにデータセット全体の損失の値を計算したことです。 しかし、あなたのデータセットがImageNetの150万画像であるならば、それは本当に遅くなるでしょう。 パラメータを1回更新するだけで、150万枚の画像の損失を計算できます。 あなたはそうしたくないでしょう。 ですから、私たちがしているのは、64枚程度の画像をランダムに一度に取得し、その64枚の画像の損失を計算して、重みを更新することです。 それから我々は別の64のランダムな画像を持っていて、そして我々は我々の重みを更新する。 言い換えれば、ループは基本的に全く同じに見えますが、一度にミニバッチをするために私たちの `x`と` y`にいくつかのランダムなインデックスを追加します、そしてそれは基本的な違いでしょう。

![](lesson2/39.png)

Once you add those grab a random few points each time, those random few points are called your mini-batch, and that approach is called SGD for Stochastic Gradient Descent.

あなたがそれらを追加する度にランダムな少数のポイントをつかむと、それらのランダムな少数のポイントはあなたのミニバッチと呼ばれ、そのアプローチは確率勾配降下法のためのSGDと呼ばれます。

#### Vocabulary [[1:49:40](https://youtu.be/Egp4Zajhzog?t=6580)]
ボキャブラリー

There's quite a bit of vocab we've just covered, so let's remind ourselves. 

ここで取り上げた語彙はかなりたくさんありますので、思い出してください。

- **Learning rate**: A thing we multiply our gradient by to decide how much to update the weights by.

 -  **学習率**：重みを更新する量を決定するために勾配を掛けます。
 
- **Epoch**: One complete run through all of our data points (e.g. all of our images). So for non-stochastic gradient descent we just did, every single loop, we did the entire dataset. But if you've got a dataset with a thousand images and our mini-batch size is 100, then it would take you 10 iterations to see every image once. So that would be one epoch. Epochs are important because if you do lots of epochs, then you are looking at your images lots of times, so every time you see an image, there's a bigger chance of overfitting. So we generally don't want to do too many epochs.

 -  **エポック**：私たちのすべてのデータポイント（たとえば、私たちのすべての画像）を一通り実行します。 そのため、非確率的勾配降下法では、ループごとにデータセット全体を作成しました。 しかし、1000個の画像を含むデータセットがあり、私たちのミニバッチサイズが100の場合、すべての画像を一度に見るには10回の繰り返しが必要になります。 だからそれは一つの時代になるでしょう。 エポックは重要です。エポックを何度も行うと、画像を何度も見ているので、画像を表示するたびにオーバーフィットする可能性が高くなります。 だから私たちは一般的にあまりにも多くの時代をやりたくはありません。
 
- **Mini-batch**: A random bunch of points that you use to update your weights.

-  **ミニバッチ**：あなたがあなたの体重を更新するために使用するポイントのランダムな束。

- **SGD**: Stochastic gradient descent using mini-batches.

 -  ** SGD **：ミニバッチを使用した確率的勾配降下。
 
- **Model / Architecture**: They kind of mean the same thing. In this case, our architecture is <img src="http://latex.codecogs.com/gif.latex?\vec{y}&space;=&space;X\vec{a}" title="\vec{y} = X\vec{a}" />﹣ the architecture is the mathematical function that you're fitting the parameters to. And we're going to learn later today or next week what the mathematical function of things like ResNet34 actually is. But it's basically pretty much what you've just seen. It's a bunch of matrix products. 

 -  **モデル/アーキテクチャ**：それらは一種の同じ意味です。 この場合、私たちのアーキテクチャは<img src = "http://latex.codecogs.com/gif.latex?\vec{y}&space;=&space;X\vec{a}" title = "\ vec {yです。 } = X \ vec {a} "/>  - アーキテクチャは、パラメータを当てはめている数学関数です。 そして、今日または来週、ResNet34のようなものの数学的機能が実際にどのようなものであるかを学びます。 しかし、それは基本的にあなたが今見たものとほとんど同じです。 それはマトリックス製品の束です。
 
- **Parameters / Coefficients / Weights**: Numbers that you are updating.

 -  **パラメータ/係数/重み**：あなたが更新している数。
 
- **Loss function**: The thing that's telling you how far away or how close you are to the correct answer. For classification problems, we use *cross entropy loss*, also known as *negative log likelihood loss*. This penalizes incorrect confident predictions, and correct unconfident predictions.

 -  **損失関数**：あなたがどれだけ遠くにいるか、またはどれだけあなたが正しい答えに近いかを言っていること。 分類問題のために、*負の対数尤度損失*としても知られている*クロスエントロピー損失*を使用します。 これは、誤った自信のある予測と、正しくない自信のある予測を不利にします。
 
[1:51:45](https://youtu.be/Egp4Zajhzog?t=6705)

These models / predictors / teddy bear classifiers are functions that take pixel values and return probabilities. They start with some functional form like <img src="http://latex.codecogs.com/gif.latex?\vec{y}&space;=&space;X\vec{a}" title="\vec{y} = X\vec{a}" /> and they fit the parameter `a` using SGD to try and do the best to calculate your predictions. So far, we've learned how to do regression which is a single number. Next we'll learn how to do the same thing for classification where we have multiple numbers, but basically the same. 

これらのモデル/予測子/テディベア分類器は、ピクセル値を取得して確率を返す関数です。 これらは<img src = "http://latex.codecogs.com/gif.latex?\vec{y}&space;=&space;X\vec{a}" title = "\ vec {yのような機能的な形式で始まります。 } = X \ vec {a} "/>そしてSGDを使ってパラメータ` a`を当てはめて予測を計算しようとします。 これまで、単一の数値である回帰を行う方法を学びました。 次に、複数の数がある場合でも分類のために同じことを行う方法を学びますが、基本的には同じです。

In the process, we had to do some math. We had to do some linear algebra and calculus and a lot of people get a bit scared at that point and tell us "I am not a math person". If that's you, that's totally okay. But you are wrong. You are a math person. In fact, it turns out that in the actual academic research around this, there are not "math people" and "non-math people". It turns out to be entirely a result of culture and expectations. So you should check out Rachel's talk: 

その過程で、我々はいくつかの数学をしなければならなかった。 我々はいくつかの線形代数と微積分学をしなければなりませんでした、そして、多くの人々はその時点で少し怖がって、そして私たちに「私は数学者ではない」と言います。 それがあなたなら、それはまったく問題ありません。 しかし、あなたは間違っています。 あなたは数学者です。 実際、これをめぐる実際の学術研究には、「数学者」と「非数学者」はいません。 それは完全に文化と期待の結果です。 それで、あなたはレイチェルの話をチェックするべきです：

[There is no such thing as "not a math person"](https://www.youtube.com/watch?v=q6DGVGJ1WP4)
「数学者ではない」というようなことはありません。

![](lesson2/rachel.png)

She will introduce you to some of that academic research. If you think of yourself as not a math person, you should watch this so that you learn that you're wrong that your thoughts are actually there because somebody has told you you're not a math person. But there's actually no academic research to suggest that there is such a thing. In fact, there are some cultures like Romania and China where the "not a math person" concept never even appeared. It's almost unheard of in some cultures for somebody to say I'm not a math person because that just never entered that cultural identity.

彼女はその学術研究のいくつかを紹介します。 あなたが自分自身を数学者ではないと考えるならば、あなたはこれを見てあなたが間違っていることを学ぶべきです。 しかし、そのようなことがあることを示唆する学術研究は実際にはありません。 実際、ルーマニアや中国のように、「数学ではない」という概念が登場しないような文化もあります。 一部の文化では、私が数学者ではないと言うのはほとんど前例のないことですが、それはその文化的アイデンティティには決して入っていないからです。

So don't freak out if words like derivative, gradient, and matrix product are things that you're kind of scared of. It's something you can learn. Something you'll be okay with.

だから派生語、グラデーション、そして行列積のような言葉があなたがちょっと怖いものであっても気にしないでください。 それはあなたが学ぶことができるものです。 あなたが大丈夫になるだろう何か。

#### Underfitting and Overfitting [[1:54:42](https://youtu.be/Egp4Zajhzog?t=6882)]
アンダーフィットとオーバーフィット

The last thing I want to close with is the idea of underfitting and overfitting. We just fit a line to our data. But imagine that our data wasn't actually line shaped. So if we try to fit which was something like constant + constant times X (i.e. a line) to it, it's never going to fit very well. No matter how much we change these two coefficients, it's never going to get really close.

最後にやりたいのは、アンダーフィットとオーバーフィットのアイデアです。 私たちは自分のデータに一行を合わせるだけです。 しかし、私たちのデータは実際には線状ではなかったと想像してください。 ですから、それが定数+定数倍X（つまり1行）のようなものに当てはめようとすると、それは決してうまく適合することはありません。 これら2つの係数をいくら変更しても、実際には近づきません。

![](lesson2/40.png)

On the other hand, we could fit some much bigger equation, so in this case it's a higher degree polynomial with lots of wiggly bits. But if we did that, it's very unlikely we go and look at some other place to find out the temperature and how much ice cream they are selling and we will get a good result. Because the wiggles are far too wiggly. So this is called overfitting. 

一方、もっと大きな方程式を当てはめることができるので、この場合、それは多くの曖昧なビットを含む高次多項式です。 しかし、それをしたのであれば、他の場所で気温やアイスクリームの売り上げを調べに行っても、あまり良い結果は得られないでしょう。 なぜならば、くねりはあまりにもひどすぎるからです。 それでこれは過剰適合と呼ばれます。

We are looking for some mathematical function that fits just right to stay with a teddy bear analogies. You might think if you have a statistics background, the way to make things it just right is to have exactly the same number of parameters (i.e. to use a mathematical function that doesn't have too many parameters in it). It turns out that's actually completely not the right way to think about it. 

私たちはテディベアの類推にとどまるのにちょうどよく適合するいくつかの数学関数を探しています。 もしあなたが統計的な背景を持っているのであれば、ちょうどいいものを作るための方法は正確に同じ数のパラメータを持つこと（つまり、あまりにも多くのパラメータを持たない数学的関数を使うこと）です。 それは実際にはそれについて考えるための正しい方法ではありません。

### Regularization and Validation Set [[1:56:07](https://youtu.be/Egp4Zajhzog?t=6967)]
正則化および検証セット

There are other ways to make sure that we don't overfit. In general, this is called regularization. Regularization or all the techniques to make sure when we train our model that it's going to work not only well on the data it's seen but on the data it hasn't seen yet. The most important thing to know when you've trained a model is actually how well does it work on data that it hasn't been trained with. As we're going to learn a lot about next week, that's why we have this thing called a validation set. 

やり過ぎないようにする方法は他にもあります。 一般に、これは正則化と呼ばれます。 モデルをトレーニングするときに、表示されているデータだけでなくまだ表示されていないデータでもうまく機能するようにするための、正規化またはすべての手法。 モデルを訓練したときに知っておくべき最も重要なことは、訓練されていないデータに対して実際にどれだけうまく機能するかということです。 来週はたくさん学ぶことになるので、これをバリデーションセットと呼びます。

What happens with the validation set is that we do our mini-batch SGD training loop with one set of data with one set of teddy bears, grizzlies, black bears. Then when we're done, we check the loss function and the accuracy to see how good is it on a bunch of images which were not included in the training. So if we do that, then if we have something which is too wiggly, it will tell us. "Oh, your loss function and your error is really bad because on the bears that it hasn't been trained with, the wiggly bits are in the wrong spot." Or else if it was underfitting, it would also tell us that your validation set is really bad. 

検証セットで起こることは、1セットのテディベア、グリズリー、クロクマを使って1セットのデータで私たちのミニバッチSGDトレーニングループを行うということです。 次に、作業が完了したら、損失関数と正確性をチェックして、トレーニングに含まれていない一連の画像でそれがどれほど優れているかを確認します。 ですから、もしそれをやれば、もしかしても曖昧すぎるものがあれば、それは私たちに教えてくれるでしょう。 「ああ、あなたの損失関数とあなたのエラーは本当に悪い。それはトレーニングされていないというクマの上では、曖昧な部分は間違った場所にあるからだ」 それ以外の場合は、それが当てはまらない場合は、それはまたあなたの検証セットが本当に悪いことを私たちに伝えるでしょう。

Even for people that don't go through this course and don't learn about the details of deep learning, if you've got managers or colleagues at work who are wanting to learn about AI, the only thing that you really need to be teaching them is about the idea of a validation set. Because that's the thing they can then use to figure out if somebody's telling them snake oil or not. They hold back some data and they get told "oh, here's a model that we're going to roll out" and then you say "okay, fine. I'm just going to check it on this held out data to see whether it generalizes." There's a lot of details to get right when you design your validation set. We will talk about them briefly next week, but a more full version would be in Rachel's piece on the fast.ai blog called [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/). And this is also one of the things we go into a lot of detail in the intro to machine learning course. So we're going to try and give you enough to get by for this course, but it is certainly something that's worth deeper study as well.

このコースを通らず、ディープラーニングの詳細を学ばない人でさえ、AIについて学びたいと思っているマネージャや同僚が仕事に携わっているのであれば、あなたが本当にする必要がある唯一のことそれらを教えることはバリデーションセットのアイデアについてです。それは、誰かが彼らに油を蛇にするように言っているかどうかを知るために彼らが使うことができるものだからです。彼らはいくつかのデータを差し控えて、「ああ、これは私たちが展開するモデルです」と言われ、それからあなたは「大丈夫、大丈夫です。一般化する」検証セットを設計する際には、細心の注意を払う必要があります。来週はそれらについて簡単にお話しますが、より完全なバージョンはfast.aiブログの[良い検証セットをどのように（そしてなぜ）作成するか]という（https://www.fast.ai/）というRachelの作品にあります。 2017/11/13 / validation-sets /）そしてこれはまた、機械学習コースのイントロで詳細に説明することの1つです。だから私たちはこのコースに入るためにあなたに十分にあなたを与えようとしているつもりです、しかしそれは確かに同様により深い研究の価値があるものです

Thanks everybody! I hope you have a great time building your web applications. See you next week.
みんなありがとう！ 私はあなたがあなたのWebアプリケーションを構築する素晴らしい時間を過ごすことを願っています。 また来週。
