





<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
  <link rel="dns-prefetch" href="https://github.githubassets.com">
  <link rel="dns-prefetch" href="https://avatars0.githubusercontent.com">
  <link rel="dns-prefetch" href="https://avatars1.githubusercontent.com">
  <link rel="dns-prefetch" href="https://avatars2.githubusercontent.com">
  <link rel="dns-prefetch" href="https://avatars3.githubusercontent.com">
  <link rel="dns-prefetch" href="https://github-cloud.s3.amazonaws.com">
  <link rel="dns-prefetch" href="https://user-images.githubusercontent.com/">



  <link crossorigin="anonymous" media="all" integrity="sha512-pWLt6abkYhNeAHaDrPVG0yXCtIGRuCkwSUqQpsyN6smAIpIt+Iuq2IZKmoH9l3Cy/9ZnjvVrFZnvFFjGiqE3EA==" rel="stylesheet" href="https://github.githubassets.com/assets/frameworks-a3b8a10d4a9e37a78f033ef4a4f525f5.css" />
  <link crossorigin="anonymous" media="all" integrity="sha512-aOF7h274OMbKmpIPpyHUahxjiTzZzRRXUhIhlK70L5B0MfzPhAkKqkTpc0rz49pHP3lEI5ic2UT1lPpuWipuZA==" rel="stylesheet" href="https://github.githubassets.com/assets/github-9bcb96f570b0873a1a19187b85566f30.css" />
  
  
  
  
  

  <meta name="viewport" content="width=device-width">
  
  <title>notes/Lesson1.md at e_j · yukari-n-erb/notes</title>
    <meta name="description" content="Contribute to yukari-n-erb/notes development by creating an account on GitHub.">
    <link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="GitHub">
  <link rel="fluid-icon" href="https://github.com/fluidicon.png" title="GitHub">
  <meta property="fb:app_id" content="1401488693436528">

    
    <meta property="og:image" content="https://avatars3.githubusercontent.com/u/45537257?s=400&amp;v=4" /><meta property="og:site_name" content="GitHub" /><meta property="og:type" content="object" /><meta property="og:title" content="yukari-n-erb/notes" /><meta property="og:url" content="https://github.com/yukari-n-erb/notes" /><meta property="og:description" content="Contribute to yukari-n-erb/notes development by creating an account on GitHub." />

  <link rel="assets" href="https://github.githubassets.com/">
  <link rel="web-socket" href="wss://live.github.com/_sockets/VjI6MzY4MTk0Nzg3OmY2YzY2OWJkOTc4OWM3MWJlMmRiMGVkMDYyOGRkMzIzOGM1ZjA0ZmNiODQwYTVhYmY1YTZmYmE5NGMzNjc4YzE=--c4921607227b01c55721866430fa13d8d3685437">
  <meta name="pjax-timeout" content="1000">
  <link rel="sudo-modal" href="/sessions/sudo_modal">
  <meta name="request-id" content="79CC:7FE8:A8FCE8:1036661:5C530592" data-pjax-transient>


  

  <meta name="selected-link" value="repo_source" data-pjax-transient>

      <meta name="google-site-verification" content="KT5gs8h0wvaagLKAVWq8bbeNwnZZK1r1XQysX3xurLU">
    <meta name="google-site-verification" content="ZzhVyEFwb7w3e0-uOTltm8Jsck2F5StVihD0exw2fsA">
    <meta name="google-site-verification" content="GXs5KoUUkNCoaAZn7wPN-t01Pywp9M3sEjnt_3_ZWPc">

  <meta name="octolytics-host" content="collector.githubapp.com" /><meta name="octolytics-app-id" content="github" /><meta name="octolytics-event-url" content="https://collector.githubapp.com/github-external/browser_event" /><meta name="octolytics-dimension-request_id" content="79CC:7FE8:A8FCE8:1036661:5C530592" /><meta name="octolytics-dimension-region_edge" content="sea" /><meta name="octolytics-dimension-region_render" content="iad" /><meta name="octolytics-actor-id" content="45537257" /><meta name="octolytics-actor-login" content="yukari-n-erb" /><meta name="octolytics-actor-hash" content="b3c8911d8fe235a158c072e69833c789982d52e072b953bdbe95592c5a3aa098" />
<meta name="analytics-location" content="/&lt;user-name&gt;/&lt;repo-name&gt;/blob/show" data-pjax-transient="true" />



    <meta name="google-analytics" content="UA-3769691-2">

  <meta class="js-ga-set" name="userId" content="068e65f5af6f9d5b7fc61aa8f0a8707a">

<meta class="js-ga-set" name="dimension1" content="Logged In">



  

      <meta name="hostname" content="github.com">
    <meta name="user-login" content="yukari-n-erb">

      <meta name="expected-hostname" content="github.com">
    <meta name="js-proxy-site-detection-payload" content="YTc0YjAyNjcwNWYyYzBlMTU2YzVjODMwOWNlNWE5ZjhiYTRlNDY5MmNlN2E0OTljNzhjZDlkYzY2NzRmNGZhMHx7InJlbW90ZV9hZGRyZXNzIjoiMTE1LjY5LjIzMi44MiIsInJlcXVlc3RfaWQiOiI3OUNDOjdGRTg6QThGQ0U4OjEwMzY2NjE6NUM1MzA1OTIiLCJ0aW1lc3RhbXAiOjE1NDg5NDQ4MTAsImhvc3QiOiJnaXRodWIuY29tIn0=">

    <meta name="enabled-features" content="UNIVERSE_BANNER,MARKETPLACE_PLAN_RESTRICTION_EDITOR,NOTIFY_ON_BLOCK,RELATED_ISSUES,MARKETPLACE_INSIGHTS_V2">

  <meta name="html-safe-nonce" content="8d1b436dd8b7b2e6dfc049a9a5bb56cb87300c17">

  <meta http-equiv="x-pjax-version" content="6723c7915aa0b9f204df4ba657f44d02">
  

      <link href="https://github.com/yukari-n-erb/notes/commits/e_j.atom" rel="alternate" title="Recent Commits to notes:e_j" type="application/atom+xml">

  <meta name="go-import" content="github.com/yukari-n-erb/notes git https://github.com/yukari-n-erb/notes.git">

  <meta name="octolytics-dimension-user_id" content="45537257" /><meta name="octolytics-dimension-user_login" content="yukari-n-erb" /><meta name="octolytics-dimension-repository_id" content="168161853" /><meta name="octolytics-dimension-repository_nwo" content="yukari-n-erb/notes" /><meta name="octolytics-dimension-repository_public" content="true" /><meta name="octolytics-dimension-repository_is_fork" content="true" /><meta name="octolytics-dimension-repository_parent_id" content="154415070" /><meta name="octolytics-dimension-repository_parent_nwo" content="hiromis/notes" /><meta name="octolytics-dimension-repository_network_root_id" content="154415070" /><meta name="octolytics-dimension-repository_network_root_nwo" content="hiromis/notes" /><meta name="octolytics-dimension-repository_explore_github_marketplace_ci_cta_shown" content="true" />


    <link rel="canonical" href="https://github.com/yukari-n-erb/notes/blob/e_j/Lesson1.md" data-pjax-transient>


  <meta name="browser-stats-url" content="https://api.github.com/_private/browser/stats">

  <meta name="browser-errors-url" content="https://api.github.com/_private/browser/errors">

  <link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000">
  <link rel="icon" type="image/x-icon" class="js-site-favicon" href="https://github.githubassets.com/favicon.ico">

<meta name="theme-color" content="#1e2327">


  <meta name="u2f-support" content="true">

  <link rel="manifest" href="/manifest.json" crossOrigin="use-credentials">

  </head>

  <body class="logged-in env-production page-blob">
    

  <div class="position-relative js-header-wrapper ">
    <a href="#start-of-content" tabindex="1" class="p-3 bg-blue text-white show-on-focus js-skip-to-content">Skip to content</a>
    <div id="js-pjax-loader-bar" class="pjax-loader-bar"><div class="progress"></div></div>

    
    
    


        
<header class="Header  f5" role="banner">
  <div class="d-flex flex-justify-between px-3 ">
    <div class="d-flex flex-justify-between ">
      <div class="">
        <a class="header-logo-invertocat" href="https://github.com/" data-hotkey="g d" aria-label="Homepage" data-ga-click="Header, go to dashboard, icon:logo">
  <svg height="32" class="octicon octicon-mark-github" viewBox="0 0 16 16" version="1.1" width="32" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/></svg>
</a>

      </div>

    </div>

    <div class="HeaderMenu d-flex flex-justify-between flex-auto">
      <nav class="d-flex" aria-label="Global">
            <div class="">
              <div class="header-search scoped-search site-scoped-search js-site-search position-relative js-jump-to"
  role="combobox"
  aria-owns="jump-to-results"
  aria-label="Search or jump to"
  aria-haspopup="listbox"
  aria-expanded="false"
>
  <div class="position-relative">
    <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="js-site-search-form" data-scope-type="Repository" data-scope-id="168161853" data-scoped-search-url="/yukari-n-erb/notes/search" data-unscoped-search-url="/search" action="/yukari-n-erb/notes/search" accept-charset="UTF-8" method="get"><input name="utf8" type="hidden" value="&#x2713;" />
      <label class="form-control header-search-wrapper header-search-wrapper-jump-to position-relative d-flex flex-justify-between flex-items-center js-chromeless-input-container">
        <input type="text"
          class="form-control header-search-input jump-to-field js-jump-to-field js-site-search-focus js-site-search-field is-clearable"
          data-hotkey="s,/"
          name="q"
          value=""
          placeholder="Search or jump to…"
          data-unscoped-placeholder="Search or jump to…"
          data-scoped-placeholder="Search or jump to…"
          autocapitalize="off"
          aria-autocomplete="list"
          aria-controls="jump-to-results"
          aria-label="Search or jump to…"
          data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations#csrf-token=wXn9lb/IrKzh3V2DcYAIv/H82StFgEzFXpOQ+9g1VxjCJml7bQ3x4Kaj+voo+usiGn36PeZO3amNl610nAdGkg=="
          spellcheck="false"
          autocomplete="off"
          >
          <input type="hidden" class="js-site-search-type-field" name="type" >
            <img src="https://github.githubassets.com/images/search-key-slash.svg" alt="" class="mr-2 header-search-key-slash">

            <div class="Box position-absolute overflow-hidden d-none jump-to-suggestions js-jump-to-suggestions-container">
              
<ul class="d-none js-jump-to-suggestions-template-container">
  

<li class="d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-suggestion" role="option">
  <a tabindex="-1" class="no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2" href="">
    <div class="jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none">
      <svg height="16" width="16" class="octicon octicon-repo flex-shrink-0 js-jump-to-octicon-repo d-none" title="Repository" aria-label="Repository" viewBox="0 0 12 16" version="1.1" role="img"><path fill-rule="evenodd" d="M4 9H3V8h1v1zm0-3H3v1h1V6zm0-2H3v1h1V4zm0-2H3v1h1V2zm8-1v12c0 .55-.45 1-1 1H6v2l-1.5-1.5L3 16v-2H1c-.55 0-1-.45-1-1V1c0-.55.45-1 1-1h10c.55 0 1 .45 1 1zm-1 10H1v2h2v-1h3v1h5v-2zm0-10H2v9h9V1z"/></svg>
      <svg height="16" width="16" class="octicon octicon-project flex-shrink-0 js-jump-to-octicon-project d-none" title="Project" aria-label="Project" viewBox="0 0 15 16" version="1.1" role="img"><path fill-rule="evenodd" d="M10 12h3V2h-3v10zm-4-2h3V2H6v8zm-4 4h3V2H2v12zm-1 1h13V1H1v14zM14 0H1a1 1 0 0 0-1 1v14a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V1a1 1 0 0 0-1-1z"/></svg>
      <svg height="16" width="16" class="octicon octicon-search flex-shrink-0 js-jump-to-octicon-search d-none" title="Search" aria-label="Search" viewBox="0 0 16 16" version="1.1" role="img"><path fill-rule="evenodd" d="M15.7 13.3l-3.81-3.83A5.93 5.93 0 0 0 13 6c0-3.31-2.69-6-6-6S1 2.69 1 6s2.69 6 6 6c1.3 0 2.48-.41 3.47-1.11l3.83 3.81c.19.2.45.3.7.3.25 0 .52-.09.7-.3a.996.996 0 0 0 0-1.41v.01zM7 10.7c-2.59 0-4.7-2.11-4.7-4.7 0-2.59 2.11-4.7 4.7-4.7 2.59 0 4.7 2.11 4.7 4.7 0 2.59-2.11 4.7-4.7 4.7z"/></svg>
    </div>

    <img class="avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none" alt="" aria-label="Team" src="" width="28" height="28">

    <div class="jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target">
    </div>

    <div class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none js-jump-to-badge-search">
      <span class="js-jump-to-badge-search-text-default d-none" aria-label="in this repository">
        In this repository
      </span>
      <span class="js-jump-to-badge-search-text-global d-none" aria-label="in all of GitHub">
        All GitHub
      </span>
      <span aria-hidden="true" class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>

    <div aria-hidden="true" class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump">
      Jump to
      <span class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>
  </a>
</li>

</ul>

<ul class="d-none js-jump-to-no-results-template-container">
  <li class="d-flex flex-justify-center flex-items-center f5 d-none js-jump-to-suggestion p-2">
    <span class="text-gray">No suggested jump to results</span>
  </li>
</ul>

<ul id="jump-to-results" role="listbox" class="p-0 m-0 js-navigation-container jump-to-suggestions-results-container js-jump-to-suggestions-results-container">
  

<li class="d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-scoped-search d-none" role="option">
  <a tabindex="-1" class="no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2" href="">
    <div class="jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none">
      <svg height="16" width="16" class="octicon octicon-repo flex-shrink-0 js-jump-to-octicon-repo d-none" title="Repository" aria-label="Repository" viewBox="0 0 12 16" version="1.1" role="img"><path fill-rule="evenodd" d="M4 9H3V8h1v1zm0-3H3v1h1V6zm0-2H3v1h1V4zm0-2H3v1h1V2zm8-1v12c0 .55-.45 1-1 1H6v2l-1.5-1.5L3 16v-2H1c-.55 0-1-.45-1-1V1c0-.55.45-1 1-1h10c.55 0 1 .45 1 1zm-1 10H1v2h2v-1h3v1h5v-2zm0-10H2v9h9V1z"/></svg>
      <svg height="16" width="16" class="octicon octicon-project flex-shrink-0 js-jump-to-octicon-project d-none" title="Project" aria-label="Project" viewBox="0 0 15 16" version="1.1" role="img"><path fill-rule="evenodd" d="M10 12h3V2h-3v10zm-4-2h3V2H6v8zm-4 4h3V2H2v12zm-1 1h13V1H1v14zM14 0H1a1 1 0 0 0-1 1v14a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V1a1 1 0 0 0-1-1z"/></svg>
      <svg height="16" width="16" class="octicon octicon-search flex-shrink-0 js-jump-to-octicon-search d-none" title="Search" aria-label="Search" viewBox="0 0 16 16" version="1.1" role="img"><path fill-rule="evenodd" d="M15.7 13.3l-3.81-3.83A5.93 5.93 0 0 0 13 6c0-3.31-2.69-6-6-6S1 2.69 1 6s2.69 6 6 6c1.3 0 2.48-.41 3.47-1.11l3.83 3.81c.19.2.45.3.7.3.25 0 .52-.09.7-.3a.996.996 0 0 0 0-1.41v.01zM7 10.7c-2.59 0-4.7-2.11-4.7-4.7 0-2.59 2.11-4.7 4.7-4.7 2.59 0 4.7 2.11 4.7 4.7 0 2.59-2.11 4.7-4.7 4.7z"/></svg>
    </div>

    <img class="avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none" alt="" aria-label="Team" src="" width="28" height="28">

    <div class="jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target">
    </div>

    <div class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none js-jump-to-badge-search">
      <span class="js-jump-to-badge-search-text-default d-none" aria-label="in this repository">
        In this repository
      </span>
      <span class="js-jump-to-badge-search-text-global d-none" aria-label="in all of GitHub">
        All GitHub
      </span>
      <span aria-hidden="true" class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>

    <div aria-hidden="true" class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump">
      Jump to
      <span class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>
  </a>
</li>

  

<li class="d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-global-search d-none" role="option">
  <a tabindex="-1" class="no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2" href="">
    <div class="jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none">
      <svg height="16" width="16" class="octicon octicon-repo flex-shrink-0 js-jump-to-octicon-repo d-none" title="Repository" aria-label="Repository" viewBox="0 0 12 16" version="1.1" role="img"><path fill-rule="evenodd" d="M4 9H3V8h1v1zm0-3H3v1h1V6zm0-2H3v1h1V4zm0-2H3v1h1V2zm8-1v12c0 .55-.45 1-1 1H6v2l-1.5-1.5L3 16v-2H1c-.55 0-1-.45-1-1V1c0-.55.45-1 1-1h10c.55 0 1 .45 1 1zm-1 10H1v2h2v-1h3v1h5v-2zm0-10H2v9h9V1z"/></svg>
      <svg height="16" width="16" class="octicon octicon-project flex-shrink-0 js-jump-to-octicon-project d-none" title="Project" aria-label="Project" viewBox="0 0 15 16" version="1.1" role="img"><path fill-rule="evenodd" d="M10 12h3V2h-3v10zm-4-2h3V2H6v8zm-4 4h3V2H2v12zm-1 1h13V1H1v14zM14 0H1a1 1 0 0 0-1 1v14a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V1a1 1 0 0 0-1-1z"/></svg>
      <svg height="16" width="16" class="octicon octicon-search flex-shrink-0 js-jump-to-octicon-search d-none" title="Search" aria-label="Search" viewBox="0 0 16 16" version="1.1" role="img"><path fill-rule="evenodd" d="M15.7 13.3l-3.81-3.83A5.93 5.93 0 0 0 13 6c0-3.31-2.69-6-6-6S1 2.69 1 6s2.69 6 6 6c1.3 0 2.48-.41 3.47-1.11l3.83 3.81c.19.2.45.3.7.3.25 0 .52-.09.7-.3a.996.996 0 0 0 0-1.41v.01zM7 10.7c-2.59 0-4.7-2.11-4.7-4.7 0-2.59 2.11-4.7 4.7-4.7 2.59 0 4.7 2.11 4.7 4.7 0 2.59-2.11 4.7-4.7 4.7z"/></svg>
    </div>

    <img class="avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none" alt="" aria-label="Team" src="" width="28" height="28">

    <div class="jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target">
    </div>

    <div class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none js-jump-to-badge-search">
      <span class="js-jump-to-badge-search-text-default d-none" aria-label="in this repository">
        In this repository
      </span>
      <span class="js-jump-to-badge-search-text-global d-none" aria-label="in all of GitHub">
        All GitHub
      </span>
      <span aria-hidden="true" class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>

    <div aria-hidden="true" class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump">
      Jump to
      <span class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>
  </a>
</li>


    <li class="d-flex flex-justify-center flex-items-center p-0 f5 js-jump-to-suggestion">
      <img src="https://github.githubassets.com/images/spinners/octocat-spinner-128.gif" alt="Octocat Spinner Icon" class="m-2" width="28">
    </li>
</ul>

            </div>
      </label>
</form>  </div>
</div>

            </div>

          <ul class="d-flex pl-2 flex-items-center text-bold list-style-none">
            <li>
              <a class="js-selected-navigation-item HeaderNavlink px-2" data-hotkey="g p" data-ga-click="Header, click, Nav menu - item:pulls context:user" aria-label="Pull requests you created" data-selected-links="/pulls /pulls/assigned /pulls/mentioned /pulls" href="/pulls">
                Pull requests
</a>            </li>
            <li>
              <a class="js-selected-navigation-item HeaderNavlink px-2" data-hotkey="g i" data-ga-click="Header, click, Nav menu - item:issues context:user" aria-label="Issues you created" data-selected-links="/issues /issues/assigned /issues/mentioned /issues" href="/issues">
                Issues
</a>            </li>
              <li class="position-relative">
                <a class="js-selected-navigation-item HeaderNavlink px-2" data-ga-click="Header, click, Nav menu - item:marketplace context:user" data-octo-click="marketplace_click" data-octo-dimensions="location:nav_bar" data-selected-links=" /marketplace" href="/marketplace">
                   Marketplace
</a>                  
              </li>
            <li>
              <a class="js-selected-navigation-item HeaderNavlink px-2" data-ga-click="Header, click, Nav menu - item:explore" data-selected-links="/explore /trending /trending/developers /integrations /integrations/feature/code /integrations/feature/collaborate /integrations/feature/ship showcases showcases_search showcases_landing /explore" href="/explore">
                Explore
</a>            </li>
          </ul>
      </nav>

      <div class="d-flex">
        
<ul class="user-nav d-flex flex-items-center list-style-none" id="user-links">
  <li class="dropdown">
    <span class="d-inline-block  px-2">
      
    <a aria-label="You have no unread notifications" class="notification-indicator tooltipped tooltipped-s  js-socket-channel js-notification-indicator" data-hotkey="g n" data-ga-click="Header, go to notifications, icon:read" data-channel="notification-changed:45537257" href="/notifications">
        <span class="mail-status "></span>
        <svg class="octicon octicon-bell" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14 12v1H0v-1l.73-.58c.77-.77.81-2.55 1.19-4.42C2.69 3.23 6 2 6 2c0-.55.45-1 1-1s1 .45 1 1c0 0 3.39 1.23 4.16 5 .38 1.88.42 3.66 1.19 4.42l.66.58H14zm-7 4c1.11 0 2-.89 2-2H5c0 1.11.89 2 2 2z"/></svg>
</a>
    </span>
  </li>

  <li class="dropdown">
    <details class="details-overlay details-reset d-flex px-2 flex-items-center">
      <summary class="HeaderNavlink"
         aria-label="Create new…"
         data-ga-click="Header, create new, icon:add">
        <svg class="octicon octicon-plus float-left mr-1 mt-1" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 9H7v5H5V9H0V7h5V2h2v5h5v2z"/></svg>
        <span class="dropdown-caret mt-1"></span>
      </summary>
      <details-menu class="dropdown-menu dropdown-menu-sw">
        
<a role="menuitem" class="dropdown-item" href="/new" data-ga-click="Header, create new repository">
  New repository
</a>

  <a role="menuitem" class="dropdown-item" href="/new/import" data-ga-click="Header, import a repository">
    Import repository
  </a>

<a role="menuitem" class="dropdown-item" href="https://gist.github.com/" data-ga-click="Header, create new gist">
  New gist
</a>

  <a role="menuitem" class="dropdown-item" href="/organizations/new" data-ga-click="Header, create new organization">
    New organization
  </a>




      </details-menu>
    </details>
  </li>

  <li class="dropdown">

    <details class="details-overlay details-reset d-flex pl-2 flex-items-center">
      <summary class="HeaderNavlink name mt-1"
        aria-label="View profile and more"
        data-ga-click="Header, show menu, icon:avatar">
        <img alt="@yukari-n-erb" class="avatar float-left mr-1" src="https://avatars0.githubusercontent.com/u/45537257?s=40&amp;v=4" height="20" width="20">
        <span class="dropdown-caret"></span>
      </summary>
      <details-menu class="dropdown-menu dropdown-menu-sw">
        <div class="header-nav-current-user css-truncate"><a role="menuitem" class="no-underline user-profile-link px-3 pt-2 pb-2 mb-n2 mt-n1 d-block" href="/yukari-n-erb" data-ga-click="Header, go to profile, text:Signed in as">Signed in as <strong class="css-truncate-target">yukari-n-erb</strong></a></div>
        <div role="none" class="dropdown-divider"></div>

        <div class="px-3 f6 user-status-container js-user-status-context pb-1" data-url="/users/status?compact=1&amp;link_mentions=0&amp;truncate=1">
          
<div class="js-user-status-container user-status-compact" data-team-hovercards-enabled>
  <details class="js-user-status-details details-reset details-overlay details-overlay-dark">
    <summary class="btn-link no-underline js-toggle-user-status-edit toggle-user-status-edit width-full" aria-haspopup="dialog" role="menuitem">
      <div class="f6 d-inline-block v-align-middle  user-status-emoji-only-header pl-0 circle lh-condensed user-status-header " style="max-width: 29px">
        <div class="user-status-emoji-container flex-shrink-0 mr-1">
          <svg class="octicon octicon-smiley" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8s3.58 8 8 8 8-3.58 8-8-3.58-8-8-8zm4.81 12.81a6.72 6.72 0 0 1-2.17 1.45c-.83.36-1.72.53-2.64.53-.92 0-1.81-.17-2.64-.53-.81-.34-1.55-.83-2.17-1.45a6.773 6.773 0 0 1-1.45-2.17A6.59 6.59 0 0 1 1.21 8c0-.92.17-1.81.53-2.64.34-.81.83-1.55 1.45-2.17.62-.62 1.36-1.11 2.17-1.45A6.59 6.59 0 0 1 8 1.21c.92 0 1.81.17 2.64.53.81.34 1.55.83 2.17 1.45.62.62 1.11 1.36 1.45 2.17.36.83.53 1.72.53 2.64 0 .92-.17 1.81-.53 2.64-.34.81-.83 1.55-1.45 2.17zM4 6.8v-.59c0-.66.53-1.19 1.2-1.19h.59c.66 0 1.19.53 1.19 1.19v.59c0 .67-.53 1.2-1.19 1.2H5.2C4.53 8 4 7.47 4 6.8zm5 0v-.59c0-.66.53-1.19 1.2-1.19h.59c.66 0 1.19.53 1.19 1.19v.59c0 .67-.53 1.2-1.19 1.2h-.59C9.53 8 9 7.47 9 6.8zm4 3.2c-.72 1.88-2.91 3-5 3s-4.28-1.13-5-3c-.14-.39.23-1 .66-1h8.59c.41 0 .89.61.75 1z"/></svg>
        </div>
      </div>
      <div class="d-inline-block v-align-middle user-status-message-wrapper f6 lh-condensed ws-normal pt-1">
          <span class="link-gray">Set your status</span>
      </div>
    </summary>
    <details-dialog class="anim-fade-in fast Box Box--overlay" role="dialog" tabindex="-1">
      <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="position-relative flex-auto js-user-status-form" action="/users/status?compact=1&amp;link_mentions=0&amp;truncate=1" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="put" /><input type="hidden" name="authenticity_token" value="l2WjDjP5IXa1dWgVdgy/b3n3/g4kP96Sy4aiX3y0QoqjH5poYR7/J14r2/BzdcOaZIVDyTp+PS6tngZLIla/lQ==" />
        <div class="Box-header">
          <button class="Box-btn-octicon js-toggle-user-status-edit btn-octicon float-right" type="reset" aria-label="Close dialog" data-close-dialog>
            <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"/></svg>
          </button>
          <h3 class="Box-title text-gray-dark">Edit status</h3>
        </div>
        <input type="hidden" name="emoji" class="js-user-status-emoji-field" value="">
        <input type="hidden" name="organization_id" class="js-user-status-org-id-field" value="">
        <div class="px-3 py-2 text-gray-dark">
          <div class="js-characters-remaining-container js-suggester-container position-relative mt-2">
            <div class="input-group form-group my-0 js-user-status-form-group">
              <span class="input-group-button">
                <button type="button" aria-label="Choose an emoji" class="btn-outline btn js-toggle-user-status-emoji-picker">
                  <span class="js-user-status-original-emoji" hidden></span>
                  <span class="js-user-status-custom-emoji"></span>
                  <span class="js-user-status-no-emoji-icon" >
                    <svg class="octicon octicon-smiley" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8s3.58 8 8 8 8-3.58 8-8-3.58-8-8-8zm4.81 12.81a6.72 6.72 0 0 1-2.17 1.45c-.83.36-1.72.53-2.64.53-.92 0-1.81-.17-2.64-.53-.81-.34-1.55-.83-2.17-1.45a6.773 6.773 0 0 1-1.45-2.17A6.59 6.59 0 0 1 1.21 8c0-.92.17-1.81.53-2.64.34-.81.83-1.55 1.45-2.17.62-.62 1.36-1.11 2.17-1.45A6.59 6.59 0 0 1 8 1.21c.92 0 1.81.17 2.64.53.81.34 1.55.83 2.17 1.45.62.62 1.11 1.36 1.45 2.17.36.83.53 1.72.53 2.64 0 .92-.17 1.81-.53 2.64-.34.81-.83 1.55-1.45 2.17zM4 6.8v-.59c0-.66.53-1.19 1.2-1.19h.59c.66 0 1.19.53 1.19 1.19v.59c0 .67-.53 1.2-1.19 1.2H5.2C4.53 8 4 7.47 4 6.8zm5 0v-.59c0-.66.53-1.19 1.2-1.19h.59c.66 0 1.19.53 1.19 1.19v.59c0 .67-.53 1.2-1.19 1.2h-.59C9.53 8 9 7.47 9 6.8zm4 3.2c-.72 1.88-2.91 3-5 3s-4.28-1.13-5-3c-.14-.39.23-1 .66-1h8.59c.41 0 .89.61.75 1z"/></svg>
                  </span>
                </button>
              </span>
              <input type="text" autocomplete="off" autofocus data-maxlength="80" class="js-suggester-field form-control js-user-status-message-field js-characters-remaining-field" style="border-bottom-left-radius: 0; border-top-left-radius: 0" placeholder="What's happening?" name="message" required value="" aria-label="What is your current status?">
              <div class="error">Could not update your status, please try again.</div>
            </div>
            <div class="suggester-container">
              <div class="suggester js-suggester js-navigation-container" data-url="/autocomplete/user-suggestions" data-no-org-url="/autocomplete/user-suggestions" data-org-url="/suggestions" hidden>
              </div>
            </div>
            <div style="margin-left: 53px" class="my-1 text-small label-characters-remaining js-characters-remaining" data-suffix="remaining" hidden>
              80 remaining
            </div>
          </div>
          <include-fragment class="js-user-status-emoji-picker" data-url="/users/status/emoji"></include-fragment>
          <div class="overflow-auto" style="max-height: 33vh">
            <div class="user-status-suggestions js-user-status-suggestions">
              <h4 class="f6 text-normal my-3">Suggestions:</h4>
              <div class="mx-3 mt-2 clearfix">
                  <div class="float-left col-6">
                      <button type="button" value=":palm_tree:" class="d-flex flex-items-baseline flex-items-stretch lh-condensed f6 btn-link link-gray no-underline js-predefined-user-status mb-1">
                        <div class="emoji-status-width mr-2 v-align-middle js-predefined-user-status-emoji">
                          <g-emoji alias="palm_tree" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f334.png">🌴</g-emoji>
                        </div>
                        <div class="d-flex flex-items-center no-underline js-predefined-user-status-message" style="border-left: 1px solid transparent">
                          On vacation
                        </div>
                      </button>
                      <button type="button" value=":face_with_thermometer:" class="d-flex flex-items-baseline flex-items-stretch lh-condensed f6 btn-link link-gray no-underline js-predefined-user-status mb-1">
                        <div class="emoji-status-width mr-2 v-align-middle js-predefined-user-status-emoji">
                          <g-emoji alias="face_with_thermometer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f912.png">🤒</g-emoji>
                        </div>
                        <div class="d-flex flex-items-center no-underline js-predefined-user-status-message" style="border-left: 1px solid transparent">
                          Out sick
                        </div>
                      </button>
                  </div>
                  <div class="float-left col-6">
                      <button type="button" value=":house:" class="d-flex flex-items-baseline flex-items-stretch lh-condensed f6 btn-link link-gray no-underline js-predefined-user-status mb-1">
                        <div class="emoji-status-width mr-2 v-align-middle js-predefined-user-status-emoji">
                          <g-emoji alias="house" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e0.png">🏠</g-emoji>
                        </div>
                        <div class="d-flex flex-items-center no-underline js-predefined-user-status-message" style="border-left: 1px solid transparent">
                          Working from home
                        </div>
                      </button>
                      <button type="button" value=":dart:" class="d-flex flex-items-baseline flex-items-stretch lh-condensed f6 btn-link link-gray no-underline js-predefined-user-status mb-1">
                        <div class="emoji-status-width mr-2 v-align-middle js-predefined-user-status-emoji">
                          <g-emoji alias="dart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png">🎯</g-emoji>
                        </div>
                        <div class="d-flex flex-items-center no-underline js-predefined-user-status-message" style="border-left: 1px solid transparent">
                          Focusing
                        </div>
                      </button>
                  </div>
              </div>
            </div>
            <div class="user-status-limited-availability-container">
              <div class="form-checkbox my-0">
                <input type="checkbox" name="limited_availability" value="1" class="js-user-status-limited-availability-checkbox" data-default-message="I may be slow to respond." aria-describedby="limited-availability-help-text-truncate-true" id="limited-availability-truncate-true">
                <label class="d-block f5 text-gray-dark mb-1" for="limited-availability-truncate-true">
                  Busy
                </label>
                <p class="note" id="limited-availability-help-text-truncate-true">
                  When others mention you, assign you, or request your review,
                  GitHub will let them know that you have limited availability.
                </p>
              </div>
            </div>
          </div>
          <include-fragment class="js-user-status-org-picker" data-url="/users/status/organizations"></include-fragment>
        </div>
        <div class="d-flex flex-items-center flex-justify-between p-3 border-top">
          <button type="submit" disabled class="width-full btn btn-primary mr-2 js-user-status-submit">
            Set status
          </button>
          <button type="button" disabled class="width-full js-clear-user-status-button btn ml-2 ">
            Clear status
          </button>
        </div>
</form>    </details-dialog>
  </details>
</div>

        </div>
        <div role="none" class="dropdown-divider"></div>

        <a role="menuitem" class="dropdown-item" href="/yukari-n-erb" data-ga-click="Header, go to profile, text:your profile">Your profile</a>
        <a role="menuitem" class="dropdown-item" href="/yukari-n-erb?tab=repositories" data-ga-click="Header, go to repositories, text:your repositories">Your repositories</a>


        <a role="menuitem" class="dropdown-item" href="/yukari-n-erb?tab=stars" data-ga-click="Header, go to starred repos, text:your stars">Your stars</a>
          <a role="menuitem" class="dropdown-item" href="https://gist.github.com/" data-ga-click="Header, your gists, text:your gists">Your gists</a>

        <div role="none" class="dropdown-divider"></div>
        <a role="menuitem" class="dropdown-item" href="https://help.github.com" data-ga-click="Header, go to help, text:help">Help</a>
        <a role="menuitem" class="dropdown-item" href="/settings/profile" data-ga-click="Header, go to settings, icon:settings">Settings</a>
        <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="logout-form" action="/logout" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="wR4MLqHHtqjkmiufx2lG5vArqe4cvTgk3dCixe7DPIDySeqzG4fK2qqZfmTZ4wf6VTcNRYpsDoM3Qw7Shbo52Q==" />
          
          <button type="submit" class="dropdown-item dropdown-signout" data-ga-click="Header, sign out, icon:logout" role="menuitem">
            Sign out
          </button>
</form>      </details-menu>
    </details>
  </li>
</ul>



        <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="sr-only right-0" action="/logout" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="r0Rsl2L8SEzuZX5pd3DPoXuijOBJ4bX2jEI74UILy7ucE4oK2Lw0PqBmK5Jp+o693r4oS98wg1Fm0Zf2KXLO4g==" />
          <button type="submit" class="dropdown-item dropdown-signout" data-ga-click="Header, sign out, icon:logout">
            Sign out
          </button>
</form>      </div>
    </div>
  </div>
</header>

      

  </div>

  <div id="start-of-content" class="show-on-focus"></div>

    <div id="js-flash-container">

</div>



  <div role="main" class="application-main " data-commit-hovercards-enabled>
        <div itemscope itemtype="http://schema.org/SoftwareSourceCode" class="">
    <div id="js-repo-pjax-container" data-pjax-container >
      


  





  <div class="pagehead repohead instapaper_ignore readability-menu experiment-repo-nav  ">
    <div class="repohead-details-container clearfix container">

      <ul class="pagehead-actions">
  <li>
        <!-- '"` --><!-- </textarea></xmp> --></option></form><form data-remote="true" class="js-social-form js-social-container" action="/notifications/subscribe" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="1kb8zSW83u1Str5MvNhUWkPxuVKZhQTEEuYEOb2d+hxvnI6NtU2lsB6UXG+oehItklengyl62nQIx/NSv/UtSw==" />      <input type="hidden" name="repository_id" id="repository_id" value="168161853" class="form-control" />

      <details class="details-reset details-overlay select-menu float-left">
        <summary class="btn btn-sm btn-with-count select-menu-button" data-ga-click="Repository, click Watch settings, action:blob#show">
          <span data-menu-button>
              <svg class="octicon octicon-eye v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.06 2C3 2 0 8 0 8s3 6 8.06 6C13 14 16 8 16 8s-3-6-7.94-6zM8 12c-2.2 0-4-1.78-4-4 0-2.2 1.8-4 4-4 2.22 0 4 1.8 4 4 0 2.22-1.78 4-4 4zm2-4c0 1.11-.89 2-2 2-1.11 0-2-.89-2-2 0-1.11.89-2 2-2 1.11 0 2 .89 2 2z"/></svg>
              Watch
          </span>
        </summary>
        <details-menu class="select-menu-modal position-absolute mt-5" style="z-index: 99;">
          <div class="select-menu-header">
            <span class="select-menu-title">Notifications</span>
          </div>
          <div class="select-menu-list">
            <button type="submit" name="do" value="included" class="select-menu-item width-full" aria-checked="true" role="menuitemradio">
              <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"/></svg>
              <div class="select-menu-item-text">
                <span class="select-menu-item-heading">Not watching</span>
                <span class="description">Be notified only when participating or @mentioned.</span>
                <span class="hidden-select-button-text" data-menu-button-contents>
                  <svg class="octicon octicon-eye v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.06 2C3 2 0 8 0 8s3 6 8.06 6C13 14 16 8 16 8s-3-6-7.94-6zM8 12c-2.2 0-4-1.78-4-4 0-2.2 1.8-4 4-4 2.22 0 4 1.8 4 4 0 2.22-1.78 4-4 4zm2-4c0 1.11-.89 2-2 2-1.11 0-2-.89-2-2 0-1.11.89-2 2-2 1.11 0 2 .89 2 2z"/></svg>
                  Watch
                </span>
              </div>
            </button>

            <button type="submit" name="do" value="release_only" class="select-menu-item width-full" aria-checked="false" role="menuitemradio">
              <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"/></svg>
              <div class="select-menu-item-text">
                <span class="select-menu-item-heading">Releases only</span>
                <span class="description">Be notified of new releases, and when participating or @mentioned.</span>
                <span class="hidden-select-button-text" data-menu-button-contents>
                  <svg class="octicon octicon-eye v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.06 2C3 2 0 8 0 8s3 6 8.06 6C13 14 16 8 16 8s-3-6-7.94-6zM8 12c-2.2 0-4-1.78-4-4 0-2.2 1.8-4 4-4 2.22 0 4 1.8 4 4 0 2.22-1.78 4-4 4zm2-4c0 1.11-.89 2-2 2-1.11 0-2-.89-2-2 0-1.11.89-2 2-2 1.11 0 2 .89 2 2z"/></svg>
                  Unwatch releases
                </span>
              </div>
            </button>

            <button type="submit" name="do" value="subscribed" class="select-menu-item width-full" aria-checked="false" role="menuitemradio">
              <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"/></svg>
              <div class="select-menu-item-text">
                <span class="select-menu-item-heading">Watching</span>
                <span class="description">Be notified of all conversations.</span>
                <span class="hidden-select-button-text" data-menu-button-contents>
                  <svg class="octicon octicon-eye v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.06 2C3 2 0 8 0 8s3 6 8.06 6C13 14 16 8 16 8s-3-6-7.94-6zM8 12c-2.2 0-4-1.78-4-4 0-2.2 1.8-4 4-4 2.22 0 4 1.8 4 4 0 2.22-1.78 4-4 4zm2-4c0 1.11-.89 2-2 2-1.11 0-2-.89-2-2 0-1.11.89-2 2-2 1.11 0 2 .89 2 2z"/></svg>
                  Unwatch
                </span>
              </div>
            </button>

            <button type="submit" name="do" value="ignore" class="select-menu-item width-full" aria-checked="false" role="menuitemradio">
              <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"/></svg>
              <div class="select-menu-item-text">
                <span class="select-menu-item-heading">Ignoring</span>
                <span class="description">Never be notified.</span>
                <span class="hidden-select-button-text" data-menu-button-contents>
                  <svg class="octicon octicon-mute v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 2.81v10.38c0 .67-.81 1-1.28.53L3 10H1c-.55 0-1-.45-1-1V7c0-.55.45-1 1-1h2l3.72-3.72C7.19 1.81 8 2.14 8 2.81zm7.53 3.22l-1.06-1.06-1.97 1.97-1.97-1.97-1.06 1.06L11.44 8 9.47 9.97l1.06 1.06 1.97-1.97 1.97 1.97 1.06-1.06L13.56 8l1.97-1.97z"/></svg>
                  Stop ignoring
                </span>
              </div>
            </button>
          </div>
        </details-menu>
      </details>
      <a class="social-count js-social-count"
        href="/yukari-n-erb/notes/watchers"
        aria-label="0 users are watching this repository">
        0
      </a>
</form>
  </li>

  <li>
      <div class="js-toggler-container js-social-container starring-container ">
    <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="starred js-social-form" action="/yukari-n-erb/notes/unstar" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="uJ83fsdAEL0PQlvOjqptlrpETUak44Z+t/H0zR7aXHB178W+dQX3PTtjiNoxYZ3o+0603+kxEZSaEKfNTupFYg==" />
      <input type="hidden" name="context" value="repository"></input>
      <button
        type="submit"
        class="btn btn-sm btn-with-count js-toggler-target"
        aria-label="Unstar this repository" title="Unstar yukari-n-erb/notes"
        data-ga-click="Repository, click unstar button, action:blob#show; text:Unstar">
        <svg class="octicon octicon-star v-align-text-bottom" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14 6l-4.9-.64L7 1 4.9 5.36 0 6l3.6 3.26L2.67 14 7 11.67 11.33 14l-.93-4.74L14 6z"/></svg>
        Unstar
      </button>
        <a class="social-count js-social-count" href="/yukari-n-erb/notes/stargazers"
           aria-label="0 users starred this repository">
          0
        </a>
</form>
    <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="unstarred js-social-form" action="/yukari-n-erb/notes/star" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="xRevZdmMxaUCcUPUIByQHnVDS/imrEI9/Qq7zqrFcGW7xDK0EOA93nbcVzgxVeDmCVVKfJIKNCJ7Ys/3e3Vv2w==" />
      <input type="hidden" name="context" value="repository"></input>
      <button
        type="submit"
        class="btn btn-sm btn-with-count js-toggler-target"
        aria-label="Star this repository" title="Star yukari-n-erb/notes"
        data-ga-click="Repository, click star button, action:blob#show; text:Star">
        <svg class="octicon octicon-star v-align-text-bottom" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14 6l-4.9-.64L7 1 4.9 5.36 0 6l3.6 3.26L2.67 14 7 11.67 11.33 14l-.93-4.74L14 6z"/></svg>
        Star
      </button>
        <a class="social-count js-social-count" href="/yukari-n-erb/notes/stargazers"
           aria-label="0 users starred this repository">
          0
        </a>
</form>  </div>

  </li>

  <li>
          <details class="details-reset details-overlay details-overlay-dark d-inline-block float-left"
            data-deferred-details-content-url="/yukari-n-erb/notes/fork?fragment=1">
            <summary class="btn btn-sm btn-with-count"
              title="Fork your own copy of yukari-n-erb/notes to your account"
              data-ga-click="Repository, show fork modal, action:blob#show; text:Fork">
              <svg class="octicon octicon-repo-forked v-align-text-bottom" viewBox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1a1.993 1.993 0 0 0-1 3.72V6L5 8 3 6V4.72A1.993 1.993 0 0 0 2 1a1.993 1.993 0 0 0-1 3.72V6.5l3 3v1.78A1.993 1.993 0 0 0 5 15a1.993 1.993 0 0 0 1-3.72V9.5l3-3V4.72A1.993 1.993 0 0 0 8 1zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm3 10c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm3-10c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"/></svg>
              Fork
            </summary>
            <details-dialog class="anim-fade-in fast Box Box--overlay d-flex flex-column">
              <div class="Box-header">
                <button class="Box-btn-octicon btn-octicon float-right" type="button" aria-label="Close dialog" data-close-dialog>
                  <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"/></svg>
                </button>
                <h3 class="Box-title">Fork notes</h3>
              </div>
              <div class="overflow-auto text-center">
                <include-fragment>
                  <div class="octocat-spinner my-3" aria-label="Loading..."></div>
                  <p class="f5 text-gray">If this dialog fails to load, you can visit <a href="/yukari-n-erb/notes/fork">the fork page</a> directly.</p>
                </include-fragment>
              </div>
            </details-dialog>
          </details>

    <a href="/yukari-n-erb/notes/network/members" class="social-count"
       aria-label="39 users forked this repository">
      39
    </a>
  </li>
</ul>

      <h1 class="public ">
  <svg class="octicon octicon-repo-forked" viewBox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1a1.993 1.993 0 0 0-1 3.72V6L5 8 3 6V4.72A1.993 1.993 0 0 0 2 1a1.993 1.993 0 0 0-1 3.72V6.5l3 3v1.78A1.993 1.993 0 0 0 5 15a1.993 1.993 0 0 0 1-3.72V9.5l3-3V4.72A1.993 1.993 0 0 0 8 1zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm3 10c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm3-10c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"/></svg>
  <span class="author" itemprop="author"><a class="url fn" rel="author" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=45537257" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/yukari-n-erb">yukari-n-erb</a></span><!--
--><span class="path-divider">/</span><!--
--><strong itemprop="name"><a data-pjax="#js-repo-pjax-container" href="/yukari-n-erb/notes">notes</a></strong>

    <span class="fork-flag">
      <span class="text">forked from <a href="/hiromis/notes">hiromis/notes</a></span>
    </span>
</h1>

    </div>
    
<nav class="reponav js-repo-nav js-sidenav-container-pjax container"
     itemscope
     itemtype="http://schema.org/BreadcrumbList"
    aria-label="Repository"
     data-pjax="#js-repo-pjax-container">

  <span itemscope itemtype="http://schema.org/ListItem" itemprop="itemListElement">
    <a class="js-selected-navigation-item selected reponav-item" itemprop="url" data-hotkey="g c" aria-current="page" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages /yukari-n-erb/notes/tree/e_j" href="/yukari-n-erb/notes/tree/e_j">
      <svg class="octicon octicon-code" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M9.5 3L8 4.5 11.5 8 8 11.5 9.5 13 14 8 9.5 3zm-5 0L0 8l4.5 5L6 11.5 2.5 8 6 4.5 4.5 3z"/></svg>
      <span itemprop="name">Code</span>
      <meta itemprop="position" content="1">
</a>  </span>


  <span itemscope itemtype="http://schema.org/ListItem" itemprop="itemListElement">
    <a data-hotkey="g p" itemprop="url" class="js-selected-navigation-item reponav-item" data-selected-links="repo_pulls checks /yukari-n-erb/notes/pulls" href="/yukari-n-erb/notes/pulls">
      <svg class="octicon octicon-git-pull-request" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"/></svg>
      <span itemprop="name">Pull requests</span>
      <span class="Counter">0</span>
      <meta itemprop="position" content="3">
</a>  </span>


    <a data-hotkey="g b" class="js-selected-navigation-item reponav-item" data-selected-links="repo_projects new_repo_project repo_project /yukari-n-erb/notes/projects" href="/yukari-n-erb/notes/projects">
      <svg class="octicon octicon-project" viewBox="0 0 15 16" version="1.1" width="15" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10 12h3V2h-3v10zm-4-2h3V2H6v8zm-4 4h3V2H2v12zm-1 1h13V1H1v14zM14 0H1a1 1 0 0 0-1 1v14a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V1a1 1 0 0 0-1-1z"/></svg>
      Projects
      <span class="Counter" >0</span>
</a>

    <a class="js-selected-navigation-item reponav-item" data-hotkey="g w" data-selected-links="repo_wiki /yukari-n-erb/notes/wiki" href="/yukari-n-erb/notes/wiki">
      <svg class="octicon octicon-book" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M3 5h4v1H3V5zm0 3h4V7H3v1zm0 2h4V9H3v1zm11-5h-4v1h4V5zm0 2h-4v1h4V7zm0 2h-4v1h4V9zm2-6v9c0 .55-.45 1-1 1H9.5l-1 1-1-1H2c-.55 0-1-.45-1-1V3c0-.55.45-1 1-1h5.5l1 1 1-1H15c.55 0 1 .45 1 1zm-8 .5L7.5 3H2v9h6V3.5zm7-.5H9.5l-.5.5V12h6V3z"/></svg>
      Wiki
</a>
    <a class="js-selected-navigation-item reponav-item" data-selected-links="repo_graphs repo_contributors dependency_graph pulse alerts security people /yukari-n-erb/notes/pulse" href="/yukari-n-erb/notes/pulse">
      <svg class="octicon octicon-graph" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M16 14v1H0V0h1v14h15zM5 13H3V8h2v5zm4 0H7V3h2v10zm4 0h-2V6h2v7z"/></svg>
      Insights
</a>
    <a class="js-selected-navigation-item reponav-item" data-selected-links="repo_settings repo_branch_settings hooks integration_installations repo_keys_settings issue_template_editor /yukari-n-erb/notes/settings" href="/yukari-n-erb/notes/settings">
      <svg class="octicon octicon-gear" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14 8.77v-1.6l-1.94-.64-.45-1.09.88-1.84-1.13-1.13-1.81.91-1.09-.45-.69-1.92h-1.6l-.63 1.94-1.11.45-1.84-.88-1.13 1.13.91 1.81-.45 1.09L0 7.23v1.59l1.94.64.45 1.09-.88 1.84 1.13 1.13 1.81-.91 1.09.45.69 1.92h1.59l.63-1.94 1.11-.45 1.84.88 1.13-1.13-.92-1.81.47-1.09L14 8.75v.02zM7 11c-1.66 0-3-1.34-3-3s1.34-3 3-3 3 1.34 3 3-1.34 3-3 3z"/></svg>
      Settings
</a>
</nav>


  </div>

<div class="container new-discussion-timeline experiment-repo-nav  ">
  <div class="repository-content ">

    
    



  
    <a class="d-none js-permalink-shortcut" data-hotkey="y" href="/yukari-n-erb/notes/blob/05aca2f04dc533f70874131f5323c99073799d58/Lesson1.md">Permalink</a>

    <!-- blob contrib key: blob_contributors:v21:5bb944fe653cce24fd142332e28995d3 -->

    

    <div class="file-navigation">
      
<div class="select-menu branch-select-menu js-menu-container js-select-menu float-left js-load-contents"
  data-contents-url="/yukari-n-erb/notes/ref-list/e_j/Lesson1.md?source_action=show&amp;source_controller=blob">
  <button class="btn btn-sm select-menu-button js-menu-target css-truncate" data-hotkey="w"
    
    type="button" aria-label="Switch branches or tags" aria-expanded="false" aria-haspopup="true">
    <i>Branch:</i>
    <span class="js-select-button css-truncate-target">e_j</span>
  </button>

  <div class="select-menu-modal-holder js-menu-content js-navigation-container" data-pjax>
    <div class="select-menu-modal">
      <div class="js-select-menu-deferred-content"></div>
      <div class="select-menu-loading-overlay anim-pulse">
        <svg height="32" class="octicon octicon-octoface" viewBox="0 0 16 16" version="1.1" width="32" aria-hidden="true"><path fill-rule="evenodd" d="M14.7 5.34c.13-.32.55-1.59-.13-3.31 0 0-1.05-.33-3.44 1.3-1-.28-2.07-.32-3.13-.32s-2.13.04-3.13.32c-2.39-1.64-3.44-1.3-3.44-1.3-.68 1.72-.26 2.99-.13 3.31C.49 6.21 0 7.33 0 8.69 0 13.84 3.33 15 7.98 15S16 13.84 16 8.69c0-1.36-.49-2.48-1.3-3.35zM8 14.02c-3.3 0-5.98-.15-5.98-3.35 0-.76.38-1.48 1.02-2.07 1.07-.98 2.9-.46 4.96-.46 2.07 0 3.88-.52 4.96.46.65.59 1.02 1.3 1.02 2.07 0 3.19-2.68 3.35-5.98 3.35zM5.49 9.01c-.66 0-1.2.8-1.2 1.78s.54 1.79 1.2 1.79c.66 0 1.2-.8 1.2-1.79s-.54-1.78-1.2-1.78zm5.02 0c-.66 0-1.2.79-1.2 1.78s.54 1.79 1.2 1.79c.66 0 1.2-.8 1.2-1.79s-.53-1.78-1.2-1.78z"/></svg>
      </div>
    </div>
  </div>
</div>

      <div class="BtnGroup float-right">
        <a href="/yukari-n-erb/notes/find/e_j"
              class="js-pjax-capture-input btn btn-sm BtnGroup-item"
              data-pjax
              data-hotkey="t">
          Find file
        </a>
        <clipboard-copy for="blob-path" class="btn btn-sm BtnGroup-item">
          Copy path
        </clipboard-copy>
      </div>
      <div id="blob-path" class="breadcrumb">
        <span class="repo-root js-repo-root"><span class="js-path-segment"><a data-pjax="true" href="/yukari-n-erb/notes/tree/e_j"><span>notes</span></a></span></span><span class="separator">/</span><strong class="final-path">Lesson1.md</strong>
      </div>
    </div>


    
  <div class="commit-tease">
      <span class="float-right">
        <a class="commit-tease-sha" href="/yukari-n-erb/notes/commit/09d2c1c843bc7d67046a38d14c02daa677817b9a" data-pjax>
          09d2c1c
        </a>
        <relative-time datetime="2019-01-23T08:07:23Z">Jan 23, 2019</relative-time>
      </span>
      <div>
        <a rel="contributor" data-skip-pjax="true" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=42315895" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/gsajko"><img class="avatar" src="https://avatars2.githubusercontent.com/u/42315895?s=40&amp;v=4" width="20" height="20" alt="@gsajko" /></a>
        <a class="user-mention" rel="contributor" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=42315895" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/gsajko">gsajko</a>
          <a data-pjax="true" title="Update Lesson1.md

minor fixes" class="message" href="/yukari-n-erb/notes/commit/09d2c1c843bc7d67046a38d14c02daa677817b9a">Update Lesson1.md</a>
      </div>

    <div class="commit-tease-contributors">
      
<details class="details-reset details-overlay details-overlay-dark lh-default text-gray-dark float-left mr-2" id="blob_contributors_box">
  <summary
      class="btn-link"
      aria-haspopup="dialog"
      
      
      >
    
    <span><strong>3</strong> contributors</span>
  </summary>
  <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast " aria-label="Users who have contributed to this file">
    <div class="Box-header">
      <button class="Box-btn-octicon btn-octicon float-right" type="button" aria-label="Close dialog" data-close-dialog>
        <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"/></svg>
      </button>
      <h3 class="Box-title">Users who have contributed to this file</h3>
    </div>
    
        <ul class="list-style-none overflow-auto">
            <li class="Box-row">
              <a class="link-gray-dark no-underline" href="/hiromis">
                <img class="avatar mr-2" alt="" src="https://avatars2.githubusercontent.com/u/5401333?s=40&amp;v=4" width="20" height="20" />
                hiromis
</a>            </li>
            <li class="Box-row">
              <a class="link-gray-dark no-underline" href="/Benudek">
                <img class="avatar mr-2" alt="" src="https://avatars1.githubusercontent.com/u/11727079?s=40&amp;v=4" width="20" height="20" />
                Benudek
</a>            </li>
            <li class="Box-row">
              <a class="link-gray-dark no-underline" href="/gsajko">
                <img class="avatar mr-2" alt="" src="https://avatars2.githubusercontent.com/u/42315895?s=40&amp;v=4" width="20" height="20" />
                gsajko
</a>            </li>
        </ul>

  </details-dialog>
</details>
          <a class="avatar-link" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=5401333" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/yukari-n-erb/notes/commits/master/Lesson1.md?author=hiromis">
      <img class="avatar" src="https://avatars2.githubusercontent.com/u/5401333?s=40&amp;v=4" width="20" height="20" alt="@hiromis" /> 
</a>    <a class="avatar-link" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=11727079" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/yukari-n-erb/notes/commits/master/Lesson1.md?author=Benudek">
      <img class="avatar" src="https://avatars1.githubusercontent.com/u/11727079?s=40&amp;v=4" width="20" height="20" alt="@Benudek" /> 
</a>    <a class="avatar-link" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=42315895" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/yukari-n-erb/notes/commits/master/Lesson1.md?author=gsajko">
      <img class="avatar" src="https://avatars2.githubusercontent.com/u/42315895?s=40&amp;v=4" width="20" height="20" alt="@gsajko" /> 
</a>

    </div>
  </div>




    <div class="file ">
      
<div class="file-header">

  <div class="file-actions">


    <div class="BtnGroup">
      <a id="raw-url" class="btn btn-sm BtnGroup-item" href="/yukari-n-erb/notes/raw/e_j/Lesson1.md">Raw</a>
        <a class="btn btn-sm js-update-url-with-hash BtnGroup-item" data-hotkey="b" href="/yukari-n-erb/notes/blame/e_j/Lesson1.md">Blame</a>
      <a rel="nofollow" class="btn btn-sm BtnGroup-item" href="/yukari-n-erb/notes/commits/e_j/Lesson1.md">History</a>
    </div>

        <a class="btn-octicon tooltipped tooltipped-nw"
           href="https://desktop.github.com"
           aria-label="Open this file in GitHub Desktop"
           data-ga-click="Repository, open with desktop, type:windows">
            <svg class="octicon octicon-device-desktop" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M15 2H1c-.55 0-1 .45-1 1v9c0 .55.45 1 1 1h5.34c-.25.61-.86 1.39-2.34 2h8c-1.48-.61-2.09-1.39-2.34-2H15c.55 0 1-.45 1-1V3c0-.55-.45-1-1-1zm0 9H1V3h14v8z"/></svg>
        </a>

          <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="inline-form js-update-url-with-hash" action="/yukari-n-erb/notes/edit/e_j/Lesson1.md" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="ciPMcRd86Ozu7qqqEl38KdSWtGTX7mellRuiOTyuORtYluT4kzCcEWVdJG/keGMED38PH1GxoTsEo58uPiC//g==" />
            <button class="btn-octicon tooltipped tooltipped-nw" type="submit"
              aria-label="Edit this file" data-hotkey="e" data-disable-with>
              <svg class="octicon octicon-pencil" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M0 12v3h3l8-8-3-3-8 8zm3 2H1v-2h1v1h1v1zm10.3-9.3L12 6 9 3l1.3-1.3a.996.996 0 0 1 1.41 0l1.59 1.59c.39.39.39 1.02 0 1.41z"/></svg>
            </button>
</form>
        <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="inline-form" action="/yukari-n-erb/notes/delete/e_j/Lesson1.md" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="qnWdf9HWI9YtMXgKAo0WC2WCJ4nduCAMnyOmHuWG9hoX1R4JtmkWIRLH4WwP3X+1OgyUasKbRmYsRej6x/prtQ==" />
          <button class="btn-octicon btn-octicon-danger tooltipped tooltipped-nw" type="submit"
            aria-label="Delete this file" data-disable-with>
            <svg class="octicon octicon-trashcan" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M11 2H9c0-.55-.45-1-1-1H5c-.55 0-1 .45-1 1H2c-.55 0-1 .45-1 1v1c0 .55.45 1 1 1v9c0 .55.45 1 1 1h7c.55 0 1-.45 1-1V5c.55 0 1-.45 1-1V3c0-.55-.45-1-1-1zm-1 12H3V5h1v8h1V5h1v8h1V5h1v8h1V5h1v9zm1-10H2V3h9v1z"/></svg>
          </button>
</form>  </div>

  <div class="file-info">
      1003 lines (565 sloc)
      <span class="file-info-divider"></span>
    75 KB
  </div>
</div>

      
  <div id="readme" class="readme blob instapaper_body js-code-block-container">
    <article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-lesson-1" class="anchor" aria-hidden="true" href="#lesson-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Lesson 1</h1>
<p><a href="http://course-v3.fast.ai/" rel="nofollow">Webpage</a> / <a href="https://youtu.be/BWWm4AzsdLk" rel="nofollow">Video</a> /  <a href="https://forums.fast.ai/t/lesson-1-official-resources-and-updates/27936" rel="nofollow">Lesson Forum</a> / <a href="https://forums.fast.ai/t/faq-resources-and-official-course-updates/27934/1" rel="nofollow">General Forum</a></p>
<h2><a id="user-content-welcome" class="anchor" aria-hidden="true" href="#welcome"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Welcome!</h2>
<p>Make sure your GPU environment is set up and you can run Jupyter Notebook</p>
<p><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl1/00_notebook_tutorial.ipynb">00_notebook_tutorial.ipynb</a></p>
<p>Four shortcuts:</p>
<ul>
<li>
<p><kbd>Shift</kbd>+<kbd>Enter</kbd>: Runs the code or markdown on a cell</p>
</li>
<li>
<p><kbd>Up Arrow</kbd>+<kbd>Down Arrow</kbd>: Toggle across cells</p>
</li>
<li>
<p><kbd>b</kbd>: Create new cell</p>
</li>
<li>
<p><kbd>0</kbd>+<kbd>0</kbd>: Restart Kernel</p>
</li>
</ul>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=165" rel="nofollow">2:45</a>]</p>
<p>Jupyter Notebook is a really interesting device for data scientists because it lets you run interactive experiments and give you not just a static piece of information but something you can interactively experiment with.</p>
<p>How to use notebooks and the materials well based on the last three years of experience:</p>
<ol>
<li>Just watch a lesson end to end.
<ul>
<li>Don't try to follow along because it's not really designed to go the speed where you can follow along. It's designed to be something where you just take in the information, you get a general sense of all the pieces, how it all fits together.</li>
<li>Then you can go back and go through it more slowly pausing the video, trying things out, making sure that you can do the things that I'm doing and you can try and extend them to do things in your own way.</li>
<li>Don't try and stop and understand everything the first time.</li>
</ul>
</li>
</ol>
<h3><a id="user-content-you-can-do-world-class-practitioner-level-deep-learning-431" class="anchor" aria-hidden="true" href="#you-can-do-world-class-practitioner-level-deep-learning-431"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>You can do world-class practitioner level deep learning [<a href="https://youtu.be/BWWm4AzsdLk?t=271" rel="nofollow">4:31</a>]</h3>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/1.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/1.png" alt="" style="max-width:100%;"></a></p>
<p>Main places to be looking for things are:</p>
<ul>
<li><a href="http://course-v3.fast.ai/" rel="nofollow">http://course-v3.fast.ai/</a></li>
<li><a href="https://forums.fast.ai/latest" rel="nofollow">https://forums.fast.ai/</a></li>
</ul>
<h3><a id="user-content-a-little-bit-about-why-we-should-listen-to-jeremy-527" class="anchor" aria-hidden="true" href="#a-little-bit-about-why-we-should-listen-to-jeremy-527"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>A little bit about why we should listen to Jeremy [<a href="https://youtu.be/BWWm4AzsdLk?t=327" rel="nofollow">5:27</a>]</h3>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/2.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/2.png" alt="" style="max-width:100%;"></a></p>
<h3><a id="user-content-using-machine-learning-to-do-useful-things-648" class="anchor" aria-hidden="true" href="#using-machine-learning-to-do-useful-things-648"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Using machine learning to do useful things [<a href="https://youtu.be/BWWm4AzsdLk?t=408" rel="nofollow">6:48</a>]</h3>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/3.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/3.png" alt="" style="max-width:100%;"></a></p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=446" rel="nofollow">7:26</a>]</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/4.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/4.png" alt="" style="max-width:100%;"></a></p>
<p>If you follow along with 10 hours a week or so approach for the 7 weeks, by the end, you will be able to:</p>
<ol>
<li>Build an image classification model on pictures that you choose that will work at a world class level</li>
<li>Classify text using whatever datasets you're interested in</li>
<li>Make predictions of commercial applications like sales</li>
<li>Build recommendation systems such as the one used by Netflix</li>
</ol>
<p>Not toy examples of any of these but actually things that can come top 10 in Kaggle competitions, that can beat everything that's in the academic community.</p>
<p>The prerequisite is one year of coding and high school math.</p>
<h3><a id="user-content-what-people-say-about-deep-learning-which-are-either-pointless-or-untrue-905" class="anchor" aria-hidden="true" href="#what-people-say-about-deep-learning-which-are-either-pointless-or-untrue-905"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What people say about deep learning which are either pointless or untrue [<a href="https://youtu.be/BWWm4AzsdLk?t=545" rel="nofollow">9:05</a>]</h3>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/5.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/5.png" alt="" style="max-width:100%;"></a></p>
<ul>
<li>It's not a black box. It's really great for interpreting what's going on.</li>
<li>It does not need much data for most practical applications.</li>
<li>You don't need a PhD. Rachel has one so it doesn't actually stop you from doing deep learning if you have a PhD.</li>
<li>It can be used very widely for lots of different applications, not just for vision.</li>
<li>You don't need lots of hardware. 36 cents an hour server is more than enough to get world-class results for most problems.</li>
<li>It is true that maybe this is not going to help you build a sentient brain, but that's not our focus. We are focused on solving interesting real-world problems.</li>
</ul>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=624" rel="nofollow">10:24</a>]</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/6.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/6.png" alt="" style="max-width:100%;"></a></p>
<p>Baseball vs. Cricket - An example by Nikhil of what you are going to be able to do by the end of lesson 1:</p>
<h3><a id="user-content-topdown-approach-1102" class="anchor" aria-hidden="true" href="#topdown-approach-1102"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Topdown approach [<a href="https://youtu.be/BWWm4AzsdLk?t=662" rel="nofollow">11:02</a>]</h3>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/7.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/7.png" alt="" style="max-width:100%;"></a></p>
<p>We are going to start by looking at code which is different to many of academic courses. We are going to learn to build a useful thing today. That means that at the end of today, you won't know all the theory. There will be lots of aspects of what we do that you don't know why or how it works. That's okay! You will learn why and how it works over the next 7 weeks. But for now, we've found that what works really well is to actually get your hands dirty coding - not focusing on theory.</p>
<h2><a id="user-content-whats-your-pet-1226" class="anchor" aria-hidden="true" href="#whats-your-pet-1226"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What's your pet [<a href="https://youtu.be/BWWm4AzsdLk?t=746" rel="nofollow">12:26</a>]</h2>
<p><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson1-pets.ipynb">lesson1-pets.ipynb</a></p>
<p><kbd>Shift</kbd>+<kbd>Enter</kbd> to run a cell</p>
<p>These three lines is what we start every notebook with:</p>
<pre><code>%reload_ext autoreload
%autoreload 2
%matplotlib inline
</code></pre>
<p>These things starting <code>%</code> are special directives to Jupyter Notebook itself, they are not Python code. They are called "magics."</p>
<ul>
<li>If somebody changes underlying library code while I'm running this, please reload it automatically</li>
<li>If somebody asks to plot something, then please plot it here in this Jupyter Notebook</li>
</ul>
<p>The next two lines load up the fastai library:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> fastai <span class="pl-k">import</span> <span class="pl-k">*</span>
<span class="pl-k">from</span> fastai.vision <span class="pl-k">import</span> <span class="pl-k">*</span></pre></div>
<p>What is fastai library? <a href="http://docs.fast.ai/" rel="nofollow">http://docs.fast.ai/</a></p>
<p>Everything we are going to do is going to be using either fastai or <a href="https://pytorch.org/" rel="nofollow">PyTorch</a> which fastai sits on top of. PyTorch is fast growing extremely popular library. We use it because we used to use TensorFlow a couple years ago and we found we can do a lot more, a lot more quickly with PyTorch.</p>
<p>Currently fastai supports four applications:</p>
<ol>
<li>Computer vision</li>
<li>Natural language text</li>
<li>Tabular data</li>
<li>Collaborative filtering</li>
</ol>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=945" rel="nofollow">15:45</a>]</p>
<p><code>import *</code> - something you've all been told to never ever do.</p>
<p>There are very good reasons to not use <code>import *</code> in standard production code with most libraries. But things like MATLAB is the opposite. Everything is there for you all the time. You don't even have to import things a lot of the time. It's kind of funny - we've got these two extremes of how do I code. The scientific programming community has one way, and then software engineering community has the other. Both have really good reasons for doing things.</p>
<p>With the fastai library, we actually support both approaches. In Jupyter Notebook where you want to be able to quickly interactively try stuff out, you don't want to constantly going back up to the top and importing more stuff. You want to be able to use lots of tab complete and be very experimental, so <code>import *</code> is great. When you are building stuff in production, you can do the normal PEP8 style proper software engineering practices. This is a different style of coding. It's not that there are no rules in data science programming, the rules are different. When you're training models, the most important thing is to be able to interactively experiment quickly. So you will see we use a lot of different processes, styles, and stuff to what you are used to. But they are there for a reason and you'll learn about them over time.</p>
<p>The other thing to mention is that the fastai library is designed in a very interesting modular way and when you do use import *, there's far less clobbering of things than you might expect. It's all explicitly designed to allow you to pull in things and use them quickly without having problems.</p>
<h2><a id="user-content-looking-at-the-data-1756" class="anchor" aria-hidden="true" href="#looking-at-the-data-1756"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Looking at the data [<a href="https://youtu.be/BWWm4AzsdLk?t=1076" rel="nofollow">17:56</a>]</h2>
<p>Two main places that we will be tending to get data from for the course:</p>
<ol>
<li>Academic datasets
<ul>
<li>Academic datasets are really important. They are really interesting. They are things where academics spend a lot of time curating and gathering a dataset so that they can show how well different kinds of approaches work with that data. The idea is they try to design datasets that are challenging in some way and require some kind of breakthrough to do them well.</li>
<li>We are going to start with an academic dataset called the pet dataset.</li>
</ul>
</li>
<li>Kaggle competition datasets</li>
</ol>
<p>Both types of datasets are interesting for us particularly because they provide strong baseline. That is to say you want to know if you are doing a good job. So with Kaggle datasets that come from a competition, you can actually submit your results to Kaggle and see how well you would have gone in that competition. If you can get in about the top 10%, then I'd say you are doing pretty well.</p>
<p>Academic datasets, academics write down in papers what the state of the art is so how well did they go with using models on that dataset. So this is what we are going to do. We are going to try to create models that get right up towards the top of Kaggle competitions, preferably in the top 10, not just top 10% or that meet or exceed academic state-of-the-art published results. So when you use an academic dataset, it's important to cite it. You don't need to read that paper right now, but if you are interested in learning more about it and why it was created and how it was created, all the details are there.</p>
<p>Pet dataset is going to ask us to distinguish between 37 different categories of dog breed and cat breed. So that's really hard. In fact, every course until this one, we've used a different dataset which is one where you just have to decide if something is a dog or a cat. So you've got a 50-50 chance right away and dogs and cats look really different. Or else lots of dog breeds and cat breeds look pretty much the same.</p>
<p>So why have we changed the dataset? We've got to the point now where deep learning os so fast and so easy that the dogs versus cats problem which a few years ago was considered extremely difficult ~80% accuracy was the state of the art, it's now too easy. Our models were basically getting everything right all the time without any tuning and so there weren't really a lot of opportunities for me to show you how to do more sophisticated stuff. So we've picked a harder problem this year.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=1251" rel="nofollow">20:51</a>]</p>
<p>This kind of thing where you have to distinguish between similar categories is called fine grained classification in the academic context.</p>
<h3><a id="user-content-untar_data" class="anchor" aria-hidden="true" href="#untar_data"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>untar_data</h3>
<p>The first thing we have to do is download and extract the data that we want. We're going to be using this function called <code>untar_data</code> which will download it automatically and untar it. AWS has been kind enough to give us lots of space and bandwidth for these datasets so they'll download super quickly for you.</p>
<div class="highlight highlight-source-python"><pre>path <span class="pl-k">=</span> untar_data(URLs.<span class="pl-c1">PETS</span>); path</pre></div>
<h3><a id="user-content-help" class="anchor" aria-hidden="true" href="#help"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>help</h3>
<p>The first question then would be how do I know what <code>untar_data</code> does. You could just type help and you will find out what module it came from (since we did <code>import *</code> you don't necessarily know that), what it does, and something you might not have seen before even if you are an experienced programmer is what exactly you pass to it. You're probably used to seeing the names: url, fname, dest, but you might not be used to seeing <code>Union[pathlib.Path, str]</code>. These bits are types and if you're used to typed programming language, you would be used to seeing them, but Python programmers are less used to it. But if you think about it, you don't actually know how to use a function unless you know what type each thing is that you're providing it. So we make sure that we give you that type information directly here in the help.</p>
<p>In this case, <code>url</code> is a string, <code>fname</code> is either path or a string and defaults to nothing (<code>Union</code> means "either"). <code>dest</code> is either a string or a path and defaults to nothing.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c1">help</span>(untar_data)</pre></div>
<pre><code>Help on function untar_data in module fastai.datasets:

untar_data(url:str, fname:Union[pathlib.Path, str]=None, dest:Union[pathlib.Path, str]=None)
    Download `url` if doesn't exist to `fname` and un-tgz to folder `dest`
</code></pre>
<p>We'll learn more shortly about how to get more documentation about the details of this, but for now, we can see we don't have to pass in a file name <code>fname</code> or a destination <code>dest</code>, it'll figure them out for us from the URL.</p>
<p>For all the datasets we'll be using in the course, we already have constants defined for all of them. So in this <a href="https://github.com/fastai/fastai/blob/master/fastai/datasets.py">URLs</a> class, you can see where it's going to grab it from.</p>
<p><code>untar_data</code> will download that to some convenient path and untar it for us and it will then return the value of path.</p>
<div class="highlight highlight-source-python"><pre>path <span class="pl-k">=</span> untar_data(URLs.<span class="pl-c1">PETS</span>); path</pre></div>
<pre><code>PosixPath('/data1/jhoward/git/course-v3/nbs/dl1/data/oxford-iiit-pet')
</code></pre>
<p>In Jupyter Notebook, you can just write a variable on its own (semicolon is just an end of statement in Python) and it prints it. You can also say <code>print(path)</code> but again, we are trying to do everything fast and interactively, so just write it and here is the path where it's given us our data.</p>
<p>Next time you run this, since you've already downloaded it, it won't download it again. Since you've already untared it, it won't untar it again. So everything is designed to be pretty automatic and easy.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=1430" rel="nofollow">23:50</a>]</p>
<p>There are some things in Python that are less convenient for interactive use than they should be. For example, when you do have a path object, seeing what's in it actually takes a lot more typing than I would like. So sometimes we add functionality into existing Python stuff. One of the things we do is add a <code>ls()</code> method to path.</p>
<div class="highlight highlight-source-python"><pre>path.ls()</pre></div>
<pre><code>['annotations', 'images']
</code></pre>
<p>These are what's inside this path, so that's what we just downloaded.</p>
<h3><a id="user-content-python-3-pathlib-2425" class="anchor" aria-hidden="true" href="#python-3-pathlib-2425"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Python 3 pathlib [<a href="https://youtu.be/BWWm4AzsdLk?t=1465" rel="nofollow">24:25</a>]</h3>
<div class="highlight highlight-source-python"><pre>path_anno <span class="pl-k">=</span> path<span class="pl-k">/</span><span class="pl-s"><span class="pl-pds">'</span>annotations<span class="pl-pds">'</span></span>
path_img <span class="pl-k">=</span> path<span class="pl-k">/</span><span class="pl-s"><span class="pl-pds">'</span>images<span class="pl-pds">'</span></span></pre></div>
<p>If you are an experienced Python programmer, you may not be familiar with this approach of using a slash like this. This is a really convenient function that's part of Python 3. It's functionality from <a href="https://docs.python.org/3/library/pathlib.html" rel="nofollow">pathlib</a>. Path object is much better to use than strings. They let you use basically create sub paths like this. It doesn't matter if you're on Windows, Linux, or Mac. It is always going to work exactly the same way. <code>path_img</code> is the path to the images in that dataset.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=1497" rel="nofollow">24:57</a>]</p>
<p>So if you are starting with a brand new dataset trying to do some deep learning on it. What do you do? Well, the first thing you would want to do is probably see what's in there. So we found that <code>annotations</code> and <code>images</code> are the directories in there, so what's in this images?</p>
<h3><a id="user-content-get_image_files-2515" class="anchor" aria-hidden="true" href="#get_image_files-2515"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>get_image_files [<a href="https://youtu.be/BWWm4AzsdLk?t=1515" rel="nofollow">25:15</a>]</h3>
<p>get_image_files will just grab an array of all of the image files based on extension in a path.</p>
<div class="highlight highlight-source-python"><pre>fnames <span class="pl-k">=</span> get_image_files(path_img)
fnames[:<span class="pl-c1">5</span>]</pre></div>
<pre><code>[PosixPath('/data1/jhoward/git/course-v3/nbs/dl1/data/oxford-iiit-pet/images/american_bulldog_146.jpg'),
 PosixPath('/data1/jhoward/git/course-v3/nbs/dl1/data/oxford-iiit-pet/images/german_shorthaired_137.jpg'),
 PosixPath('/data1/jhoward/git/course-v3/nbs/dl1/data/oxford-iiit-pet/images/japanese_chin_139.jpg'),
 PosixPath('/data1/jhoward/git/course-v3/nbs/dl1/data/oxford-iiit-pet/images/great_pyrenees_121.jpg'),
 PosixPath('/data1/jhoward/git/course-v3/nbs/dl1/data/oxford-iiit-pet/images/Bombay_151.jpg')]
</code></pre>
<p>This is a pretty common way for computer vision datasets to get passed around - just one folder with a whole bunch of files in it. So the interesting bit then is how do we get the labels. In machine learning, the labels refer to the thing we are trying to predict. If we just eyeball this, we could immediately see that the labels are actually part of the file names. It's kind of like <code>path/label_number.extension</code>. We need to somehow get a list of <code>label</code> bits of each file name, and that will give us our labels. Because that's all you need to build a deep learning model:</p>
<ul>
<li>Pictures (files containing the images)</li>
<li>Labels</li>
</ul>
<p>In fastai, this is made really easy. There is an object called <code>ImageDataBunch</code>. An ImageDataBunch represents all of the data you need to build a model and there's some factory method which try to make it really easy for you to create that data bunch - a training set, a validation set with images and labels.</p>
<p>In this case, we need to extract the labels from the names. We are going to use <code>from_name_re</code>. <code>re</code> is the module in Python that does regular expressions - things that's really useful for extracting text.</p>
<p>Here is the regular expression that extract the label for this dataset:</p>
<div class="highlight highlight-source-python"><pre>np.random.seed(<span class="pl-c1">2</span>)
pat <span class="pl-k">=</span> <span class="pl-sr"><span class="pl-k">r</span><span class="pl-pds">'</span>/<span class="pl-c1">(</span>[<span class="pl-k">^</span><span class="pl-c1">/</span>]<span class="pl-k">+</span><span class="pl-c1">)</span>_<span class="pl-c1">\d</span><span class="pl-k">+</span><span class="pl-c1">.</span>jpg<span class="pl-c1">$</span><span class="pl-pds">'</span></span></pre></div>
<p>With this factory method, we can basically say:</p>
<ul>
<li>path_img: a path containing images</li>
<li>fnames: a list of file names</li>
<li>pat: a regular expression (i.e. pattern) to be used to extract the label from the file name</li>
<li>ds_tfm: we'll talk about transforms later</li>
<li>size: what size images do you want to work with.</li>
</ul>
<p>This might seem weird because images have size. This is a shortcoming of current deep learning technology which is that a GPU has to apply the exact same instruction to a whole bunch of things at the same time in order to be fast. If the images are different shapes and sizes, you can't do that. So we actually have to make all of the images the same shape and size. In part 1 of the course, we are always going to be making images square shapes. Part 2, we will learn how to use rectangles as well. It turns out to be surprisingly nuanced. But pretty much everybody in pretty much all computer vision modeling nearly all of it uses this approach of square. 224 by 224, for reasons we'll learn about, is an extremely common size that most models tend to use so if you just use size=224, you're probably going to get pretty good results most of the time. This is kind of the little bits of artisanship that I want to teach you which is what generally just works. So if you just use size 224, that'll generally just work for most things most of the time.</p>
<div class="highlight highlight-source-python"><pre>data <span class="pl-k">=</span> ImageDataBunch.from_name_re(path_img, fnames, pat, <span class="pl-v">ds_tfms</span><span class="pl-k">=</span>get_transforms(), <span class="pl-v">size</span><span class="pl-k">=</span><span class="pl-c1">224</span>)
data.normalize(imagenet_stats)</pre></div>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=1756" rel="nofollow">29:16</a>]</p>
<p><code>ImageDataBunch.from_name_re</code> is going to return a DataBunch object. In fastai, everything you model with is going to be a DataBunch object. Basically DataBunch object contains 2 or 3 datasets - it contains your training data, validation data, and optionally test data. For each of those, it contains your images and your labels, your texts and your labels, or your tabular data and your labels, or so forth. And that all sits there in this one place(i.e. <code>data</code>).</p>
<p>Something we will learn more about in a little bit is normalization. But generally in nearly all machine learning tasks, you have to make all of your data about the same "size" - they are specifically about the same mean and standard deviation.  So there is a normalize function that we can use to normalize our data bunch in that way.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=1825" rel="nofollow">30:25</a>]</p>
<p>Question: What does the function do if the image size is not 224?</p>
<p>This is what we are going to learn about shortly. Basically this thing called transforms is used to do a number of the things and one of the things it does is to make something size 224.</p>
<h3><a id="user-content-datashow_batch" class="anchor" aria-hidden="true" href="#datashow_batch"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>data.show_batch</h3>
<p>Let's take a look at a few pictures. Here are a few pictures of things from my data bunch. So you can see data.show_batch can be used to show me some of the contents in my data bunch. So you can see roughly what's happened is that they all seem to have being zoomed and cropped in a reasonably nice way. So basically what it'll do is something called by default center cropping which means it'll grab the middle bit and it'll also resize it. We'll talk more about the detail of this because it turns out to actually be quite important, but basically a combination of cropping and resizing is used.</p>
<div class="highlight highlight-source-python"><pre>data.show_batch(<span class="pl-v">rows</span><span class="pl-k">=</span><span class="pl-c1">3</span>, <span class="pl-v">figsize</span><span class="pl-k">=</span>(<span class="pl-c1">7</span>,<span class="pl-c1">6</span>))</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/8.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/8.png" alt="" style="max-width:100%;"></a></p>
<p>Something else we are going to learn about is we also use this to do something called data augmentation. So there's actually some randomization in how much and where it crops and stuff like that.</p>
<p>Basic the basic idea is some cropping, resizing, and padding. So there's all kinds of different ways it depends on data augmentation which we are going to learn about shortly.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=1911" rel="nofollow">31:51</a>]</p>
<p><strong>Question</strong>: What does it mean to normalize the images?</p>
<p>Normalizing the images, we're going to be learning more about later in the course, but in short, it means that the pixel values start out from naught to 255. And some channels might tend to be really bright, some might tend to be really not bright at all, some might vary a lot, and some might not very much at all. It really helps train a deep learning model if each one of those red green and blue channels has a mean of zero and a standard deviation of one.</p>
<p>If your data is not normalized, it can be quite difficult for your model to train well. So if you have trouble training a model, one thing to check is that you've normalized it.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=1980" rel="nofollow">33:00</a>]
<strong>Question</strong>: As GPU mem will be in power of 2, doesn't size 256 sound more practical considering GPU utilization compared to 224?</p>
<p>The brief answer is that the models are designed so that the final layer is of size 7 by 7, so we actually want something where if you go 7 times 2 a bunch of times (224 = 7<em>2</em>2<em>2</em>2*2), then you end up with something that's a good size.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=2007" rel="nofollow">33:27</a>]</p>
<p>We will get to all these details but the key thing is I wanted to get to training a model as quickly as possible.</p>
<h3><a id="user-content-it-is-important-to-look-at-the-data" class="anchor" aria-hidden="true" href="#it-is-important-to-look-at-the-data"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>It is important to look at the data</h3>
<p>One of the most important thing to be a really good practitioner is to be able to look at your data. So it's really important to remember to go to <code>data.show_batch</code> and take a look. It's surprising how often when you actually look at the dataset you've been given that you realize it's got weird black borders on it, some of the things have text covering up some of it, or some of it is rotated in odd ways. So make sure you take a look.</p>
<p>The other thing we want to do is to look at the labels. All of the possible label names are called your classes. With DataBunch, you can print out your <code>data.classes</code>.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c1">print</span>(data.classes)
<span class="pl-c1">len</span>(data.classes),data.c</pre></div>
<pre><code>['american_bulldog', 'german_shorthaired', 'japanese_chin', 'great_pyrenees', 'Bombay', 'Bengal', 'keeshond', 'shiba_inu', 'Sphynx', 'boxer', 'english_cocker_spaniel', 'american_pit_bull_terrier', 'Birman', 'basset_hound', 'British_Shorthair', 'leonberger', 'Abyssinian', 'wheaten_terrier', 'scottish_terrier', 'Maine_Coon', 'saint_bernard', 'newfoundland', 'yorkshire_terrier', 'Persian', 'havanese', 'pug', 'miniature_pinscher', 'Russian_Blue', 'staffordshire_bull_terrier', 'beagle', 'Siamese', 'samoyed', 'chihuahua', 'Egyptian_Mau', 'Ragdoll', 'pomeranian', 'english_setter']

(37, 37)
</code></pre>
<p>That's all of the possible labels that we found by using that regular expression on the file names. We learnt earlier on at the top that there are 37 possible categories, so just checking <code>len(data.classes)</code>, it is indeed 37. DataBunch will always have a property called <code>c</code>. We will get to the technical detail later, but for now, you can kind of think of it as being the number of classes. For things like regression problems and multi-label classification, that's not exactly accurate, but it'll do for now. It is important to know that <code>data.c</code> is a really important piece of information that is something like, or at least for classification problems it is, the number of classes.</p>
<h2><a id="user-content-training-3507" class="anchor" aria-hidden="true" href="#training-3507"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Training [<a href="https://youtu.be/BWWm4AzsdLk?t=2107" rel="nofollow">35:07</a>]</h2>
<p>Believe it or not, we are now ready to train a model. A model is trained in fastai using something called a "learner".</p>
<ul>
<li><strong>DataBunch</strong>: A general fastai concept for your data, and from there, there are subclasses for particular applications like ImageDataBunch</li>
<li><strong>Learner</strong>: A general concept for things that can learn to fit a model. From that, there are various subclasses to make things easier in particular, there is a convnet learner  (something that will create a convolutional neural network for you).</li>
</ul>
<div class="highlight highlight-source-python"><pre>learn <span class="pl-k">=</span> create_cnn(data, models.resnet34, <span class="pl-v">metrics</span><span class="pl-k">=</span>error_rate)</pre></div>
<p>For now, just know that to create a learner for a convolutional neural network, you just have to tell it two things:
<code>data</code>: What's your data. Not surprisingly, it takes a data bunch.
<code>arch</code>: What's your architecture. There are lots of different ways of constructing a convolutional neural network.</p>
<p>For now, the most important thing for you to know is that there's a particular kind of model called ResNet which works extremely well nearly all the time. For a while, at least, you really only need to be doing choosing between two things which is what size ResNet do you want. There are ResNet34 and ResNet50. When we are getting started with something, I'll pick a smaller one because it'll train faster. That's as much as you need to know to be a pretty good practitioner about architecture for now which is that there are two variants of one architecture that work pretty well: ResNet34 and ResNet50. Start with a smaller one and see if it's good enough.</p>
<p>That is all the information we need to create a convolutional neural network learner.</p>
<p>There is one other thing I'm going to give it though which is a list of metrics. Metrics are literally just things that gets printed out as it's training. So I'm saying I would like you to print out error rate.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=2245" rel="nofollow">37:25</a>]</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/c1.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/c1.png" alt="" style="max-width:100%;"></a></p>
<p>The first time I run this on a newly installed box, it downloads the ResNet34 pre-trained weights. What that means is that this particular model has actually already been trained for a particular task. And that particular task is that it was trained on looking at about one and a half million pictures of all kinds of different things, a thousand categories of things, using an image dataset called ImageNet. So we can download those pre-trained weights so that we don't start with a model that knows nothing about anything, but we actually start with a model that knows how to recognize a thousand categories of things in ImageNet. I don't think all of these 37 categories of pet are in ImageNet but there were certainly some kinds of dog and some kinds of cat. So this pre-trained model knows quite a little bit about what pets look like, and it certainly knows quite a lot about what animals look like and what photos look like. So the idea is that we don't start with a model that knows nothing at all, but we start by downloading a model that knows something about recognizing images already. So it downloads for us automatically, the first time we use it, a pre-trained model and then from now on, it won't need to download it again - it'll just use the one we've got.</p>
<h2><a id="user-content-transfer-learning-3854" class="anchor" aria-hidden="true" href="#transfer-learning-3854"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transfer learning [<a href="https://youtu.be/BWWm4AzsdLk?t=2334" rel="nofollow">38:54</a>]</h2>
<p>This is really important. We are going to learn a lot about this. It's kind of the focus of the whole course which is how to do this thing called "transfer learning." How to take a model that already knows how to do something pretty well and make it so that it can do your thing really well. We will take a pre-trained model, and then we fit it so that instead of predicting a thousand categories of ImageNet with ImageNet data, it predicts the 37 categories of pets using your pet data. By doing this, you can train models in 1/100 or less of the time of regular model training with 1/100 or less of the data of regular model training. Potentially, many thousands of times less. Remember I showed you the slide of Nikhil's lesson 1 project from last year? He used 30 images. There are not cricket and baseball images in ImageNet but it turns out that ImageNet is already so good at recognizing things in the world that just 30 examples of people playing baseball and cricket was enough to build a nearly perfect classifier.</p>
<h2><a id="user-content-overfitting-4005" class="anchor" aria-hidden="true" href="#overfitting-4005"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Overfitting [<a href="https://youtu.be/BWWm4AzsdLk?t=2405" rel="nofollow">40:05</a>]</h2>
<p>Wait a minute, how do you know it can actually recognize pictures of people playing cricket versus baseball in general? Maybe it just learnt to recognize those 30. Maybe it's just cheating. That's called "overfitting". We'll be talking a lot about that during this course. But overfitting is where you don't learn to recognize pictures of say cricket versus baseball, but just these particular cricketers in these particular photos and these particular baseball players in these particular photos. We have to make sure that we don't overfit. The way to do that is using something called a validation set. A validation set is a set of images that your model does not get to look at. So these metrics (e.g. error_rate) get printed out automatically using the validation set - a set of images that our model never got to see.  When we created our data bunch, it automatically created a validation set for us. We'll learn lots of ways of creating and using validation sets, but because we're trying to bake in all of the best practices, we actually make it nearly impossible for you not to use a validation set. Because if you're not using a validation set, you don't know if you're overfitting. So we always print out the metrics on a validation, we've always hold it out, we always make sure that the model doesn't touch it. That's all done for you, and all built into this data bunch object.</p>
<h2><a id="user-content-fitting-your-model-4140" class="anchor" aria-hidden="true" href="#fitting-your-model-4140"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fitting your model [<a href="https://youtu.be/BWWm4AzsdLk?t=2500" rel="nofollow">41:40</a>]</h2>
<p>So now we have a ConvLearner, we can fit it. You can just use a method called <code>fit</code> but in practice, you should nearly always use a method called <code>fit_one_cycle</code>. In short, one cycle learning is <a href="https://arxiv.org/pdf/1803.09820.pdf" rel="nofollow">a paper</a> that was released in April and turned out to be dramatically better both more accurate and faster than any previous approach. Again, I don't want to teach you how to do 2017 deep learning. In 2018, the best way to fit models is to use something called one cycle.</p>
<p>For now, just know that this number, 4, basically decides how many times do we go through the entire dataset, how many times do we show the dataset to the model so that it can learn from it. Each time it sees a picture, it's going to get a little bit better. But it's going to take time and it means it could overfit. If it sees the same picture too many times, it will just learn to recognize that picture, not pets in general. We'll learn all about how to tune this number during the next couple of lessons but starting out with 4 is a pretty good start just to see how it goes and you can actually see after four epochs or four cycles, we got an error rate of 6%. And it took 1 minute and 56 seconds.</p>
<div class="highlight highlight-source-python"><pre>learn.fit_one_cycle(<span class="pl-c1">4</span>)</pre></div>
<pre><code>Total time: 01:10
epoch  train loss  valid loss  error_rate
1      1.175709    0.318438    0.099800    (00:18)
2      0.492309    0.229078    0.075183    (00:17)
3      0.336315    0.211106    0.067199    (00:17)
4      0.233666    0.191813    0.057219    (00:17)
</code></pre>
<p>So 94% of the time, we correctly picked the exact right one of those 37 dog and cat breeds which feels pretty good to me. But to get a sense of how good it is, maybe we should go back and look at the paper. Remember, I said the nice thing about using academic papers or Kaggle dataset is we can compare our solution to whatever the best people in Kaggle did or in the academics did. This particular dataset of pet breeds is from 2012 and if I scroll through the paper, you'll generally find in any academic paper there'll be a section called experiments about 2/3 of the way through. If you find a section on experiments, then you can find a section on accuracy and they've got lots of different models and their models. The models as you'll read about in the paper, it's really pet specific. They learn something about how pet heads look and how pet bodies look, and pet image in general look. And they combine them all together and once they use all of this complex code and math, they got an accuracy of 59%. So in 2012, this highly pet specific analysis got an accuracy of 59%. These were the top researchers from Oxford University. Today in 2018, with basically about three lines of code, we got 94% (i.e. 6% error). So that gives you a sense of how far we've come with deep learning, and particularly with PyTorch and fastai, how easy things are.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=2803" rel="nofollow">46:43</a>]
We just trained a model. We don't know exactly what that involved or how it happened but we do know that with 3 or 4 lines of code, we've built something which smashed the accuracy of the state-of-the-art of 2012. 6% error certainly sounds like pretty impressive for something that can recognize different dog breeds and cat breeds, but we don't really know why it work, but we will. That's okay.</p>
<h3><a id="user-content-the-number-one-regret-of-past-students" class="anchor" aria-hidden="true" href="#the-number-one-regret-of-past-students"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The number one regret of past students:</h3>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/102.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/102.png" alt="" style="max-width:100%;"></a></p>
<blockquote>
<h3><a id="user-content-so-please-run-the-code-really-run-the-code-4754" class="anchor" aria-hidden="true" href="#so-please-run-the-code-really-run-the-code-4754"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>So please run the code. Really run the code.</strong> [<a href="https://youtu.be/BWWm4AzsdLk?t=2874" rel="nofollow">47:54</a>]</h3>
</blockquote>
<p>Your most important skills to practice are learning and understanding what goes in and what comes out.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/103.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/103.png" alt="" style="max-width:100%;"></a></p>
<p>Fastai library is pretty new, but it's getting an extraordinary amount of traction. It's making a lot of things a lot easier, but it's also making new things possible. So really understanding the fastai software is something which is going to take you a long way. And the best way to really understand the fastai software well is by using the <a href="http://docs.fast.ai/" rel="nofollow">fastai documentation</a>.</p>
<h3><a id="user-content-keras-4925" class="anchor" aria-hidden="true" href="#keras-4925"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Keras[ <a href="https://youtu.be/BWWm4AzsdLk?t=2965" rel="nofollow">49:25</a>]</h3>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/105.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/105.png" alt="" style="max-width:100%;"></a></p>
<p>So how does it compare? There's only one major other piece of software like fastai that tries to make deep learning easy to use and that's Keras. Keras is a really terrific piece of software, we actually used it for the previous courses until we switch to fastai. It runs on top of Tensorflow. It was the gold standard for making deep learning easy to use before. But life is much easier with fastai. So if you look at the last year's course exercise which is getting dogs vs. cats, fastai lets you get much more accurate (less than half the error on a validation set), training time is less than half the time, lines of code is about 1/6. The lines of code are more important than you might realize because those 31 lines of Keras code involved you making a lot of decisions, setting lots of parameters, doing lots of configuration. So that's all stuff where you have to know how to set those things to get best practice results. Or else, those 5 lines of code, any time we know what to do for you, we do it for you. Anytime we can pick a good default, we pick it for you. So hopefully you will find this a really useful library, not just for learning deep learning but for taking it a very long way.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=3053" rel="nofollow">50:53</a>]</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/106.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/106.png" alt="" style="max-width:100%;"></a></p>
<p>How far can you take it? All of the research that we do at fastai uses the library and an example of the research we did which was recently featured in Wired describes a new breakthrough in a natural language processing which people are calling the ImageNet moment which is basically we broke a new state-of-the-art result in text classification which OpenAI then built on top of our paper with more computing, more data to do different tasks to take it even further. This is an example of something we've done in the last 6 months in conjunction with my colleague Sebastian Ruder - an example of something that's being built in the fastai library and you are going to learn how to use this brand new model in three lessons time. You're actually going to get this exact result from this exact paper yourself.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=3110" rel="nofollow">51:50</a>]
<a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/107.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/107.png" alt="" style="max-width:100%;"></a>
Another example, one of our alumni, Hamel Husain built a new system for natural language semantic code search, you can find it on Github where you can actually type in English sentences and find snippets of code that do the thing you asked for. Again, it's being built with the fastai library using the techniques you'll learn in the next seven weeks.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=3147" rel="nofollow">52:27</a>]</p>
<p>The best place to learn about these things and get involved in these things is on the forums where as well as categories for each part of the course and there is also a general category for deep learning where people talk about research papers applications.</p>
<p>Even though today, we are focusing on a small number of lines of code to a particular thing which is image classification and we are not learning much math or theory, over these seven weeks and then part two, we are going to go deeper and deeper.</p>
<h3><a id="user-content-where-can-that-take-you-5305" class="anchor" aria-hidden="true" href="#where-can-that-take-you-5305"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Where can that take you? [<a href="https://youtu.be/BWWm4AzsdLk?t=3185" rel="nofollow">53:05</a>]</h3>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/108.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/108.png" alt="" style="max-width:100%;"></a></p>
<p>This is Sarah Hooker. She did our first course a couple of years ago. She started learning to code two years before she took our course. She started a nonprofit called Delta Analytics, they helped build this amazing system where they attached old mobile phones to trees in Kanyan rain forests and used it to listen for chainsaw noises, and then they used deep learning to figure out when there was a chainsaw being used and then they had a system setup to alert rangers to go out and stop illegal deforestation in the rainforests. That was something she was doing while she was in the course as part of her class projects.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/109.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/109.png" alt="" style="max-width:100%;"></a>
She is now a Google Brain researcher, publishing some papers, and now she is going to Africa to set up a Google Brain's first deep learning research center in Africa. She worked her arse off. She really really invested in this course. Not just doing all of the assignments but also going out and reading Ian Goodfellow's book, and doing lots of other things. It really shows where somebody who has no computer science or math background at all can be now one of the world's top deep learning researchers and doing very valuable work.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=3289" rel="nofollow">54:49</a>]</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/110.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/110.png" alt="" style="max-width:100%;"></a></p>
<p>Another example from our most recent course, Christine Payne. She is now at OpenAI and you can find <a href="http://christinemcleavey.com/clara-a-neural-net-music-generator/" rel="nofollow">her post</a> and actually listen to her music samples of something she built to automatically create chamber music compositions.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/111.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/111.png" alt="" style="max-width:100%;"></a></p>
<p>She is a classical pianist. Now I will say she is not your average classical pianist. She's a classical pianist who also has a master's in medical research in Stanford, and studied neuroscience, and was a high-performance computing expert at DE Shaw, Co-Valedictorian at Princeton. Anyway. Very annoying person, good at everything she does. But I think it's really cool to see how a domain expert of playing piano can go through the fastai course and come out the other end as OpenAI fellow.</p>
<p>Interestingly, one of our other alumni of the course recently interviewed her for a blog post series he is doing on top AI researchers and she said one of the most important pieces of advice she got was from me and she said the advice was:</p>
<blockquote>
<h4><a id="user-content-pick-one-project-do-it-really-well-make-it-fantastic-5620" class="anchor" aria-hidden="true" href="#pick-one-project-do-it-really-well-make-it-fantastic-5620"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Pick one project. Do it really well. Make it fantastic. <a href="https://youtu.be/BWWm4AzsdLk?t=3380" rel="nofollow">56:20</a></h4>
</blockquote>
<p>We're going to be talking a lot about you doing projects and making them fantastic during this course.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=3396" rel="nofollow">56:36</a>]
Having said that, I don't really want you to go to AI or Google Brain. What I really want you to do is to go back to your workplace or your passion project and apply these skills there.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/112.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/112.png" alt="" style="max-width:100%;"></a>
MIT released a deep learning course and they highlighted in their announcement this medical imaging example. One of our students Alex who is a radiologist said you guys just showed a model overfitting. I can tell because I am a radiologist and this is not what this would look like on a chest film. This is what it should look like and as a deep learning practitioner, this is how I know this is what happened in your model. So Alex is combining his knowledge of radiology and his knowledge of deep learning to assess MIT's model from just two images very accurately. So this is actually what I want most of you to be doing is to take your domain expertise and combine it with the deep learning practical aspects you'll learn in this course and bring them together like Alex is doing here. So a lot of radiologists have actually gone through this course now and have built journal clubs and American Council of Radiology practice groups. There's a data science institute at the ACR now and Alex is one of the people who is providing a lot of leadership in this area. And I would love you to do the same kind of thing that Alex is doing which is to really bring deep learning leadership into your industry and to your social impact project, whatever it is that you are trying to do.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=3502" rel="nofollow">58:22</a>]</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/113.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/113.png" alt="" style="max-width:100%;"></a></p>
<p>Another great example. This is Melissa Fabros who is a English literature PhD who studied gendered language in English literature or something and actually Rachel at the previous job taught her to code. Then she came to the fastai course. She helped Kiva, a micro lending a social impact organization, to build a system that can recognize faces. Why is that necessary? We're going to be talking a lot about this but because most AI researchers are white men, most computer vision software can only recognize white male faces effectively. In fast, I think it was IBM system was like 99.8% accurate on common white face men versus 65% accurate on dark skinned women. So it's like 30 or 40 times worse for black women versus white men. This is really important because for Kiva, black women perhaps are the most common user base for their micro lending platform. So Melissa after taking our course, again working her arse off, and being super intense in her study and her work won this $1,000,000 AI challenge for her work for Kiva.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=3593" rel="nofollow">59:53</a>]</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/114.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/114.png" alt="" style="max-width:100%;"></a></p>
<p>Karthik did our course and realized that the thing he wanted to do wasn't at his company. It was something else which is to help blind people to understand the world around them. So he started a new startup called envision. You can download the app and point your phone to things and it will tell you what it sees. I actually talked to a blind lady about these kinds of apps the other day and she confirmed to me this is a super useful thing for visually disabled users.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=3624" rel="nofollow">1:00:24</a>]
<a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/115.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/115.png" alt="" style="max-width:100%;"></a></p>
<p>The level that you can get to, with the content that you're going to get over these seven weeks and with this software can get you right to the cutting edge in areas you might find surprising. I helped a team of some of our students and some collaborators on actually breaking the world record for how quickly you can train ImageNet. We used standard AWS cloud infrastructure, cost of $40 of compute to train this model using fastai library, the technique you learn in this course. So it can really take you a long way. So don't be put off by this what might seem pretty simple at first. We are going deeper and deeper.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=3677" rel="nofollow">1:01:17</a>]
<a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/116.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/116.png" alt="" style="max-width:100%;"></a></p>
<p>You can also use it for other kinds of passion project. Helena Sarin - you should definitely check out her Twitter account <a href="https://twitter.com/glagolista" rel="nofollow">@glagolista</a>. This art is basically a new style of art that she's developed which combines her painting and drawing with generative adversarial models to create these extraordinary results. I think this is super cool. She is not a professional artists, she is a professional software developer but she keeps on producing these beautiful results. When she started, her art had not really been shown or discussed anywhere, now there's recently been some quite high profile article describing how she is creating a new form of art.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/117.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/117.png" alt="" style="max-width:100%;"></a></p>
<p>Equally important, Brad Kenstler who figured out how to make a picture of Kanye out of pictures of Patrick Stewart's head. Also something you will learn to do if you wish to. This particular type of what's called "style transfer" - it's a really interesting tweak that allowed him to do something that hadn't quite been done before. This particular picture helped him to get a job as a deep learning specialist at AWS.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=3761" rel="nofollow">1:02:41</a>]</p>
<p>Another alumni actually worked at Splunk as a software engineer and he designed an algorithm which basically turned Splunk to be fantastically good at identifying fraud and we'll talk more about it shortly.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/118.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/118.png" alt="" style="max-width:100%;"></a></p>
<p>If you've seen Silicon Valley, the HBO series, the hotdog Not Hotdog app - that's actually a real app you can download and it was built by Tim Anglade as a fastai student project. So there's a lot of cool stuff that you can do. It was Emmy nominated. We only have one Emmy nominated fastai alumni at this stage, so please help change that.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/119.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/119.png" alt="" style="max-width:100%;"></a>
[<a href="https://youtu.be/BWWm4AzsdLk?t=3810" rel="nofollow">1:03:30</a>]</p>
<p>The other thing, the forum thread can turn into these really cool things. So Francisco was a really boring McKinsey consultant like me. So Francisco and I both have this shameful past that we were McKinsey consultants, but we left and we're okay now. He started this thread saying like this stuff we've just been learning about building NLP in different languages, let's try and do lots of different languages, and he started this thing called the language model zoo and out of that, there's now been an academic competition was won in Polish that led to an academic paper, Thai state of the art, German state of the art, basically as students have been coming up with new state of the art results across lots of different languages and this all is entirely done by students working together through the forum.</p>
<p>So please get on the forum. But don't be intimidated because everybody you see on the forum, the vast majority of posting post all the darn time. They've been doing this a lot and they do it a lot of the time. So at first, it can feel intimidating because it can feel like you're the only new person there. But you're not. You're all new people, so when you just get out there and say like "okay all you people getting these state of the art results in German language modeling, I can't start my server, I try to click the notebook and I get an error, what do I do?" People will help you. Just make sure you provide all the information (<a href="https://forums.fast.ai/t/how-to-ask-for-help/10421" rel="nofollow">how to ask for help</a>).</p>
<p>Or if you've got something to add! If people are talking about crop yield analysis and you're a farmer and you think oh I've got something to add, please mention it even if you are not sure it's exactly relevant. It's fine. Just get involved. Because remember, everybody else in the forum started out also intimidated. We all start out not knowing things. So just get out there and try it!</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=3959" rel="nofollow">1:05:59</a>]
<strong>Question</strong>: Why are we using ResNet as opposed to Inception?</p>
<p>There are lots of architectures to choose from and it would be fair to say there isn't one best one but if you look at things like the Stanford DAWNBench benchmark of image classification, you'll see in first place, second place,  third place, and fourth place all use ResNet. ResNet is good enough, so it's fine.
<a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/120.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/120.png" alt="" style="max-width:100%;"></a></p>
<p>The main reason you might want a different architecture is if you want to do edge computing, so if you want to create a model that's going to sit on somebody's mobile phone. Having said that, even there, most of the time, I reckon the best way to get a model onto somebody's mobile phone is to run it on your server and then have your mobile phone app talk to it. It really makes life a lot easier and you get a lot more flexibility. But if you really do need to run something on a low powered device, then there are special architectures for that. So the particular question was about Inception. That's a particular another architecture which tends to be pretty memory intensive but it's okay. It's not terribly resilient. One of the things we try to show you is stuff which just tends to always work even if you don't quite tune everything perfectly. So ResNet tends to work pretty well across a wide range of different kind of details around choices that you might make. So I think it's pretty good.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=4078" rel="nofollow">1:07:58</a>]</p>
<p>We've got this trained model and what's actually happened as we'll learn is it's basically creating a set of weights. If you've ever done anything like a linear regression or logistic regression, you'll be familiar with coefficients. We basically found some coefficients and parameters that work pretty well and it took us a minute and 56 seconds. So if we want to start doing some more playing around and come back later, we probably should save those weights. You can just go <code>learn.save</code> and give it a name. It's going to put it in a model subdirectory in the same place the data came from, so if you save different models or different data bunches from different datasets, they'll all be kept separate. So don't worry about it.</p>
<div class="highlight highlight-source-python"><pre>learn.save(<span class="pl-s"><span class="pl-pds">'</span>stage-1<span class="pl-pds">'</span></span>)</pre></div>
<h2><a id="user-content-results-10854" class="anchor" aria-hidden="true" href="#results-10854"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results [<a href="https://youtu.be/BWWm4AzsdLk?t=4134" rel="nofollow">1:08:54</a>]</h2>
<p>To see what comes out, we could use this class for class interpretation. We are going to use this factory method from learner, so we pass in a learn object. Remember a learn object knows two things:</p>
<ol>
<li>What's your data</li>
<li>What is your model. Now it's not just an architecture, it's actually a trained model</li>
</ol>
<p>That's all the information we need to interpret that model.</p>
<div class="highlight highlight-source-python"><pre>interp <span class="pl-k">=</span> ClassificationInterpretation.from_learner(learn)</pre></div>
<p>One of the things, perhaps the most useful things to do is called plot_top_losses. We are going to be learning a lot about this idea of loss functions shortly but in short, a loss function is something that tells you how good was your prediction. Specifically that means if you predicted one class of cat with great confidence, but actually you were wrong, then that's going to have a high loss because you were very confident about the wrong answer. So that's what it basically means to have high loss. By plotting the top losses, we are going to find out what were the things that we were the most wrong on, or the most confident about what we got wrong.</p>
<div class="highlight highlight-source-python"><pre>interp.plot_top_losses(<span class="pl-c1">9</span>, <span class="pl-v">figsize</span><span class="pl-k">=</span>(<span class="pl-c1">15</span>,<span class="pl-c1">11</span>))
</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/9.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/9.png" alt="" style="max-width:100%;"></a></p>
<p>It prints out four things. What do they mean? Perhaps we should look at the document.</p>
<p>We have already seen <code>help</code>, and <code>help</code> just prints out a quick little summary. But if you want to really see how to do something use <code>doc</code>.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/121.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/121.png" alt="" style="max-width:100%;"></a></p>
<p><code>doc</code> tells you the same information as <code>help</code> but it has this very important thing which is <code>Show in docs</code>. When you click on it, it pops up the documentation for that method or class or function or whatever:</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/122.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/122.png" alt="" style="max-width:100%;"></a></p>
<p>It starts out by showing us the same information about what are the parameters it takes a long with the doc string. But then tells you more information:</p>
<blockquote>
<p>The title of each image shows: prediction, actual, loss, probability of actual class.</p>
</blockquote>
<p>The documentation always has working code. This is your friend when you're trying to figure out how to use these things. The other thing I'll mention is if you're somewhat experienced Python programmer, you'll find the source code of fastai really easy to read. We are trying to write everything in just a small number (much less than half a screen) of code. If you click on <code>[source]</code> you can jump straight to the source code.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/123.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/123.png" alt="" style="max-width:100%;"></a></p>
<p>Here is plot_top_loss, and this is also a great way to find out how to use the fastai library. Because nearly every line of code here,  is calling stuff in the fastai library. So don't be afraid to look at the source code.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=4368" rel="nofollow">1:12:48</a>]</p>
<p>So that's how we can look at top losses and these are perhaps the most important image classification interpretation tools that we have because it lets us see what we are getting wrong. In this case, if you are a dog and cat expert, you'll realize that the things that's getting wrong are breeds that are actually very difficult to tell apart and you'd be able to look at these and say "oh I can see why they've got this one wrong". So this is a really useful tool.</p>
<h3><a id="user-content-confusion-matrix-11321" class="anchor" aria-hidden="true" href="#confusion-matrix-11321"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Confusion matrix <a href="https://youtu.be/BWWm4AzsdLk?t=4401" rel="nofollow">1:13:21</a></h3>
<p>Another useful tool, kind of, is to use something called a confusion matrix which basically shows you for every actual type of dog or cat, how many times was it predicted to be that dog or cat. But unfortunately, in this case, because it's so accurate, this diagonal basically says how it's pretty much right all the time.
<a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/10.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/10.png" alt="" style="max-width:100%;"></a></p>
<p>And you can see there is slightly darker ones like a five here, it's really hard to read exactly what their combination is. So what I suggest you use is instead of, if you've got lots of classes, don't use confusion matrix, but this is my favorite named function in fastai and I'm very proud of this - you can call "most confused".</p>
<h3><a id="user-content-most-confused-11352" class="anchor" aria-hidden="true" href="#most-confused-11352"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Most confused [<a href="https://youtu.be/BWWm4AzsdLk?t=4432" rel="nofollow">1:13:52</a>]</h3>
<div class="highlight highlight-source-python"><pre>interp.most_confused(<span class="pl-v">min_val</span><span class="pl-k">=</span><span class="pl-c1">2</span>)</pre></div>
<pre><code>[('american_pit_bull_terrier', 'staffordshire_bull_terrier', 5),
 ('Birman', 'Ragdoll', 5),
 ('english_setter', 'english_cocker_spaniel', 4),
 ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 4),
 ('boxer', 'american_bulldog', 4),
 ('Ragdoll', 'Birman', 3),
 ('miniature_pinscher', 'chihuahua', 3),
 ('Siamese', 'Birman', 3)]
</code></pre>
<p><code>most_confused</code> will simply grab out of the confusion matrix the particular combinations of predicted and actual that got wrong the most often. So this case, <code>('american_pit_bull_terrier', 'staffordshire_bull_terrier', 7)</code>:</p>
<ul>
<li>Actual <code>'american_pit_bull_terrier'</code></li>
<li>Prediction <code>'staffordshire_bull_terrier'</code></li>
<li>This particular combination happened 7 times.</li>
</ul>
<p>So this is a very useful thing because you can look and say "with my domain expertise, does it make sense?"</p>
<h3><a id="user-content-unfreezing-fine-tuning-and-learning-rates-11438" class="anchor" aria-hidden="true" href="#unfreezing-fine-tuning-and-learning-rates-11438"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Unfreezing, fine-tuning, and learning rates [<a href="https://youtu.be/BWWm4AzsdLk?t=4478" rel="nofollow">1:14:38</a>]</h3>
<p>Let's make our model better. How? We can make it better by using fine-tuning. So far we fitted 4 epochs and it ran pretty quickly. The reason it ran pretty quickly is that there was a little trick we used. These convolutional networks, they have many layers. We'll learn a lot about exactly what layers are, but for now, just know it goes through a lot of computations. What we did was we added a few extra layers to the end and we only trained those. We basically left most of the model exactly as it was, so that's really fast. If we are trying to build a model at something that's similar to the original pre-trained model (in this case, similar to the ImageNet data), that works pretty well.</p>
<p>But what we really want to do is to go back and train the whole model. This is why we pretty much always use this two stage process. By default, when we call <code>fit</code> or <code>fit_one_cycle</code> on a ConvLearner, it'll just fine-tune these few extra layers added to the end and it will run very fast. It will basically never overfit but to really get it good, you have to call <code>unfreeze</code>. <code>unfreeze</code> is the thing that says please train the whole model. Then I can call fit_one_cycle again.</p>
<div class="highlight highlight-source-python"><pre>learn.unfreeze()
learn.fit_one_cycle(<span class="pl-c1">1</span>)</pre></div>
<pre><code>
Total time: 00:20
epoch  train_loss  valid_loss  error_rate
1      1.045145    0.505527    0.159681    (00:20)
</code></pre>
<p>Uh-oh. The error got much worse. Why? In order to understand why, we are actually going to have to learn more about exactly what's going on behind the scenes. So let's start out by trying to get an intuitive understanding of what's going on behind the scenes. We are going to do it by looking at pictures.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=4588" rel="nofollow">1:16:28</a>]
<a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/100.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/100.png" alt="" style="max-width:100%;"></a></p>
<p>These pictures come from <a href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" rel="nofollow">a fantastic paper</a> by Matt Zeiler who nowadays is a CEO of Clarify which is a very successful computer vision startup and his supervisor for his PhD Rob Fergus. They wrote a paper showing how you can visualize the layers of a convolutional neural network. A convolutional neural network, which we will learn mathematically about what the layers are shortly, but the basic idea is that your red, green, and blue pixel values that are numbers from nought to 255 go into the simple computation (i.e. the first layer) and something comes out of that, and then the result of that goes into a second layer, and the result of that goes into the third layer and so forth. There can be up to a thousand layers of neural network. ResNet34 has 34 layers, and ResNet50 has 50 layers, but let's look at layer one. There's this very simple computation which is a convolution if you know what they are. What comes out of this first layer? Well, we can actually visualize these specific coefficients, the specific parameters by drawing them as a picture. There's actually a few dozen of them in the first layer, so we don't draw all of them. Let's just look at 9 at random.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=4665" rel="nofollow">1:17:45</a>]</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/124.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/124.png" alt="" style="max-width:100%;"></a></p>
<p>Here are nine examples of the actual coefficients from the first layer. So these operate on groups of pixels that are next to each other. So this first one basically finds groups of pixels that have a little diagonal line, the second one finds diagonal line in the other direction, the third one finds gradients that go from yellow to blue, and so forth. They are very simple little filters. That's layer one of ImageNet pre-trained convolutional neural net.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/125.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/125.png" alt="" style="max-width:100%;"></a></p>
<p>Layer 2 takes the results of those filters and does a second layer of computation. The bottom right are nine examples of a way of visualizing one of the second layer features. AS you can see, it basically learned to create something that looks for top left corners. There are ones that learned to find right-hand curves, and little circles, etc. In layer one, we have things that can find just one line, and in layer 2, we can find things that have two lines joined up or one line repeated. If you then look over to the right, these nine show you nine examples of actual bits of the actual photos that activated this filter a lot. So in other words, the filter on the bottom right was good at finding these window corners etc.</p>
<p>So this is the kind of stuff you've got to get a really good intuitive understanding for. The start of my neural net is going to find very simple gradients and lines, the second layer can find very simple shapes, the third layer can find  combination of those.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/126.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/126.png" alt="" style="max-width:100%;"></a></p>
<p>Now we can find repeating pattern of two dimensional objects or we can find things that joins together, or bits of text (although sometimes windows) - so it seems to find repeated horizontal patterns. There are also ones that seem to find edges of fluffy or flowery things or geometric patterns. So layer 3 was able to take all the stuff from layer 2 and combine them together.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/127.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/127.png" alt="" style="max-width:100%;"></a></p>
<p>Layer 4 can take all the stuff from layer 3 and combine them together. By layer 4, we got something that can find dog faces or bird legs.</p>
<p>By layer 5, we've got something that can find the eyeballs of bird and lizards, or faces of particular breeds of dogs and so forth. So you can see how by the time you get to layer 34, you can find specific dog breeds and cat breeds. This is kind of how it works.</p>
<p>So when we first trained (i.e. fine-tuned) the pre-trained model, we kept all of these layers that you've seen so far and we just trained a few more layers on top of all of those sophisticated features that are already being created. So now we are going back and saying "let's change all of these". We will start with where they are, but let's see if we can make them better.</p>
<p>Now, it seems very unlikely that we can make layer 1 features better. It's very unlikely that the definition of a diagonal line is going to be different when we look at dog and cat breeds versus the ImageNet data that this was originally trained on. So we don't really want to change the layer 1 very much if at all. Or else, the last layers, like types of dog face seems very likely that we do want to change that. So you want this intuition, this understanding that the different layers of a neural network represents different level of semantic complexity.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=4926" rel="nofollow">1:22:06</a>]</p>
<p>This is why our attempt to fine-tune this model didn't work because by default, it trains all the layers at the same speed which is to say it will update those things representing diagonal lines and gradients just as much as it tries to update the things that represent the exact specifics of what an eyeball looks like, so we have to change that.</p>
<p>To change it, we first of all need to go back to where we were before. We just broke this model, much worse than it started out. So if we just go:</p>
<div class="highlight highlight-source-python"><pre>learn.load(<span class="pl-s"><span class="pl-pds">'</span>stage-1<span class="pl-pds">'</span></span>)</pre></div>
<p>This brings back the model that we saved earlier. So let's load that back up and now our models back to where it was before we killed it.</p>
<h3><a id="user-content-learning-rate-finder-12258" class="anchor" aria-hidden="true" href="#learning-rate-finder-12258"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Learning rate finder [<a href="https://youtu.be/BWWm4AzsdLk?t=4978" rel="nofollow">1:22:58</a>]</h3>
<p>Let's run learning rate finder. We are learning about what that is next week, but for now, just know this is the thing that figures out what is the fastest I can train this neural network at without making it zip off the rails and get blown apart.</p>
<div class="highlight highlight-source-python"><pre>learn.lr_find()
learn.recorder.plot()</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/11.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/11.png" alt="" style="max-width:100%;"></a></p>
<p>This will plot the result of our LR finder and what this basically shows you is this key parameter called a learning rate. The learning rate basically says how quickly am I updating the parameters in my model. The x-axis one here shows me what happens as I increase the learning rate. The y axis show what the loss is. So you can see, once the learning rate gets passed 10^-4, my loss gets worse. It actually so happens, in fact I can check this if I press <kbd>shift</kbd>+<kbd>tab</kbd> here, my learning defaults to 0.003. So you can see why our loss got worse. Because we are trying to fine-tune things now, we can't use such a high learning rate. So based on the learning rate finder, I tried to pick something well before it started getting worse. So I decided to pick <code>1e-6</code>. But there's no point training all the layers at that rate, because we know that the later layers worked just fine before when we were training much more quickly. So what we can actually do is we can pass a range of learning rates to <code>learn.fit_one_cycle</code>. And we do it like this:</p>
<div class="highlight highlight-source-python"><pre>learn.unfreeze()
learn.fit_one_cycle(<span class="pl-c1">2</span>, <span class="pl-v">max_lr</span><span class="pl-k">=</span><span class="pl-c1">slice</span>(<span class="pl-c1">1e-6</span>,<span class="pl-c1">1e-4</span>))</pre></div>
<pre><code>Total time: 00:41
epoch  train_loss  valid_loss  error_rate
1      0.226494    0.173675    0.057219    (00:20)
2      0.197376    0.170252    0.053227    (00:20)
</code></pre>
<p>You use this keyword in Python called <code>slice</code> and that can take a start value and a stop value and basically what this says is train the very first layers at a learning rate of 1e-6, and the very last layers at a rate of 1e-4, and distribute all the other layers across that (i.e. between those two values equally).</p>
<h3><a id="user-content-how-to-pick-learning-rates-after-unfreezing-12523" class="anchor" aria-hidden="true" href="#how-to-pick-learning-rates-after-unfreezing-12523"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How to pick learning rates after unfreezing [<a href="https://youtu.be/BWWm4AzsdLk?t=5123" rel="nofollow">1:25:23</a>]</h3>
<p>A good rule of thumb is after you unfreeze (i.e. train the whole thing), pass a max learning rate parameter, pass it a slice, make the second part of that slice about 10 times smaller than your first stage. Our first stage defaulted to about 1e-3 so it's about 1e-4. And the first part of the slice should be a value from your learning rate finder which is well before things started getting worse. So you can see things are starting to get worse maybe about here:</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/128.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/128.png" alt="" style="max-width:100%;"></a></p>
<p>So I picked something that's at least 10 times smaller than that.</p>
<p>If I do that, then the error rate gets a bit better. So I would perhaps say for most people most of the time, these two stages are enough to get pretty much a world-class model. You won't win a Kaggle competition, particularly because now a lot of fastai alumni are competing on Kaggle and this is the first thing that they do. But in practice, you'll get something that's about as good in practice as the vast majority of practitioners can do.</p>
<h2><a id="user-content-resnet50-12655" class="anchor" aria-hidden="true" href="#resnet50-12655"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>ResNet50 [<a href="https://youtu.be/BWWm4AzsdLk?t=5215" rel="nofollow">1:26:55</a>]</h2>
<p>We can improve it by using more layers and we will do this next week but by basically doing a ResNet50 instead of ResNet34. And you can try running this during the week if you want to. You'll see it's exactly the same as before, but I'm using ResNet50.</p>
<div class="highlight highlight-source-python"><pre>data <span class="pl-k">=</span> ImageDataBunch.from_name_re(path_img, fnames, pat, <span class="pl-v">ds_tfms</span><span class="pl-k">=</span>get_transforms(), <span class="pl-v">size</span><span class="pl-k">=</span><span class="pl-c1">320</span>, <span class="pl-v">bs</span><span class="pl-k">=</span>bs<span class="pl-k">//</span><span class="pl-c1">2</span>)
data.normalize(imagenet_stats)</pre></div>
<div class="highlight highlight-source-python"><pre>learn <span class="pl-k">=</span> ConvLearner(data, models.resnet50, <span class="pl-v">metrics</span><span class="pl-k">=</span>error_rate)</pre></div>
<p>What you'll find is it's very likely if you try to do this, you will get an error and the error will be your GPU has ran out of memory. The reason for that is that ResNet50 is bigger than ResNet34, and therefore, it has more parameters and use more of your graphics card memory, just totally separate to your normal computer RAM, this is GPU RAM. If you're using the default Salamander,  AWS, then you'll be having a 16G of GPU memory. The card I use most of the time has 11G GPU memory, the cheaper ones have 8G. That's kind of the main range you tend to get. If yours have less than 8G of GPU memory, it's going to be frustrating for you.</p>
<p>It's very likely that if you try to run this, you'll get an out of memory error and that's because it's just trying to do too much - too many parameter updates for the amount of RAM you have. That's easily fixed. <code>ImageDataBunch</code> constructor has a parameter at the end <code>bs</code> - a batch size. This basically says how many images do you train at one time. If you run out of memory, just make it smaller.</p>
<p>It's fine to use a smaller bath size. It might take a little bit longer. That's all. So that's just one number you'll need to try during the week.</p>
<div class="highlight highlight-source-python"><pre>learn.fit_one_cycle(<span class="pl-c1">8</span>, <span class="pl-v">max_lr</span><span class="pl-k">=</span><span class="pl-c1">slice</span>(<span class="pl-c1">1e-3</span>))</pre></div>
<pre><code>Total time: 07:08
epoch  train_loss  valid_loss  error_rate
1      0.926640    0.320040    0.076555    (00:52)
2      0.394781    0.205191    0.063568    (00:52)
3      0.307754    0.203281    0.069036    (00:53)
4      0.244182    0.160488    0.054682    (00:53)
5      0.185785    0.153520    0.049214    (00:53)
6      0.157732    0.149660    0.047163    (00:53)
7      0.107212    0.136898    0.043062    (00:53)
8      0.097324    0.136638    0.042379    (00:54)
</code></pre>
<p>Again, we fit it for a while and we get down to 4.2% error rage. So this is pretty extraordinary. I was pretty surprised because when we first did in the first course, this cats vs. dogs, we were getting somewhere around 3% error for something where you've got a 50% chance of being right and the two things look totally different. So the fact that we can get 4.2% error for such a fine grain thing, it's quite extraordinary.</p>
<h3><a id="user-content-interpreting-the-results-again-12941" class="anchor" aria-hidden="true" href="#interpreting-the-results-again-12941"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Interpreting the results again <a href="https://youtu.be/BWWm4AzsdLk?t=5381" rel="nofollow">1:29:41</a></h3>
<div class="highlight highlight-source-python"><pre>interp <span class="pl-k">=</span> ClassificationInterpretation.from_learner(learn)
interp.most_confused(<span class="pl-v">min_val</span><span class="pl-k">=</span><span class="pl-c1">2</span>)</pre></div>
<pre><code>[('Ragdoll', 'Birman', 7),
 ('american_pit_bull_terrier', 'staffordshire_bull_terrier', 6),
 ('Egyptian_Mau', 'Bengal', 6),
 ('Maine_Coon', 'Bengal', 3),
 ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 3)]
</code></pre>
<p>You can call the most_confused here and you can see the kinds of things that it's getting wrong. Depending on when you run it, you're going to get slightly different numbers, but you'll get roughly the same kind of things. So quite often, I find the Ragdoll and Birman are things that it gets confused. I actually have never heard of either of those things, so I actually looked them up and found a page on the cat site called "Is this a Birman or Ragdoll kitten?" and there was a long thread of cat experts arguing intensely about which it is. So I feel fine that my computer had problems.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/129.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/129.png" alt="" style="max-width:100%;"></a></p>
<p>I found something similar, I think it was this pitbull versus staffordshire bull terrier, apparently the main difference is the particular kennel club guidelines as to how they are assessed. But some people thing that one of them might have a slightly redder nose. So this is the kind of stuff where actually even if you're not a domain expert, it helps you become one. Because I now know more about which kinds of pet breeds are hard to identify than I used to. So model interpretation works both ways.</p>
<h2><a id="user-content-homework-13058" class="anchor" aria-hidden="true" href="#homework-13058"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Homework [<a href="https://youtu.be/BWWm4AzsdLk?t=5458" rel="nofollow">1:30:58</a>]</h2>
<p>So what I want you to do this week is to run this notebook, make sure you can get through it, but then I really want you to do is to get your own image dataset and actually Francisco is putting together a guide that will show you how to download data from Google Images so you can create your own dataset to play with. But before I do, I want to show you how to create labels in lots of different ways because your dataset where you get it from won't necessarily be that kind of regex based approach. It could be in lots of different formats. So to show you how to do this, I'm going to use the MNIST sample. MNIST is a picture of hand drawn numbers - just because I want to show you different ways of creating these datasets.</p>
<div class="highlight highlight-source-python"><pre>path <span class="pl-k">=</span> untar_data(URLs.<span class="pl-c1">MNIST_SAMPLE</span>); path</pre></div>
<div class="highlight highlight-source-python"><pre>path.ls()</pre></div>
<pre><code>['train', 'valid', 'labels.csv', 'models']
</code></pre>
<p>You see there are a training set and the validation set already. So basically the people that put together this dataset have already decided what they want you to use as a validation set.</p>
<h3><a id="user-content-scenario-1-labels-are-folder-names" class="anchor" aria-hidden="true" href="#scenario-1-labels-are-folder-names"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scenario 1: Labels are folder names</h3>
<div class="highlight highlight-source-python"><pre>(path<span class="pl-k">/</span><span class="pl-s"><span class="pl-pds">'</span>train<span class="pl-pds">'</span></span>).ls()</pre></div>
<pre><code>['3', '7']
</code></pre>
<p>There are a folder called 3 and a folder called 7. Now this is really common way to give people labels. Basically it says everything that's a three, I put in a folder called three. Everything that's a seven, I'll put in a folder called seven. This is often called an "ImageNet style dataset" because this is how ImageNet is distributed. So if you have something in this format where the labels are just whatever the folders are called, you can say <code>from_folder</code>.</p>
<div class="highlight highlight-source-python"><pre>tfms <span class="pl-k">=</span> get_transforms(<span class="pl-v">do_flip</span><span class="pl-k">=</span><span class="pl-c1">False</span>)
data <span class="pl-k">=</span> ImageDataBunch.from_folder(path, <span class="pl-v">ds_tfms</span><span class="pl-k">=</span>tfms, <span class="pl-v">size</span><span class="pl-k">=</span><span class="pl-c1">26</span>)</pre></div>
<p>This will create an ImageDataBunch for you and as you can see it created the labels:</p>
<div class="highlight highlight-source-python"><pre>data.show_batch(<span class="pl-v">rows</span><span class="pl-k">=</span><span class="pl-c1">3</span>, <span class="pl-v">figsize</span><span class="pl-k">=</span>(<span class="pl-c1">5</span>,<span class="pl-c1">5</span>))</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/12.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/12.png" alt="" style="max-width:100%;"></a></p>
<h3><a id="user-content-scenario-2-csv-file-13317" class="anchor" aria-hidden="true" href="#scenario-2-csv-file-13317"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scenario 2: CSV file [<a href="https://youtu.be/BWWm4AzsdLk?t=5597" rel="nofollow">1:33:17</a>]</h3>
<p>Another possibility, and for this MNIST sample, I've got both, it might come with a CSV file that would look something like this.</p>
<div class="highlight highlight-source-python"><pre>df <span class="pl-k">=</span> pd.read_csv(path<span class="pl-k">/</span><span class="pl-s"><span class="pl-pds">'</span>labels.csv<span class="pl-pds">'</span></span>)
df.head()</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/130.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/130.png" alt="" style="max-width:100%;"></a></p>
<p>For each file name, what's its label. In this case, labels are not three or seven, they are 0 or 1 which basically is it a 7 or not. So that's another possibility. If this is how your labels are, you can use <code>from_csv</code>:</p>
<div class="highlight highlight-source-python"><pre>data <span class="pl-k">=</span> ImageDataBunch.from_csv(path, <span class="pl-v">ds_tfms</span><span class="pl-k">=</span>tfms, <span class="pl-v">size</span><span class="pl-k">=</span><span class="pl-c1">28</span>)</pre></div>
<p>And if it is called <code>labels.csv</code>, you don't even have to pass in a file name. If it's called something else, then you can pass in the <code>csv_labels</code></p>
<div class="highlight highlight-source-python"><pre>data.show_batch(<span class="pl-v">rows</span><span class="pl-k">=</span><span class="pl-c1">3</span>, <span class="pl-v">figsize</span><span class="pl-k">=</span>(<span class="pl-c1">5</span>,<span class="pl-c1">5</span>))
data.classes</pre></div>
<pre><code>[0, 1]
</code></pre>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/13.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/13.png" alt="" style="max-width:100%;"></a></p>
<h3><a id="user-content-scenario-3-using-regular-expression" class="anchor" aria-hidden="true" href="#scenario-3-using-regular-expression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scenario 3: Using regular expression</h3>
<div class="highlight highlight-source-python"><pre>fn_paths <span class="pl-k">=</span> [path<span class="pl-k">/</span>name <span class="pl-k">for</span> name <span class="pl-k">in</span> df[<span class="pl-s"><span class="pl-pds">'</span>name<span class="pl-pds">'</span></span>]]; fn_paths[:<span class="pl-c1">2</span>]</pre></div>
<pre><code>[PosixPath('/home/jhoward/.fastai/data/mnist_sample/train/3/7463.png'),
 PosixPath('/home/jhoward/.fastai/data/mnist_sample/train/3/21102.png')]
</code></pre>
<p>This is the same thing, these are the folders. But I could actually grab the label by using a regular expression. We've already seen this approach:</p>
<div class="highlight highlight-source-python"><pre>pat <span class="pl-k">=</span> <span class="pl-sr"><span class="pl-k">r</span><span class="pl-pds">"</span>/<span class="pl-c1">(</span><span class="pl-c1">\d</span><span class="pl-c1">)</span>/<span class="pl-c1">\d</span><span class="pl-k">+</span><span class="pl-cce">\.</span>png<span class="pl-c1">$</span><span class="pl-pds">"</span></span>
data <span class="pl-k">=</span> ImageDataBunch.from_name_re(path, fn_paths, <span class="pl-v">pat</span><span class="pl-k">=</span>pat, <span class="pl-v">ds_tfms</span><span class="pl-k">=</span>tfms, <span class="pl-v">size</span><span class="pl-k">=</span><span class="pl-c1">24</span>)
data.classes</pre></div>
<pre><code>['3', '7']
</code></pre>
<h3><a id="user-content-scenario-4-something-more-complex-13421" class="anchor" aria-hidden="true" href="#scenario-4-something-more-complex-13421"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scenario 4: Something more complex [<a href="https://youtu.be/BWWm4AzsdLk?t=5661" rel="nofollow">1:34:21</a>]</h3>
<p>You can create an arbitrary function that extracts a label from the file name or path. In that case, you would say <code>from_name_func</code>:</p>
<div class="highlight highlight-source-python"><pre>data <span class="pl-k">=</span> ImageDataBunch.from_name_func(path, fn_paths, <span class="pl-v">ds_tfms</span><span class="pl-k">=</span>tfms, <span class="pl-v">size</span><span class="pl-k">=</span><span class="pl-c1">24</span>,
        <span class="pl-v">label_func</span> <span class="pl-k">=</span> <span class="pl-k">lambda</span> <span class="pl-smi">x</span>: <span class="pl-s"><span class="pl-pds">'</span>3<span class="pl-pds">'</span></span> <span class="pl-k">if</span> <span class="pl-s"><span class="pl-pds">'</span>/3/<span class="pl-pds">'</span></span> <span class="pl-k">in</span> <span class="pl-c1">str</span>(x) <span class="pl-k">else</span> <span class="pl-s"><span class="pl-pds">'</span>7<span class="pl-pds">'</span></span>)
data.classes</pre></div>
<h3><a id="user-content-scenario-5-you-need-something-even-more-flexible" class="anchor" aria-hidden="true" href="#scenario-5-you-need-something-even-more-flexible"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scenario 5: You need something even more flexible</h3>
<p>If you need something even more flexible than that, you're going to write some code to create an array of labels. So in that case, you can just use <code>from_lists</code> and pass in the array.</p>
<div class="highlight highlight-source-python"><pre>labels <span class="pl-k">=</span> [(<span class="pl-s"><span class="pl-pds">'</span>3<span class="pl-pds">'</span></span> <span class="pl-k">if</span> <span class="pl-s"><span class="pl-pds">'</span>/3/<span class="pl-pds">'</span></span> <span class="pl-k">in</span> <span class="pl-c1">str</span>(x) <span class="pl-k">else</span> <span class="pl-s"><span class="pl-pds">'</span>7<span class="pl-pds">'</span></span>) <span class="pl-k">for</span> x <span class="pl-k">in</span> fn_paths]
labels[:<span class="pl-c1">5</span>]</pre></div>
<div class="highlight highlight-source-python"><pre>data <span class="pl-k">=</span> ImageDataBunch.from_lists(path, fn_paths, <span class="pl-v">labels</span><span class="pl-k">=</span>labels, <span class="pl-v">ds_tfms</span><span class="pl-k">=</span>tfms, <span class="pl-v">size</span><span class="pl-k">=</span><span class="pl-c1">24</span>)
data.classes</pre></div>
<p>So you can see there's lots of different ways of creating labels. So during the week, try this out.</p>
<p>Now you might be wondering how would you know to do all these things? Where am I going to find this kind of information? So I'll show you something incredibly cool. You know how to get documentation:</p>
<div class="highlight highlight-source-python"><pre>doc(ImageDataBunch.from_name_re)</pre></div>
<p>[<a href="https://docs.fast.ai/vision.data.html#ImageDataBunch.from_name_re" rel="nofollow">Show in docs</a>]</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/131.png"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/131.png" alt="" style="max-width:100%;"></a></p>
<p>Every single line of code I just showed you, I took it this morning and I copied and pasted it from the documentation. So you can see here the exact code that I just used. So the documentation for fastai doesn't just tell you what to do, but step to step how to do it. And here is perhaps the coolest bit. If you go to <a href="https://github.com/fastai/fastai_docs">fastai/fastai_docs</a> and click on <a href="https://github.com/fastai/fastai_docs/tree/master/docs_src">docs/src</a>.</p>
<p>All of our documentation is actually just Jupyter Notebooks. You can git clone this repo and if you run it, you can actually run every single line of the documentation yourself.</p>
<p>This is the kind of the ultimate example to me of experimenting. Anything that you read about in the documentation, nearly everything in the documentation has actual working examples in it with actual datasets that are already sitting in there in the repo for you. So you can actually try every single function in your browser, try seeing what goes in and try seeing what comes out.</p>
<p>[<a href="https://youtu.be/BWWm4AzsdLk?t=5847" rel="nofollow">1:37:27</a>]</p>
<p><strong>Question</strong>: Will the library use multi GPUs in parallel by default?</p>
<p>The library will use multiple CPUs by default but just one GPU by default. We probably won't be looking at multi GPU until part 2. It's easy to do and you'll find it on the forum, but most people won't be needing to use that now.</p>
<p><strong>Question</strong>: Can the library use 3D data such as MRI or CAT scan?</p>
<p>Yes, it can. ANd there is actually a forum thread about that already. Although that's not as developed as 2D yet but maybe by the time the MOOC is out, it will be.</p>
<h3><a id="user-content-splunk-anti-fraud-software-13810" class="anchor" aria-hidden="true" href="#splunk-anti-fraud-software-13810"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Splunk Anti-Fraud Software [<a href="https://youtu.be/BWWm4AzsdLk?t=5890" rel="nofollow">1:38:10</a>]</h3>
<p><a href="https://www.splunk.com/blog/2017/04/18/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html" rel="nofollow">blog</a></p>
<p>Before I wrap up, I'll just show you an example of the kind of interesting stuff that you can do by doing this kind of exercise.</p>
<p>Remember earlier I mentioned that one of our alumni who works at Splunk which is a NASDAQ listed big successful company created this new anti-fraud software. This is actually how he created it as part of a fastai part 1 class project:</p>
<p><a target="_blank" rel="noopener noreferrer" href="/yukari-n-erb/notes/blob/e_j/lesson1/132.jpg"><img src="/yukari-n-erb/notes/raw/e_j/lesson1/132.jpg" alt="" style="max-width:100%;"></a></p>
<p>He took the telemetry of users who had Splunk analytics installed and watched their mouse movements and he created pictures of the mouse movements. He converted speed into color and right and left clicks into splotches. He then took the exact code that we saw with an earlier version of the software and trained a CNN in exactly the same way we saw and used that to train his fraud model. So he took something which is not obviously a picture and he turned it into a picture and got these fantastically good results for a piece of fraud analysis software.</p>
<p>So it pays to think creatively. So if you are wanting to study sounds, a lot of people that study sounds do it by actually creating a spectrogram image and then sticking that into a ConvNet. So there's a lot of cool stuff you can do with this.</p>
<p>So during the week, get your GPU going, try and use your first notebook, make sure that you can use lesson 1 and work through it. Then see if you can repeat the process on your own dataset. Get on the forum and tell us any little success you had. Any constraints you hit, try it for an hour or two but if you get stuck, please ask. If you are able to successfully build a model with a new dataset, let us know! I will see you next week.</p>
</article>
  </div>

    </div>

  

  <details class="details-reset details-overlay details-overlay-dark">
    <summary data-hotkey="l" aria-label="Jump to line"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast linejump" aria-label="Jump to line">
      <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="js-jump-to-line-form Box-body d-flex" action="" accept-charset="UTF-8" method="get"><input name="utf8" type="hidden" value="&#x2713;" />
        <input class="form-control flex-auto mr-3 linejump-input js-jump-to-line-field" type="text" placeholder="Jump to line&hellip;" aria-label="Jump to line" autofocus>
        <button type="submit" class="btn" data-close-dialog>Go</button>
</form>    </details-dialog>
  </details>



  </div>
  <div class="modal-backdrop js-touch-events"></div>
</div>

    </div>
  </div>

  </div>

        
<div class="footer container-lg px-3" role="contentinfo">
  <div class="position-relative d-flex flex-justify-between pt-6 pb-2 mt-6 f6 text-gray border-top border-gray-light ">
    <ul class="list-style-none d-flex flex-wrap ">
      <li class="mr-3">&copy; 2019 <span title="0.49148s from unicorn-6c4dd6445-ggb6j">GitHub</span>, Inc.</li>
        <li class="mr-3"><a data-ga-click="Footer, go to terms, text:terms" href="https://github.com/site/terms">Terms</a></li>
        <li class="mr-3"><a data-ga-click="Footer, go to privacy, text:privacy" href="https://github.com/site/privacy">Privacy</a></li>
        <li class="mr-3"><a data-ga-click="Footer, go to security, text:security" href="https://github.com/security">Security</a></li>
        <li class="mr-3"><a href="https://githubstatus.com/" data-ga-click="Footer, go to status, text:status">Status</a></li>
        <li><a data-ga-click="Footer, go to help, text:help" href="https://help.github.com">Help</a></li>
    </ul>

    <a aria-label="Homepage" title="GitHub" class="footer-octicon mr-lg-4" href="https://github.com">
      <svg height="24" class="octicon octicon-mark-github" viewBox="0 0 16 16" version="1.1" width="24" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/></svg>
</a>
   <ul class="list-style-none d-flex flex-wrap ">
        <li class="mr-3"><a data-ga-click="Footer, go to contact, text:contact" href="https://github.com/contact">Contact GitHub</a></li>
        <li class="mr-3"><a href="https://github.com/pricing" data-ga-click="Footer, go to Pricing, text:Pricing">Pricing</a></li>
      <li class="mr-3"><a href="https://developer.github.com" data-ga-click="Footer, go to api, text:api">API</a></li>
      <li class="mr-3"><a href="https://training.github.com" data-ga-click="Footer, go to training, text:training">Training</a></li>
        <li class="mr-3"><a href="https://github.blog" data-ga-click="Footer, go to blog, text:blog">Blog</a></li>
        <li><a data-ga-click="Footer, go to about, text:about" href="https://github.com/about">About</a></li>

    </ul>
  </div>
  <div class="d-flex flex-justify-center pb-6">
    <span class="f6 text-gray-light"></span>
  </div>
</div>



  <div id="ajax-error-message" class="ajax-error-message flash flash-error">
    <svg class="octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"/></svg>
    <button type="button" class="flash-close js-ajax-error-dismiss" aria-label="Dismiss error">
      <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"/></svg>
    </button>
    You can’t perform that action at this time.
  </div>


    
    <script crossorigin="anonymous" integrity="sha512-jmiDi11+OpgKRxT04ET3AFjcnpo6ZRlazPlsLjkUAAhSdCfO+qJQg9TSXTHBrZFsOEzDlQvcyd8j0ej+NIyWlA==" type="application/javascript" src="https://github.githubassets.com/assets/frameworks-4da747c86010088450ec11c4e199a7da.js"></script>
    
    <script crossorigin="anonymous" async="async" integrity="sha512-HRXb6CpTXT+P401CFZe29CmtQvMP8dMjrMlLFHNIcg5rEPY0wUCJ67ZW8dzvbtC874W9Y9Fn3K9c6NZLSVyosw==" type="application/javascript" src="https://github.githubassets.com/assets/github-2ce692273fafd686763972cca8cc26d6.js"></script>
    
    
    
  <div class="js-stale-session-flash stale-session-flash flash flash-warn flash-banner d-none">
    <svg class="octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"/></svg>
    <span class="signed-in-tab-flash">You signed in with another tab or window. <a href="">Reload</a> to refresh your session.</span>
    <span class="signed-out-tab-flash">You signed out in another tab or window. <a href="">Reload</a> to refresh your session.</span>
  </div>
  <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default text-gray-dark" open>
    <summary aria-haspopup="dialog" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog>
        <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"/></svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

  <div class="Popover js-hovercard-content position-absolute" style="display: none; outline: none;" tabindex="0">
  <div class="Popover-message Popover-message--bottom-left Popover-message--large Box box-shadow-large" style="width:360px;">
  </div>
</div>

<div id="hovercard-aria-description" class="sr-only">
  Press h to open a hovercard with more details.
</div>

  <div aria-live="polite" class="js-global-screen-reader-notice sr-only"></div>

  </body>
</html>

